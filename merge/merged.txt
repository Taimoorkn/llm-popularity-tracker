i asked claude to improve my codebase so that it can handle more users better, so that it will scale gracefully and without realizing he kept adding new files of all types and kinds and didnt even use most of them and now im left with a mess of a codebase. I have no idea which file is the one being using. Which files to keep. I see all these docker files and dont know what to do. I need to use websockets and i need to make my docker to use the better stuff but im just in pure confusion because of so many files


{
  "name": "llm-popularity-tracker",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint",
    "db:migrate": "node scripts/migrate.js",
    "db:seed": "node scripts/seed.js",
    "db:reset": "node scripts/reset.js",
    "db:reset-all": "node scripts/reset-all.js"
  },
  "dependencies": {
    "@fingerprintjs/fingerprintjs": "^4.6.2",
    "@fingerprintjs/fingerprintjs-pro-react": "^2.7.0",
    "@socket.io/redis-adapter": "^8.3.0",
    "autoprefixer": "^10.4.21",
    "axios": "^1.11.0",
    "bcryptjs": "^2.4.3",
    "cookies-next": "^6.1.0",
    "express-rate-limit": "^7.4.1",
    "framer-motion": "^12.23.12",
    "ioredis": "^5.4.1",
    "joi": "^17.13.3",
    "jsonwebtoken": "^9.0.2",
    "lucide-react": "^0.542.0",
    "next": "15.5.2",
    "pg": "^8.13.1",
    "pg-pool": "^3.7.0",
    "pino": "^9.6.0",
    "pino-pretty": "^12.0.0",
    "postcss": "^8.5.6",
    "pusher": "^5.2.0",
    "pusher-js": "^8.4.0",
    "rate-limiter-flexible": "^7.2.0",
    "react": "19.1.0",
    "react-dom": "19.1.0",
    "recharts": "^3.1.2",
    "socket.io": "^4.8.1",
    "socket.io-client": "^4.8.1",
    "sonner": "^2.0.7",
    "swr": "^2.3.6",
    "tailwindcss": "^3.4.17",
    "uuid": "^10.0.0",
    "winston": "^3.17.0",
    "zustand": "^5.0.8"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@types/bcryptjs": "^2.4.6",
    "@types/jsonwebtoken": "^9.0.7",
    "@types/pg": "^8.11.12",
    "@types/uuid": "^10.0.0",
    "eslint": "^9",
    "eslint-config-next": "15.5.2"
  }
}


# Dockerfile.optimized
# Multi-stage build for optimized production image
FROM node:18-alpine AS builder

# Install build dependencies
RUN apk add --no-cache libc6-compat

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install all dependencies (including dev)
RUN npm ci

# Copy source code
COPY . .

# Build the Next.js application
RUN npm run build

# Production stage
FROM node:18-alpine AS runner

WORKDIR /app

# Install production dependencies only
RUN apk add --no-cache libc6-compat curl

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nextjs -u 1001

# Copy built application from builder
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next/standalone ./
COPY --from=builder /app/.next/static ./.next/static
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

# Copy configuration files
COPY --from=builder /app/lib ./lib
COPY --from=builder /app/scripts ./scripts
COPY --from=builder /app/config ./config

# Set environment
ENV NODE_ENV=production
ENV NEXT_TELEMETRY_DISABLED=1

# Change ownership
RUN chown -R nextjs:nodejs /app

USER nextjs

EXPOSE 3000

ENV PORT=3000
ENV HOSTNAME="0.0.0.0"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:3000/api/health || exit 1

# Start the application
CMD ["node", "server.js"]

# Dockerfile
# Multi-stage build for optimal size and security
FROM node:18-alpine AS deps
# Install dependencies only when needed
RUN apk add --no-cache libc6-compat
WORKDIR /app

# Copy dependency files
COPY package.json package-lock.json ./
RUN npm ci --only=production

# Rebuild the source code only when needed
FROM node:18-alpine AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Build the Next.js app
RUN npm run build

# Production image, copy all the files and run next
FROM node:18-alpine AS runner
WORKDIR /app

ENV NODE_ENV production

# Create a non-root user
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

# Copy built application
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

# Copy necessary files
COPY --from=builder /app/lib ./lib
COPY --from=builder /app/scripts ./scripts
COPY --from=builder /app/app ./app
COPY --from=builder /app/config ./config

# Set correct permissions
RUN chown -R nextjs:nodejs /app

USER nextjs

EXPOSE 3000

ENV PORT 3000

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3000/api/health', (r) => {r.statusCode === 200 ? process.exit(0) : process.exit(1)})"

CMD ["node", "server.js"]

# docker-compose.yml
# Development Docker Compose - PostgreSQL and Redis only
# Run your Next.js app locally for development
services:
  postgres:
    image: postgres:14-alpine
    container_name: llm-tracker-postgres-dev
    restart: unless-stopped
    environment:
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: changeme
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data_dev:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U llm_user -d llm_tracker"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: llm-tracker-redis-dev
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data_dev:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

# Production services (use with --profile production)
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-tracker-app
    restart: unless-stopped
    ports:
      - "${PORT:-3000}:3000"
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://llm_user:changeme@postgres:5432/llm_tracker
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: changeme
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ""
      JWT_SECRET: ${JWT_SECRET:-change-this-secret-in-production}
      SESSION_SECRET: ${SESSION_SECRET:-change-this-session-secret}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    command: >
      sh -c "
        echo 'Waiting for database...' &&
        sleep 5 &&
        npm run db:migrate &&
        npm run db:seed &&
        node server.js
      "
    profiles:
      - production

  nginx:
    image: nginx:alpine
    container_name: llm-tracker-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - app
    profiles:
      - production

volumes:
  postgres_data_dev:
    driver: local
  redis_data_dev:
    driver: local

networks:
  default:
    name: llm-tracker-network
    driver: bridge

#docker-compose.test-scale.yml
version: '3.8'

services:
  # PostgreSQL Primary (using existing setup)
  postgres:
    image: postgres:14-alpine
    container_name: llm-postgres-scaled
    restart: unless-stopped
    environment:
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: changeme
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_scaled_data:/var/lib/postgresql/data
      - ./config/postgres-optimization.sql:/docker-entrypoint-initdb.d/01-optimize.sql
      - ./init-db:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U llm_user -d llm_tracker"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - llm-network

  # Redis with enhanced configuration
  redis:
    image: redis:7-alpine
    container_name: llm-redis-scaled
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_scaled_data:/data
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory 1gb 
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - llm-network

  # Single app instance for testing
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-app-scaled
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://llm_user:changeme@postgres:5432/llm_tracker
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: changeme
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ""
      JWT_SECRET: ${JWT_SECRET:-change-this-secret-in-production}
      SESSION_SECRET: ${SESSION_SECRET:-change-this-session-secret}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    command: >
      sh -c "
        echo 'Waiting for database...' &&
        sleep 5 &&
        npm run db:migrate &&
        npm run db:seed &&
        node server.js
      "
    networks:
      - llm-network

  # Nginx for load balancing (when scaling app)
  nginx:
    image: nginx:alpine
    container_name: llm-nginx-scaled
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx-scale.conf:/etc/nginx/nginx.conf:ro
      - nginx_cache:/var/cache/nginx
    depends_on:
      - app
    networks:
      - llm-network

volumes:
  postgres_scaled_data:
  redis_scaled_data:
  nginx_cache:

networks:
  llm-network:
    driver: bridge

# Production Environment Variables for 30k+ Monthly Users

# Database Configuration
DATABASE_URL=postgresql://llm_user:changeme@postgres:5432/llm_tracker
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=llm_tracker
POSTGRES_USER=llm_user
POSTGRES_PASSWORD=changeme

# Redis Configuration
REDIS_URL=redis://redis:6379
REDIS_HOST=redis
REDIS_PORT=6379

# Application Configuration
NODE_ENV=production
PORT=3001
CORS_ORIGIN=http://localhost

# WebSocket Configuration
ENABLE_WEBSOCKETS=true
NEXT_PUBLIC_WS_URL=http://localhost

# Security
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production
SESSION_SECRET=your-super-secret-session-key-change-this-in-production

# Rate Limiting (Optimized for 30k+ users)
RATE_LIMIT_MAX=120
RATE_LIMIT_WINDOW_MS=60000

# Feature Flags
ENABLE_ANALYTICS=true
ENABLE_REAL_TIME=true

# Performance Tuning
MAX_POOL_SIZE=50
MIN_POOL_SIZE=10
IDLE_TIMEOUT_MS=30000
CONNECTION_TIMEOUT_MS=5000
QUERY_TIMEOUT_MS=10000

# Cache Settings
CACHE_TTL_SECONDS=300
MEMORY_CACHE_TTL_MS=60000

# Scaling Settings (for future growth)
ENABLE_MATERIALIZED_VIEWS=true
REFRESH_MATERIALIZED_VIEWS_INTERVAL_MS=300000

# Local development environment for optimized setup

# Database Configuration
DATABASE_URL=postgresql://llm_user:changeme@localhost:5432/llm_tracker
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=llm_tracker
POSTGRES_USER=llm_user
POSTGRES_PASSWORD=changeme

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379

# Application Configuration
NODE_ENV=development
PORT=3000
CORS_ORIGIN=http://localhost:3000

# WebSocket Configuration
ENABLE_WEBSOCKETS=true
NEXT_PUBLIC_WS_URL=http://localhost:3000

# Security (development keys)
JWT_SECRET=dev-jwt-secret-key
SESSION_SECRET=dev-session-secret-key

# Rate Limiting
RATE_LIMIT_MAX=120
RATE_LIMIT_WINDOW_MS=60000

# Feature Flags
ENABLE_ANALYTICS=true
ENABLE_REAL_TIME=true

# Database Configuration
DATABASE_URL=postgresql://llm_user:changeme@localhost:5432/llm_tracker
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=llm_tracker
POSTGRES_USER=llm_user
POSTGRES_PASSWORD=changeme
POSTGRES_READ_HOSTS=postgres-read1,postgres-read2
PGBOUNCER_PORT=6432

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_SENTINELS=localhost:26379,localhost:26380,localhost:26381
REDIS_MASTER_NAME=mymaster

# Application Configuration
NODE_ENV=production
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=your-secret-key-here-change-this-in-production
JWT_SECRET=your-jwt-secret-here-change-this-in-production
JWT_EXPIRES_IN=7d

# WebSocket Configuration
NEXT_PUBLIC_WS_URL=http://localhost:3000
CORS_ORIGIN=*

# Rate Limiting
RATE_LIMIT_MAX=100
RATE_LIMIT_WINDOW_MS=900000

# Logging
LOG_LEVEL=info

# Security
BCRYPT_ROUNDS=12
SESSION_SECRET=your-session-secret-here-change-this-in-production

# Feature Flags
ENABLE_ANALYTICS=true
ENABLE_REAL_TIME=true

# Performance
PM2_INSTANCES=max
PM2_EXEC_MODE=cluster

# Monitoring
GRAFANA_PASSWORD=admin

# Port Configuration
PORT=3000


#nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream app_cluster {
        least_conn;
        server app:3000 max_fails=3 fail_timeout=30s;
        # Add more app instances for scaling
        # server app2:3000 max_fails=3 fail_timeout=30s;
        # server app3:3000 max_fails=3 fail_timeout=30s;
    }

    # Rate limiting zones
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=vote_limit:10m rate=5r/s;
    
    # Connection limiting
    limit_conn_zone $binary_remote_addr zone=conn_limit:10m;

    # Cache settings
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=api_cache:10m max_size=100m inactive=60m use_temp_path=off;

    server {
        listen 80;
        server_name localhost;
        
        # Redirect to HTTPS in production
        # return 301 https://$server_name$request_uri;
        
        # Security headers
        add_header X-Frame-Options "DENY" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;
        add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-eval' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' ws: wss:;" always;
        
        # Gzip compression
        gzip on;
        gzip_vary on;
        gzip_min_length 1024;
        gzip_types text/plain text/css text/xml text/javascript application/json application/javascript application/xml+rss application/rss+xml application/atom+xml image/svg+xml text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype;
        
        # Connection limiting
        limit_conn conn_limit 10;
        
        # API endpoints with rate limiting
        location /api/vote {
            limit_req zone=vote_limit burst=10 nodelay;
            limit_req_status 429;
            
            proxy_pass http://app_cluster;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
        }
        
        location /api/ {
            limit_req zone=api_limit burst=20 nodelay;
            
            # Cache GET requests
            proxy_cache api_cache;
            proxy_cache_valid 200 1m;
            proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;
            proxy_cache_bypass $http_cache_control;
            add_header X-Cache-Status $upstream_cache_status;
            
            proxy_pass http://app_cluster;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
        }
        
        # Static files
        location /_next/static {
            proxy_cache api_cache;
            proxy_cache_valid 200 60m;
            proxy_pass http://app_cluster;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
            
            # Long cache for static assets
            expires 365d;
            add_header Cache-Control "public, immutable";
        }
        
        # Health check endpoint (no rate limiting)
        location /api/health {
            proxy_pass http://app_cluster;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            access_log off;
        }
        
        # Default location
        location / {
            proxy_pass http://app_cluster;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
        }
    }
    
    # HTTPS configuration (uncomment for production)
    # server {
    #     listen 443 ssl http2;
    #     server_name your-domain.com;
    #     
    #     ssl_certificate /etc/nginx/ssl/cert.pem;
    #     ssl_certificate_key /etc/nginx/ssl/key.pem;
    #     ssl_protocols TLSv1.2 TLSv1.3;
    #     ssl_ciphers HIGH:!aNULL:!MD5;
    #     ssl_prefer_server_ciphers on;
    #     ssl_session_cache shared:SSL:10m;
    #     ssl_session_timeout 10m;
    #     
    #     # Copy all location blocks from above
    # }
}

#nginx-scale.conf
user nginx;
worker_processes auto;
worker_rlimit_nofile 65535;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 10240;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/access.log main buffer=16k;

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    keepalive_requests 100;
    types_hash_max_size 2048;
    client_max_body_size 20M;
    client_body_buffer_size 128k;
    
    # Gzip Settings
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/rss+xml application/atom+xml image/svg+xml 
               text/x-js text/x-cross-domain-policy application/x-font-ttf 
               application/x-font-opentype application/vnd.ms-fontobject 
               image/x-icon;

    # Rate Limiting Zones
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/s;
    limit_req_zone $binary_remote_addr zone=vote:10m rate=60r/m;
    limit_req_zone $binary_remote_addr zone=static:10m rate=200r/s;
    limit_conn_zone $binary_remote_addr zone=addr:10m;

    # Cache Settings
    proxy_cache_path /var/cache/nginx/static levels=1:2 keys_zone=static_cache:100m 
                     max_size=1g inactive=24h use_temp_path=off;
    proxy_cache_path /var/cache/nginx/api levels=1:2 keys_zone=api_cache:50m 
                     max_size=500m inactive=5m use_temp_path=off;

    # Upstream Backend Servers
    upstream app_servers {
        least_conn;
        
        # Application servers
        server app:3000 max_fails=3 fail_timeout=30s;
        
        # Add more servers as they scale
        # Docker Compose will create app_2, app_3, etc.
        server app_2:3000 max_fails=3 fail_timeout=30s backup;
        server app_3:3000 max_fails=3 fail_timeout=30s backup;
        server app_4:3000 max_fails=3 fail_timeout=30s backup;
        
        # Keepalive connections to backend
        keepalive 32;
    }

    # WebSocket Upstream
    upstream websocket_servers {
        ip_hash;  # Sticky sessions for WebSocket
        
        server app:3000;
        server app_2:3000;
        server app_3:3000;
        server app_4:3000;
        
        keepalive 64;
    }

    # Map for WebSocket upgrade
    map $http_upgrade $connection_upgrade {
        default upgrade;
        '' close;
    }

    # Main Server Block
    server {
        listen 80;
        listen [::]:80;
        server_name _;
        
        # Security Headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header Referrer-Policy "no-referrer-when-downgrade" always;
        add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'" always;
        
        # Connection limits
        limit_conn addr 100;
        
        # Static files with aggressive caching
        location /_next/static {
            proxy_pass http://app_servers;
            proxy_cache static_cache;
            proxy_cache_valid 200 302 7d;
            proxy_cache_valid 404 1m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            proxy_cache_lock on;
            
            add_header X-Cache-Status $upstream_cache_status;
            add_header Cache-Control "public, max-age=604800, immutable";
            
            # Rate limiting for static files
            limit_req zone=static burst=50 nodelay;
        }
        
        # Images and assets
        location ~* \.(jpg|jpeg|png|gif|ico|svg|woff|woff2|ttf|eot)$ {
            proxy_pass http://app_servers;
            proxy_cache static_cache;
            proxy_cache_valid 200 302 30d;
            proxy_cache_valid 404 1m;
            
            add_header X-Cache-Status $upstream_cache_status;
            add_header Cache-Control "public, max-age=2592000, immutable";
            expires 30d;
        }
        
        # API endpoints with rate limiting
        location /api/vote {
            proxy_pass http://app_servers;
            proxy_http_version 1.1;
            
            # Strict rate limiting for voting
            limit_req zone=vote burst=5 nodelay;
            limit_req_status 429;
            
            # No caching for vote endpoints
            proxy_cache off;
            
            # Headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 10s;
            proxy_read_timeout 10s;
        }
        
        # Other API endpoints
        location /api {
            proxy_pass http://app_servers;
            proxy_http_version 1.1;
            
            # Rate limiting for API
            limit_req zone=api burst=20 nodelay;
            limit_req_status 429;
            
            # Conditional caching
            proxy_cache api_cache;
            proxy_cache_methods GET HEAD;
            proxy_cache_valid 200 5s;
            proxy_cache_valid 404 1m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_bypass $http_pragma $http_authorization;
            proxy_no_cache $http_pragma $http_authorization;
            
            # Headers
            add_header X-Cache-Status $upstream_cache_status;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Connection "";
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }
        
        # WebSocket support
        location /socket.io {
            proxy_pass http://websocket_servers;
            proxy_http_version 1.1;
            
            # WebSocket headers
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $connection_upgrade;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket timeouts
            proxy_connect_timeout 7d;
            proxy_send_timeout 7d;
            proxy_read_timeout 7d;
            
            # Disable buffering for WebSocket
            proxy_buffering off;
            proxy_cache off;
        }
        
        # Health check endpoint (no rate limiting)
        location /api/health {
            proxy_pass http://app_servers;
            proxy_http_version 1.1;
            access_log off;
            
            proxy_set_header Host $host;
            proxy_connect_timeout 1s;
            proxy_send_timeout 1s;
            proxy_read_timeout 1s;
        }
        
        # Default location
        location / {
            proxy_pass http://app_servers;
            proxy_http_version 1.1;
            
            # General rate limiting
            limit_req zone=api burst=30 nodelay;
            
            # Caching for HTML pages
            proxy_cache api_cache;
            proxy_cache_valid 200 10s;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            
            # Headers
            add_header X-Cache-Status $upstream_cache_status;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Connection "";
            
            # Keep-alive
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }
        
        # Custom error pages
        error_page 404 /404.html;
        error_page 500 502 503 504 /50x.html;
        
        location = /50x.html {
            root /usr/share/nginx/html;
        }
    }

    # HTTPS Server Block (when SSL is configured)
    server {
        listen 443 ssl http2;
        listen [::]:443 ssl http2;
        server_name _;
        
        # SSL Configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers off;
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 10m;
        ssl_session_tickets off;
        ssl_stapling on;
        ssl_stapling_verify on;
        
        # Copy all location blocks from HTTP server
        # ... (same configuration as above)
    }
    
    # Redirect HTTP to HTTPS (when SSL is enabled)
    # server {
    #     listen 80;
    #     listen [::]:80;
    #     server_name _;
    #     return 301 https://$server_name$request_uri;
    # }
}

#docker-compose.scale.yml
version: '3.8'

# Production Docker Compose for 200k+ users with scaling capabilities
services:
  # PostgreSQL Primary
  postgres-primary:
    image: postgres:14-alpine
    container_name: llm-postgres-primary
    restart: unless-stopped
    environment:
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=C"
      POSTGRES_HOST_AUTH_METHOD: md5
    ports:
      - "5432:5432"
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./config/postgres-optimization.sql:/docker-entrypoint-initdb.d/01-optimize.sql
      - ./init-db:/docker-entrypoint-initdb.d
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U llm_user -d llm_tracker"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - llm-network

  # PostgreSQL Read Replica 1
  postgres-read1:
    image: postgres:14-alpine
    container_name: llm-postgres-read1
    restart: unless-stopped
    environment:
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_PRIMARY_HOST: postgres-primary
      POSTGRES_PRIMARY_PORT: 5432
      POSTGRES_PRIMARY_USER: llm_user
      POSTGRES_PRIMARY_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
    volumes:
      - postgres_read1_data:/var/lib/postgresql/data
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
    command: |
      bash -c "
      pg_basebackup -h postgres-primary -D /var/lib/postgresql/data -U llm_user -v -P -W -X stream
      echo 'standby_mode = on' >> /var/lib/postgresql/data/recovery.conf
      echo 'primary_conninfo = host=postgres-primary port=5432 user=llm_user' >> /var/lib/postgresql/data/recovery.conf
      postgres -c config_file=/etc/postgresql/postgresql.conf
      "
    depends_on:
      postgres-primary:
        condition: service_healthy
    networks:
      - llm-network

  # PostgreSQL Read Replica 2
  postgres-read2:
    image: postgres:14-alpine
    container_name: llm-postgres-read2
    restart: unless-stopped
    environment:
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_PRIMARY_HOST: postgres-primary
      POSTGRES_PRIMARY_PORT: 5432
      POSTGRES_PRIMARY_USER: llm_user
      POSTGRES_PRIMARY_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
    volumes:
      - postgres_read2_data:/var/lib/postgresql/data
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
    command: |
      bash -c "
      pg_basebackup -h postgres-primary -D /var/lib/postgresql/data -U llm_user -v -P -W -X stream
      echo 'standby_mode = on' >> /var/lib/postgresql/data/recovery.conf
      echo 'primary_conninfo = host=postgres-primary port=5432 user=llm_user' >> /var/lib/postgresql/data/recovery.conf
      postgres -c config_file=/etc/postgresql/postgresql.conf
      "
    depends_on:
      postgres-primary:
        condition: service_healthy
    networks:
      - llm-network

  # PgBouncer Connection Pooler
  pgbouncer:
    image: bitnami/pgbouncer:latest
    container_name: llm-pgbouncer
    restart: unless-stopped
    environment:
      DATABASES_HOST: postgres-primary
      DATABASES_PORT: 5432
      DATABASES_DBNAME: llm_tracker
      DATABASES_USER: llm_user
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 10000
      DEFAULT_POOL_SIZE: 25
      MIN_POOL_SIZE: 10
      RESERVE_POOL_SIZE: 5
    ports:
      - "6432:5432"
    volumes:
      - ./config/pgbouncer.ini:/etc/pgbouncer/pgbouncer.ini
      - ./config/userlist.txt:/etc/pgbouncer/userlist.txt
    depends_on:
      - postgres-primary
    networks:
      - llm-network

  # Redis Primary
  redis-primary:
    image: redis:7-alpine
    container_name: llm-redis-primary
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_primary_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - llm-network

  # Redis Sentinel 1
  redis-sentinel1:
    image: redis:7-alpine
    container_name: llm-redis-sentinel1
    restart: unless-stopped
    ports:
      - "26379:26379"
    volumes:
      - ./config/sentinel1.conf:/usr/local/etc/redis/sentinel.conf
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    depends_on:
      - redis-primary
    networks:
      - llm-network

  # Redis Sentinel 2
  redis-sentinel2:
    image: redis:7-alpine
    container_name: llm-redis-sentinel2
    restart: unless-stopped
    ports:
      - "26380:26379"
    volumes:
      - ./config/sentinel2.conf:/usr/local/etc/redis/sentinel.conf
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    depends_on:
      - redis-primary
    networks:
      - llm-network

  # Redis Sentinel 3
  redis-sentinel3:
    image: redis:7-alpine
    container_name: llm-redis-sentinel3
    restart: unless-stopped
    ports:
      - "26381:26379"
    volumes:
      - ./config/sentinel3.conf:/usr/local/etc/redis/sentinel.conf
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    depends_on:
      - redis-primary
    networks:
      - llm-network

  # Application instances (scale with docker-compose up --scale app=8)
  app:
    build:
      context: .
      dockerfile: Dockerfile.optimized
    restart: unless-stopped
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://llm_user:${POSTGRES_PASSWORD:-changeme}@pgbouncer:5432/llm_tracker
      POSTGRES_HOST: pgbouncer
      POSTGRES_PORT: 5432
      POSTGRES_DB: llm_tracker
      POSTGRES_USER: llm_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      REDIS_SENTINELS: redis-sentinel1:26379,redis-sentinel2:26379,redis-sentinel3:26379
      REDIS_MASTER_NAME: mymaster
      JWT_SECRET: ${JWT_SECRET:-change-this-secret-in-production}
      SESSION_SECRET: ${SESSION_SECRET:-change-this-session-secret}
      PORT: 3000
      # PM2 Configuration
      PM2_INSTANCES: max
      PM2_EXEC_MODE: cluster
    volumes:
      - ./data:/app/data
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    depends_on:
      - pgbouncer
      - redis-primary
      - redis-sentinel1
      - redis-sentinel2
      - redis-sentinel3
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Nginx Load Balancer
  nginx:
    image: nginx:alpine
    container_name: llm-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx-scale.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx
    depends_on:
      - app
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: llm-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - llm-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: llm-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: redis-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - llm-network

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: llm-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    networks:
      - llm-network

  # Postgres Exporter for database metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: llm-postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://llm_user:${POSTGRES_PASSWORD:-changeme}@postgres-primary:5432/llm_tracker?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres-primary
    networks:
      - llm-network

  # Redis Exporter for cache metrics
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: llm-redis-exporter
    restart: unless-stopped
    environment:
      REDIS_ADDR: redis://redis-primary:6379
    ports:
      - "9121:9121"
    depends_on:
      - redis-primary
    networks:
      - llm-network

volumes:
  postgres_primary_data:
  postgres_read1_data:
  postgres_read2_data:
  redis_primary_data:
  nginx_cache:
  prometheus_data:
  grafana_data:

networks:
  llm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\api\health\route.js
import { NextResponse } from 'next/server';
import { getAdaptedVoteManager } from '@/lib/vote-manager-wrapper';
import dbManager from '@/lib/database';
import cacheManager from '@/lib/cache';
import logger from '@/lib/logger';
import { securityHeaders } from '@/lib/middleware';

export async function GET(request) {
  const startTime = Date.now();
  
  try {
    const voteManager = await getAdaptedVoteManager();
    const health = await voteManager.checkHealth();
    
    // Additional health checks
    const checks = {
      database: health.database,
      cache: health.cache,
      api: {
        status: 'healthy',
        latency: `${Date.now() - startTime}ms`
      },
      overall: health.status
    };
    
    // Determine HTTP status based on health
    const httpStatus = health.status === 'healthy' ? 200 : 
                       health.status === 'degraded' ? 503 : 500;
    
    logger.debug('Health check performed', checks);
    
    return NextResponse.json(
      {
        status: health.status,
        checks,
        timestamp: new Date().toISOString(),
        uptime: process.uptime(),
        memory: process.memoryUsage(),
        version: process.env.npm_package_version || '0.1.0'
      },
      {
        status: httpStatus,
        headers: securityHeaders()
      }
    );
  } catch (error) {
    logger.error('Health check failed:', error);
    
    return NextResponse.json(
      {
        status: 'unhealthy',
        error: error.message,
        timestamp: new Date().toISOString()
      },
      {
        status: 500,
        headers: securityHeaders()
      }
    );
  }
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\api\stats\route.js
import { NextResponse } from 'next/server';
import { getAdaptedVoteManager } from '@/lib/vote-manager-wrapper';
import { apiMiddleware, securityHeaders, logResponse } from '@/lib/middleware';
import logger from '@/lib/logger';

export async function GET(request) {
  // Apply middleware
  const middlewareResult = await apiMiddleware(request, {
    rateLimit: {
      maxRequests: 200,
      windowMs: 60000 // 200 requests per minute
    }
  });
  
  if (middlewareResult.status) {
    return middlewareResult;
  }
  
  const { requestInfo, rateLimitHeaders } = middlewareResult;
  
  try {
    const voteManager = await getAdaptedVoteManager();
    const stats = await voteManager.getStats();
    const rankings = await voteManager.getRankings();
    
    logResponse(requestInfo, { status: 200 });
    
    return NextResponse.json(
      {
        stats,
        rankings,
        timestamp: new Date().toISOString(),
      },
      {
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  } catch (error) {
    logger.error('Stats error:', error);
    logResponse(requestInfo, { status: 500 }, error);
    
    return NextResponse.json(
      { error: 'Failed to get stats' },
      { 
        status: 500,
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  }
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\api\vote\route.js
import { NextResponse } from 'next/server';
import { getAdaptedVoteManager } from '@/lib/vote-manager-wrapper';
import { apiMiddleware, schemas, detectFraud, securityHeaders, logResponse } from '@/lib/middleware';
import logger from '@/lib/logger';

export async function POST(request) {
  // Apply middleware
  const middlewareResult = await apiMiddleware(request, {
    schema: schemas.vote,
    rateLimit: {
      maxRequests: 60,
      windowMs: 60000 // 60 requests per minute
    }
  });
  
  if (middlewareResult.status) {
    // Middleware returned an error response
    return middlewareResult;
  }
  
  const { validatedData, requestInfo, rateLimitHeaders } = middlewareResult;
  
  try {
    const { fingerprint, llmId, voteType } = validatedData;
    
    // Fraud detection
    const fraudCheck = await detectFraud(fingerprint, 'vote', {
      llmId,
      voteType
    });
    
    if (fraudCheck.isSuspicious && fraudCheck.riskScore > 50) {
      logger.business.fraudulentVoteDetected(
        fingerprint,
        'High risk score',
        fraudCheck
      );
      
      return NextResponse.json(
        { error: 'Suspicious activity detected. Please try again later.' },
        { status: 429 }
      );
    }
    
    // Process vote using database
    const voteManager = await getAdaptedVoteManager();
    const result = await voteManager.vote(fingerprint, llmId, voteType, {
      ip: requestInfo.ip,
      userAgent: requestInfo.userAgent
    });
    
    if (!result.success) {
      return NextResponse.json(
        { error: result.error },
        { status: 400, headers: { ...securityHeaders(), ...rateLimitHeaders } }
      );
    }
    
    // Log successful vote
    logResponse(requestInfo, { status: 200 });
    
    return NextResponse.json(
      {
        success: true,
        votes: result.votes,
        userVote: result.userVote,
        previousVote: result.previousVote,
      },
      { 
        status: 200,
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  } catch (error) {
    logger.error('Vote error:', error);
    logResponse(requestInfo, { status: 500 }, error);
    
    return NextResponse.json(
      { error: 'Failed to process vote' },
      { 
        status: 500,
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  }
}

// This endpoint is deprecated - use POST /api/vote/sync instead
export async function GET(request) {
  // Apply middleware
  const middlewareResult = await apiMiddleware(request, {
    rateLimit: {
      maxRequests: 100,
      windowMs: 60000 // 100 requests per minute
    }
  });
  
  if (middlewareResult.status) {
    return middlewareResult;
  }
  
  const { requestInfo, rateLimitHeaders } = middlewareResult;
  
  try {
    const voteManager = await getAdaptedVoteManager();
    const votes = await voteManager.getVotes();
    const rankings = await voteManager.getRankings();
    const stats = await voteManager.getStats();
    
    logger.logResponse(requestInfo, { status: 200 });
    
    return NextResponse.json(
      {
        votes,
        rankings,
        stats,
        userVotes: {},
      },
      {
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  } catch (error) {
    logger.error('Get votes error:', error);
    logger.logResponse(requestInfo, { status: 500 }, error);
    
    return NextResponse.json(
      { error: 'Failed to get votes' },
      { 
        status: 500,
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  }
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\api\vote\sync\route.js
import { NextResponse } from 'next/server';
import { getAdaptedVoteManager } from '@/lib/vote-manager-wrapper';
import { apiMiddleware, schemas, securityHeaders, logResponse } from '@/lib/middleware';
import logger from '@/lib/logger';

export async function POST(request) {
  // Apply middleware
  const middlewareResult = await apiMiddleware(request, {
    schema: schemas.sync,
    rateLimit: {
      maxRequests: 100,
      windowMs: 60000 // 100 requests per minute
    }
  });
  
  if (middlewareResult.status) {
    return middlewareResult;
  }
  
  const { validatedData, requestInfo, rateLimitHeaders } = middlewareResult;
  
  try {
    const { fingerprint } = validatedData;
    
    const voteManager = await getAdaptedVoteManager();
    const syncData = await voteManager.syncUserVotes(fingerprint);
    
    logResponse(requestInfo, { status: 200 });
    
    return NextResponse.json(
      syncData,
      {
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  } catch (error) {
    logger.error('Sync votes error:', error);
    logResponse(requestInfo, { status: 500 }, error);
    
    return NextResponse.json(
      { error: 'Failed to sync votes' },
      { 
        status: 500,
        headers: { ...securityHeaders(), ...rateLimitHeaders }
      }
    );
  }
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\globals.css
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }

  body {
    @apply bg-background text-foreground;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  }

  * {
    @apply border-border;
  }
}

@layer utilities {
  .text-gradient {
    @apply bg-gradient-to-r from-primary via-accent to-success bg-clip-text text-transparent;
  }

  .glow {
    @apply shadow-lg shadow-primary/20;
  }

  .card-glow {
    @apply hover:shadow-xl hover:shadow-primary/10 transition-shadow duration-300;
  }

  .scrollbar-thin {
    scrollbar-width: thin;
    scrollbar-color: #4b5563 #1f2937;
  }

  .scrollbar-thin::-webkit-scrollbar {
    width: 8px;
    height: 8px;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply bg-gray-800 rounded-full;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply bg-gray-600 rounded-full hover:bg-gray-500;
  }
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\layout.js
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata = {
  title: "LLM Popularity Tracker 2025",
  description: "Vote for your favorite Large Language Models and see real-time community rankings",
};

export default function RootLayout({ children }) {
  return (
    <html lang="en" className="dark">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased bg-background text-foreground`}
      >
        {children}
      </body>
    </html>
  );
}


// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\app\page.js
'use client';

import { useEffect, useState } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { Search, Filter, TrendingUp } from 'lucide-react';
import { Toaster } from 'sonner';
import Header from '@/components/Header';
import LLMCard from '@/components/LLMCard';
import StatsPanel from '@/components/StatsPanel';
import VoteChart from '@/components/VoteChart';
import useVoteStore from '@/store/useVoteStore';

export default function Home() {
  const [searchTerm, setSearchTerm] = useState('');
  const [sortBy, setSortBy] = useState('name'); // votes, name, company
  const [showChart, setShowChart] = useState(true);
  
  const { llms, initializeVotes, rankings, loading } = useVoteStore();
  
  useEffect(() => {
    initializeVotes();
    
    // Set up polling for real-time updates (every 5 seconds)
    const pollInterval = setInterval(() => {
      // Silently sync with server to get latest votes
      useVoteStore.getState().syncWithServer();
    }, 5000); // Poll every 5 seconds for better real-time experience
    
    // Clean up on unmount
    return () => clearInterval(pollInterval);
  }, [initializeVotes]);
  
  // Filter LLMs but maintain stable ordering
  const filteredLLMs = llms
    .filter(llm => 
      llm.name.toLowerCase().includes(searchTerm.toLowerCase()) ||
      llm.company.toLowerCase().includes(searchTerm.toLowerCase()) ||
      llm.description.toLowerCase().includes(searchTerm.toLowerCase())
    );
  
  // Create stable sorted order that doesn't change based on vote changes
  const stableSortedLLMs = [...filteredLLMs].sort((a, b) => {
    if (sortBy === 'name') {
      return a.name.localeCompare(b.name);
    } else if (sortBy === 'company') {
      return a.company.localeCompare(b.company);
    } else if (sortBy === 'votes') {
      // Use initial vote counts from data, not dynamic rankings
      const aInitialRank = llms.findIndex(llm => llm.id === a.id);
      const bInitialRank = llms.findIndex(llm => llm.id === b.id);
      return aInitialRank - bInitialRank;
    }
    return 0;
  });
  
  return (
    <div className="min-h-screen bg-background">
      <Toaster position="bottom-right" theme="dark" />
      <Header />
      
      <main className="container mx-auto px-4 py-8">
        {/* Hero Section */}
        <motion.div
          initial={{ opacity: 0, y: 20 }}
          animate={{ opacity: 1, y: 0 }}
          className="text-center mb-8"
        >
          <h2 className="text-3xl md:text-4xl font-bold text-foreground mb-4">
            Which LLM Rules in 2025?
          </h2>
          <p className="text-muted-foreground max-w-2xl mx-auto">
            Cast your vote for the AI models you love. Upvote your favorites, downvote the ones you don&apos;t prefer.
            Every vote counts in determining the community&apos;s choice!
          </p>
        </motion.div>
        
        {/* Stats Panel */}
        <StatsPanel />
        
        {/* Controls */}
        <div className="flex flex-col md:flex-row gap-4 mb-8">
          <div className="flex-1 relative">
            <Search className="absolute left-3 top-1/2 transform -translate-y-1/2 text-muted-foreground" size={20} />
            <input
              type="text"
              placeholder="Search LLMs..."
              value={searchTerm}
              onChange={(e) => setSearchTerm(e.target.value)}
              className="w-full pl-10 pr-4 py-2 bg-card border border-border rounded-lg text-foreground placeholder-muted-foreground focus:outline-none focus:border-primary transition-colors"
            />
          </div>
          
          <div className="flex gap-2">
            <select
              value={sortBy}
              onChange={(e) => setSortBy(e.target.value)}
              className="px-4 py-2 bg-card border border-border rounded-lg text-foreground focus:outline-none focus:border-primary transition-colors"
            >
              <option value="votes">Sort by Votes</option>
              <option value="name">Sort by Name</option>
              <option value="company">Sort by Company</option>
            </select>
            
            
          </div>
        </div>
        
        {/* Chart - Always visible */}
        <div className="mb-8">
          <VoteChart />
        </div>
        
        {/* LLM Grid/List */}
        {loading ? (
          <div className="flex items-center justify-center h-64">
            <motion.div
              animate={{ rotate: 360 }}
              transition={{ duration: 1, repeat: Infinity, ease: "linear" }}
              className="w-8 h-8 border-4 border-primary border-t-transparent rounded-full"
            />
          </div>
        ) : (
          <motion.div
            layout
            className="grid gap-4 grid-cols-2 md:grid-cols-3 lg:grid-cols-4 xl:grid-cols-5"
          >
            {stableSortedLLMs.map((llm, index) => (
              <LLMCard key={llm.id} llm={llm} index={index} />
            ))}
          </motion.div>
        )}
        
        {stableSortedLLMs.length === 0 && !loading && (
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            className="text-center py-16"
          >
            <p className="text-muted-foreground text-lg">
              No LLMs found matching your search.
            </p>
          </motion.div>
        )}
      </main>
      
      {/* Footer */}
      <footer className="border-t border-border mt-16 py-8">
        <div className="container mx-auto px-4 text-center">
          <p className="text-muted-foreground text-sm">
            Made with  by the AI Community | 2025
          </p>
          <p className="text-xs text-muted-foreground mt-2">
            Vote responsibly. Each user can upvote or downvote any model.
          </p>
        </div>
      </footer>
    </div>
  );
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\components\Header.jsx
'use client';

import { motion } from 'framer-motion';
import { Sparkles, Github } from 'lucide-react';

export default function Header() {
  return (
    <motion.header
      initial={{ opacity: 0, y: -20 }}
      animate={{ opacity: 1, y: 0 }}
      className="border-b border-border bg-card/50 backdrop-blur-sm sticky top-0 z-50"
    >
      <div className="container mx-auto px-4 py-4">
        <div className="flex items-center justify-between">
          <div className="flex items-center gap-3">
            <motion.div
              animate={{ rotate: 360 }}
              transition={{ duration: 20, repeat: Infinity, ease: "linear" }}
            >
              <Sparkles size={32} className="text-primary" />
            </motion.div>
            <div>
              <h1 className="text-2xl font-bold text-gradient">LLM Popularity Tracker</h1>
              <p className="text-xs text-muted-foreground">Vote for your favorite AI models</p>
            </div>
          </div>
          
          <div className="flex items-center gap-4">
            <motion.div
              whileHover={{ scale: 1.05 }}
              whileTap={{ scale: 0.95 }}
              className="text-sm bg-primary/10 text-primary px-4 py-2 rounded-full font-medium"
            >
              2025 Edition
            </motion.div>
            <motion.a
              href="https://github.com"
              target="_blank"
              rel="noopener noreferrer"
              whileHover={{ scale: 1.1 }}
              whileTap={{ scale: 0.9 }}
              className="text-muted-foreground hover:text-foreground transition-colors"
            >
              <Github size={20} />
            </motion.a>
          </div>
        </div>
      </div>
    </motion.header>
  );
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\components\LLMCard.jsx
'use client';

import { useState } from 'react';
import { motion } from 'framer-motion';
import { ChevronUp, ChevronDown, TrendingUp, X } from 'lucide-react';
import useVoteStore from '@/store/useVoteStore';
import { toast } from 'sonner';

export default function LLMCard({ llm, index }) {
  const [imageError, setImageError] = useState(false);
  const { vote, getUserVote, getVoteCount, isTrending, getRank } = useVoteStore();
  
  const userVote = getUserVote(llm.id);
  const voteCount = getVoteCount(llm.id);
  const trending = isTrending(llm.id);
  const rank = getRank(llm.id);
  
  const handleVote = async (voteType) => {
    console.log(' [CARD] Vote button clicked:', { 
      llm: llm.name, 
      llmId: llm.id, 
      voteType, 
      currentUserVote: userVote,
      currentVoteCount: voteCount 
    });
    
    if (voteType === 0) {
      // Clear vote
      console.log(' [CARD] Clearing vote for', llm.name);
      await vote(llm.id, 0);
      toast.success('Vote removed');
    } else if (userVote !== voteType) {
      // Only vote if it's different from current vote
      console.log(' [CARD] Casting new vote for', llm.name, ':', voteType === 1 ? 'UPVOTE' : 'DOWNVOTE');
      await vote(llm.id, voteType);
      toast.success(voteType === 1 ? 'Upvoted!' : 'Downvoted!');
    } else {
      console.log(' [CARD] Same vote clicked, ignoring:', { llm: llm.name, voteType });
    }
    // If clicking the same vote button, do nothing (no toggle)
  };
  
  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      transition={{ delay: index * 0.05 }}
      whileHover={{ scale: 1.02 }}
      className="relative bg-gradient-to-br from-card to-card/80 border border-border/50 rounded-xl p-5 shadow-lg hover:shadow-xl transition-all duration-300 backdrop-blur-sm"
    >
      {/* Rank Badge */}
      {rank && rank <= 5 && (
        <div className={`absolute -top-2 -right-2 w-7 h-7 rounded-full flex items-center justify-center text-xs font-bold shadow-lg
          ${rank === 1 ? 'bg-gradient-to-r from-yellow-400 to-yellow-500 text-black' : ''}
          ${rank === 2 ? 'bg-gradient-to-r from-gray-300 to-gray-400 text-black' : ''}
          ${rank === 3 ? 'bg-gradient-to-r from-orange-400 to-orange-600 text-white' : ''}
          ${rank === 4 ? 'bg-gradient-to-r from-blue-400 to-blue-500 text-white' : ''}
          ${rank === 5 ? 'bg-gradient-to-r from-green-400 to-green-500 text-white' : ''}
        `}>
          #{rank}
        </div>
      )}
      
      {/* Trending Badge */}
      {trending && (
        <motion.div
          initial={{ scale: 0, rotate: -10 }}
          animate={{ scale: 1, rotate: 0 }}
          className="absolute top-2 left-2 bg-gradient-to-r from-red-500 to-pink-500 text-white px-2 py-1 rounded-full text-xs font-bold flex items-center gap-1 shadow-md"
        >
          <TrendingUp size={10} />
          HOT
        </motion.div>
      )}
      
      {/* Header with Logo and Info */}
      <div className="flex items-start gap-4 mb-5">
        <div className="flex-shrink-0 w-12 h-12 rounded-lg overflow-hidden bg-white/10 backdrop-blur-sm border border-white/20 flex items-center justify-center">
          {llm.image && !imageError ? (
            <img 
              src={llm.image} 
              alt={`${llm.company} logo`}
              className="w-10 h-10 object-contain"
              onError={() => setImageError(true)}
            />
          ) : (
            <span className="text-xl">{llm.logo}</span>
          )}
        </div>
        <div className="flex-1 min-w-0">
          <h3 className="text-lg font-bold text-foreground truncate">{llm.name}</h3>
          <p className="text-sm text-muted-foreground/80 font-medium">{llm.company}</p>
          <p className="text-xs text-muted-foreground/60 mt-1">{llm.releaseYear}</p>
        </div>
      </div>
      
      {/* Vote Section */}
      <div className="flex items-center justify-between bg-black/20 backdrop-blur-sm rounded-lg p-3">
        <motion.button
          whileTap={userVote !== 1 ? { scale: 0.95 } : {}}
          whileHover={userVote !== 1 ? { scale: 1.05 } : {}}
          onClick={() => handleVote(1)}
          disabled={userVote === 1}
          className={`flex items-center justify-center w-10 h-10 rounded-lg transition-all shadow-md ${
            userVote === 1
              ? 'bg-gradient-to-r from-green-500 to-emerald-500 text-white shadow-green-500/30 cursor-default opacity-100'
              : 'bg-white/10 hover:bg-green-500/20 text-green-400 hover:text-green-300 border border-green-400/30 cursor-pointer'
          }`}
          aria-label="Upvote"
          title={userVote === 1 ? "You upvoted this" : "Upvote"}
        >
          <ChevronUp size={20} strokeWidth={2.5} />
        </motion.button>
        
        <div className="flex items-center gap-2">
          {/* Vote count */}
          <div className="flex flex-col items-center">
            <span className={`text-xl font-bold ${
              voteCount > 0 ? 'text-green-400' : voteCount < 0 ? 'text-red-400' : 'text-gray-400'
            }`}>
              {voteCount > 0 ? '+' : ''}{voteCount}
            </span>
            <span className="text-xs text-muted-foreground/60">votes</span>
          </div>
          
          {/* Clear vote button - only show if user has voted */}
          {userVote !== 0 && (
            <motion.button
              initial={{ scale: 0, opacity: 0 }}
              animate={{ scale: 1, opacity: 1 }}
              exit={{ scale: 0, opacity: 0 }}
              whileTap={{ scale: 0.9 }}
              whileHover={{ scale: 1.1 }}
              onClick={() => handleVote(0)}
              className="flex items-center justify-center w-8 h-8 rounded-full bg-white/10 hover:bg-white/20 text-gray-400 hover:text-white border border-gray-400/30 transition-all"
              aria-label="Clear vote"
              title="Clear vote"
            >
              <X size={16} strokeWidth={2.5} />
            </motion.button>
          )}
        </div>
        
        <motion.button
          whileTap={userVote !== -1 ? { scale: 0.95 } : {}}
          whileHover={userVote !== -1 ? { scale: 1.05 } : {}}
          onClick={() => handleVote(-1)}
          disabled={userVote === -1}
          className={`flex items-center justify-center w-10 h-10 rounded-lg transition-all shadow-md ${
            userVote === -1
              ? 'bg-gradient-to-r from-red-500 to-rose-500 text-white shadow-red-500/30 cursor-default opacity-100'
              : 'bg-white/10 hover:bg-red-500/20 text-red-400 hover:text-red-300 border border-red-400/30 cursor-pointer'
          }`}
          aria-label="Downvote"
          title={userVote === -1 ? "You downvoted this" : "Downvote"}
        >
          <ChevronDown size={20} strokeWidth={2.5} />
        </motion.button>
      </div>
    </motion.div>
  );
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\components\StatsPanel.jsx
'use client';

import { motion } from 'framer-motion';
import { TrendingUp, Users, Clock, Trophy, Activity } from 'lucide-react';
import useVoteStore from '@/store/useVoteStore';

export default function StatsPanel() {
  const { stats } = useVoteStore();
  
  const statCards = [
    {
      icon: Users,
      label: 'Total Votes',
      value: stats.totalVotes || 0,
      color: 'text-primary',
      bgColor: 'bg-primary/10',
    },
    {
      icon: Activity,
      label: 'Votes Today',
      value: stats.votesToday || 0,
      color: 'text-success',
      bgColor: 'bg-success/10',
    },
    {
      icon: Clock,
      label: 'Last Hour',
      value: stats.votesLastHour || 0,
      color: 'text-accent',
      bgColor: 'bg-accent/10',
    },
    {
      icon: Trophy,
      label: 'Leading',
      value: stats.topModel || 'None',
      color: 'text-yellow-500',
      bgColor: 'bg-yellow-500/10',
      small: true,
    },
  ];
  
  return (
    <div className="grid grid-cols-2 md:grid-cols-4 gap-4 mb-8">
      {statCards.map((stat, index) => (
        <motion.div
          key={stat.label}
          initial={{ opacity: 0, scale: 0.9 }}
          animate={{ opacity: 1, scale: 1 }}
          transition={{ delay: index * 0.1 }}
          className="bg-card border border-border rounded-lg p-4"
        >
          <div className="flex items-center justify-between mb-2">
            <div className={`p-2 rounded-lg ${stat.bgColor}`}>
              <stat.icon size={20} className={stat.color} />
            </div>
            {stat.label === 'Last Hour' && stats.votesLastHour > 0 && (
              <TrendingUp size={16} className="text-success" />
            )}
          </div>
          <p className="text-xs text-muted-foreground mb-1">{stat.label}</p>
          <p className={`${stat.small ? 'text-lg' : 'text-2xl'} font-bold text-foreground`}>
            {typeof stat.value === 'number' ? stat.value.toLocaleString() : stat.value}
          </p>
        </motion.div>
      ))}
    </div>
  );
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\components\VoteChart.jsx
'use client';

import { useEffect, useState } from 'react';
import { motion } from 'framer-motion';
import {
  BarChart,
  Bar,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ResponsiveContainer,
  Cell,
} from 'recharts';
import useVoteStore from '@/store/useVoteStore';
import { BarChart3, PieChart } from 'lucide-react';

export default function VoteChart() {
  const [chartType, setChartType] = useState('bar');
  const { votes, llms } = useVoteStore();
  
  // Prepare chart data - show ALL LLMs with their vote counts
  const chartData = llms.map((llm) => {
    const voteCount = votes[llm.id] || 0;
    return {
      name: llm.name,
      votes: voteCount,
      color: llm.color || 'from-gray-500 to-gray-600',
      id: llm.id,
    };
  }).sort((a, b) => b.votes - a.votes); // Sort by votes descending
  
  // Extract gradient colors for bars
  const getBarColor = (color) => {
    const colors = color.match(/from-(\w+)-\d+\sto-(\w+)-\d+/);
    if (colors) {
      const colorMap = {
        green: '#10b981',
        emerald: '#10b981',
        orange: '#f59e0b',
        amber: '#f59e0b',
        blue: '#3b82f6',
        cyan: '#06b6d4',
        purple: '#8b5cf6',
        violet: '#8b5cf6',
        red: '#ef4444',
        pink: '#ec4899',
        indigo: '#6366f1',
        gray: '#6b7280',
        slate: '#64748b',
        teal: '#14b8a6',
        yellow: '#eab308',
      };
      return colorMap[colors[1]] || '#6b7280';
    }
    return '#6b7280';
  };
  
  const CustomTooltip = ({ active, payload }) => {
    if (active && payload && payload[0]) {
      return (
        <div className="bg-card border border-border rounded-lg p-3 shadow-lg">
          <p className="font-bold text-foreground">{payload[0].payload.name}</p>
          <p className="text-sm text-muted-foreground">
            Votes: <span className="text-primary font-bold">{payload[0].value}</span>
          </p>
        </div>
      );
    }
    return null;
  };
  
  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className="bg-card border border-border rounded-lg p-6"
    >
      <div className="flex items-center justify-between mb-6">
        <h2 className="text-xl font-bold text-foreground flex items-center gap-2">
          <BarChart3 size={24} className="text-primary" />
          All LLM Votes
        </h2>
        <div className="flex gap-2">
          <button
            onClick={() => setChartType('bar')}
            className={`p-2 rounded-lg transition-all ${
              chartType === 'bar' ? 'bg-primary text-white' : 'bg-card-hover text-muted-foreground'
            }`}
          >
            <BarChart3 size={16} />
          </button>
        </div>
      </div>
      
      <ResponsiveContainer width="100%" height={500}>
        <BarChart data={chartData} margin={{ top: 20, right: 30, left: 20, bottom: 100 }}>
          <CartesianGrid strokeDasharray="3 3" stroke="#27272a" />
          <XAxis
            dataKey="name"
            angle={-45}
            textAnchor="end"
            height={120}
            tick={{ fill: '#9ca3af', fontSize: 10 }}
            stroke="#27272a"
            interval={0}
          />
          <YAxis
            tick={{ fill: '#9ca3af' }}
            stroke="#27272a"
            domain={['dataMin - 1', 'dataMax + 1']}
          />
          <Tooltip content={<CustomTooltip />} />
          <Bar dataKey="votes" radius={[4, 4, 0, 0]}>
            {chartData.map((entry, index) => (
              <Cell key={`cell-${index}`} fill={getBarColor(entry.color)} />
            ))}
          </Bar>
        </BarChart>
      </ResponsiveContainer>
    </motion.div>
  );
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\config\pgbouncer.ini
[databases]
; Database connection settings
llm_tracker = host=postgres port=5432 dbname=llm_tracker

; Connection pool for read replicas
llm_tracker_read = host=postgres-read1 port=5432 dbname=llm_tracker
llm_tracker_read2 = host=postgres-read2 port=5432 dbname=llm_tracker

[pgbouncer]
; Connection pool settings for 200k users
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt

; Pool configuration
pool_mode = transaction
max_client_conn = 10000
default_pool_size = 25
min_pool_size = 10
reserve_pool_size = 5
reserve_pool_timeout = 3
max_db_connections = 100
max_user_connections = 100

; Performance tuning
server_idle_timeout = 600
server_lifetime = 3600
server_connect_timeout = 15
server_login_retry = 15
query_wait_timeout = 120
client_idle_timeout = 0
client_login_timeout = 60

; DNS settings
dns_max_ttl = 15
dns_nxdomain_ttl = 15

; Logging
logfile = /var/log/pgbouncer/pgbouncer.log
pidfile = /var/run/pgbouncer/pgbouncer.pid
admin_users = postgres
stats_users = stats, postgres
verbose = 0

; Security
server_tls_sslmode = prefer
server_tls_ca_file = /etc/pgbouncer/root.crt
server_tls_protocols = TLSv1.2,TLSv1.3

; Connection limits per user
; Adjust based on your user patterns
ignore_startup_parameters = extra_float_digits

; Stats collection
stats_period = 60

; Unix socket settings
unix_socket_dir = /var/run/pgbouncer
unix_socket_mode = 0755

; Buffer sizes
pkt_buf = 4096
sbuf_loopcnt = 5
tcp_defer_accept = 45
tcp_socket_buffer = 0
tcp_keepalive = 1
tcp_keepcnt = 9
tcp_keepidle = 7200
tcp_keepintvl = 75

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\config\postgres-optimization.sql
-- PostgreSQL Performance Optimization for 200k+ concurrent users
-- Execute these queries to optimize database performance

-- 1. Create compound indexes for frequently queried columns
CREATE INDEX IF NOT EXISTS idx_user_votes_fingerprint_llm ON user_votes(fingerprint, llm_id);
CREATE INDEX IF NOT EXISTS idx_user_votes_created_at ON user_votes(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_user_votes_llm_created ON user_votes(llm_id, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_votes_vote_count ON votes(vote_count DESC);
CREATE INDEX IF NOT EXISTS idx_analytics_event_created ON analytics(event_type, created_at DESC);

-- 2. Create partial indexes for common WHERE conditions
CREATE INDEX IF NOT EXISTS idx_user_votes_non_zero ON user_votes(llm_id, created_at) 
WHERE vote_type != 0;

-- 3. Enable query statistics tracking
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- 4. Table partitioning for user_votes by month
-- First, recreate user_votes as a partitioned table
ALTER TABLE user_votes RENAME TO user_votes_old;

CREATE TABLE user_votes (
    id SERIAL,
    fingerprint VARCHAR(255) NOT NULL,
    llm_id VARCHAR(100) NOT NULL,
    vote_type SMALLINT NOT NULL DEFAULT 0,
    previous_vote SMALLINT DEFAULT 0,
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- Create partitions for the next 12 months
CREATE TABLE user_votes_2025_08 PARTITION OF user_votes
    FOR VALUES FROM ('2025-08-01') TO ('2025-09-01');
CREATE TABLE user_votes_2025_09 PARTITION OF user_votes
    FOR VALUES FROM ('2025-09-01') TO ('2025-10-01');
CREATE TABLE user_votes_2025_10 PARTITION OF user_votes
    FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');
CREATE TABLE user_votes_2025_11 PARTITION OF user_votes
    FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');
CREATE TABLE user_votes_2025_12 PARTITION OF user_votes
    FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');
CREATE TABLE user_votes_2026_01 PARTITION OF user_votes
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
CREATE TABLE user_votes_2026_02 PARTITION OF user_votes
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');
CREATE TABLE user_votes_2026_03 PARTITION OF user_votes
    FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');
CREATE TABLE user_votes_2026_04 PARTITION OF user_votes
    FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');
CREATE TABLE user_votes_2026_05 PARTITION OF user_votes
    FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');
CREATE TABLE user_votes_2026_06 PARTITION OF user_votes
    FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');
CREATE TABLE user_votes_2026_07 PARTITION OF user_votes
    FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');

-- Migrate data from old table
INSERT INTO user_votes SELECT * FROM user_votes_old;

-- Create indexes on each partition
CREATE INDEX idx_user_votes_2025_08_fingerprint ON user_votes_2025_08(fingerprint);
CREATE INDEX idx_user_votes_2025_09_fingerprint ON user_votes_2025_09(fingerprint);
CREATE INDEX idx_user_votes_2025_10_fingerprint ON user_votes_2025_10(fingerprint);
CREATE INDEX idx_user_votes_2025_11_fingerprint ON user_votes_2025_11(fingerprint);
CREATE INDEX idx_user_votes_2025_12_fingerprint ON user_votes_2025_12(fingerprint);
CREATE INDEX idx_user_votes_2026_01_fingerprint ON user_votes_2026_01(fingerprint);
CREATE INDEX idx_user_votes_2026_02_fingerprint ON user_votes_2026_02(fingerprint);
CREATE INDEX idx_user_votes_2026_03_fingerprint ON user_votes_2026_03(fingerprint);
CREATE INDEX idx_user_votes_2026_04_fingerprint ON user_votes_2026_04(fingerprint);
CREATE INDEX idx_user_votes_2026_05_fingerprint ON user_votes_2026_05(fingerprint);
CREATE INDEX idx_user_votes_2026_06_fingerprint ON user_votes_2026_06(fingerprint);
CREATE INDEX idx_user_votes_2026_07_fingerprint ON user_votes_2026_07(fingerprint);

-- Drop old table
DROP TABLE user_votes_old;

-- 5. Create materialized views for vote summaries
CREATE MATERIALIZED VIEW IF NOT EXISTS mv_vote_summary AS
SELECT 
    l.id as llm_id,
    COALESCE(v.vote_count, 0) as vote_count,
    COALESCE(v.positive_votes, 0) as positive_votes,
    COALESCE(v.negative_votes, 0) as negative_votes,
    RANK() OVER (ORDER BY COALESCE(v.vote_count, 0) DESC) as rank,
    NOW() as last_refreshed
FROM llms l
LEFT JOIN votes v ON l.id = v.llm_id
WITH DATA;

CREATE UNIQUE INDEX idx_mv_vote_summary_llm ON mv_vote_summary(llm_id);
CREATE INDEX idx_mv_vote_summary_rank ON mv_vote_summary(rank);

-- 6. Create materialized view for hourly statistics
CREATE MATERIALIZED VIEW IF NOT EXISTS mv_hourly_stats AS
SELECT 
    DATE_TRUNC('hour', created_at) as hour,
    llm_id,
    COUNT(*) as vote_count,
    COUNT(CASE WHEN vote_type = 1 THEN 1 END) as upvotes,
    COUNT(CASE WHEN vote_type = -1 THEN 1 END) as downvotes
FROM user_votes
WHERE created_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE_TRUNC('hour', created_at), llm_id
WITH DATA;

CREATE INDEX idx_mv_hourly_stats_hour ON mv_hourly_stats(hour DESC);
CREATE INDEX idx_mv_hourly_stats_llm ON mv_hourly_stats(llm_id, hour DESC);

-- 7. Create function for automatic materialized view refresh
CREATE OR REPLACE FUNCTION refresh_materialized_views()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_vote_summary;
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_hourly_stats;
END;
$$ LANGUAGE plpgsql;

-- 8. Create triggers for vote count caching
CREATE OR REPLACE FUNCTION update_vote_counts()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        -- New vote
        INSERT INTO votes (llm_id, vote_count, positive_votes, negative_votes)
        VALUES (
            NEW.llm_id,
            NEW.vote_type,
            CASE WHEN NEW.vote_type = 1 THEN 1 ELSE 0 END,
            CASE WHEN NEW.vote_type = -1 THEN 1 ELSE 0 END
        )
        ON CONFLICT (llm_id) DO UPDATE
        SET 
            vote_count = votes.vote_count + NEW.vote_type,
            positive_votes = votes.positive_votes + CASE WHEN NEW.vote_type = 1 THEN 1 ELSE 0 END,
            negative_votes = votes.negative_votes + CASE WHEN NEW.vote_type = -1 THEN 1 ELSE 0 END,
            updated_at = NOW();
    ELSIF TG_OP = 'UPDATE' THEN
        -- Vote changed
        UPDATE votes
        SET 
            vote_count = vote_count - OLD.vote_type + NEW.vote_type,
            positive_votes = positive_votes 
                - CASE WHEN OLD.vote_type = 1 THEN 1 ELSE 0 END
                + CASE WHEN NEW.vote_type = 1 THEN 1 ELSE 0 END,
            negative_votes = negative_votes 
                - CASE WHEN OLD.vote_type = -1 THEN 1 ELSE 0 END
                + CASE WHEN NEW.vote_type = -1 THEN 1 ELSE 0 END,
            updated_at = NOW()
        WHERE llm_id = NEW.llm_id;
    ELSIF TG_OP = 'DELETE' THEN
        -- Vote removed
        UPDATE votes
        SET 
            vote_count = vote_count - OLD.vote_type,
            positive_votes = positive_votes - CASE WHEN OLD.vote_type = 1 THEN 1 ELSE 0 END,
            negative_votes = negative_votes - CASE WHEN OLD.vote_type = -1 THEN 1 ELSE 0 END,
            updated_at = NOW()
        WHERE llm_id = OLD.llm_id;
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS trigger_update_vote_counts ON user_votes;
CREATE TRIGGER trigger_update_vote_counts
    AFTER INSERT OR UPDATE OR DELETE ON user_votes
    FOR EACH ROW
    EXECUTE FUNCTION update_vote_counts();

-- 9. Vacuum and analyze all tables
VACUUM ANALYZE user_votes;
VACUUM ANALYZE votes;
VACUUM ANALYZE user_sessions;
VACUUM ANALYZE analytics;

-- 10. Update table statistics
ANALYZE;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\config\postgresql.conf
# PostgreSQL Configuration for 200k+ concurrent users
# Optimized for high-concurrency voting application

# Memory Configuration (for 16GB RAM server)
shared_buffers = 4GB              # 25% of system RAM
effective_cache_size = 12GB       # 75% of system RAM
work_mem = 16MB                   # Per operation memory
maintenance_work_mem = 1GB        # For maintenance operations
wal_buffers = 16MB

# Connection Settings (with PgBouncer)
max_connections = 500              # PgBouncer will handle pooling
superuser_reserved_connections = 5

# Write Performance
checkpoint_completion_target = 0.9
checkpoint_timeout = 15min
max_wal_size = 4GB
min_wal_size = 1GB
wal_compression = on
wal_log_hints = on
synchronous_commit = off          # Acceptable for vote tracking

# Query Planner
random_page_cost = 1.1            # SSD optimized
effective_io_concurrency = 200    # SSD optimized
default_statistics_target = 200   # Better query plans

# Parallel Query Execution
max_worker_processes = 16
max_parallel_workers_per_gather = 8
max_parallel_workers = 16
parallel_setup_cost = 1000
parallel_tuple_cost = 0.1

# Logging and Monitoring
log_destination = 'csvlog'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_duration_statement = 500  # Log slow queries > 500ms
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0
log_autovacuum_min_duration = 0
log_line_prefix = '%t [%p]: [%l-1] db=%d,user=%u,app=%a,client=%h '

# Statistics
track_activities = on
track_counts = on
track_io_timing = on
track_functions = all
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all

# Autovacuum (aggressive for high write load)
autovacuum = on
autovacuum_max_workers = 6
autovacuum_naptime = 30s
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.02
autovacuum_analyze_scale_factor = 0.01
autovacuum_vacuum_cost_delay = 2ms
autovacuum_vacuum_cost_limit = 1000

# Lock Management
deadlock_timeout = 1s
max_locks_per_transaction = 128

# Background Writer
bgwriter_delay = 200ms
bgwriter_lru_maxpages = 200
bgwriter_lru_multiplier = 4.0

# Replication Settings (for read replicas)
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
hot_standby = on
hot_standby_feedback = on

# Statement Timeout Protection
statement_timeout = 30s
idle_in_transaction_session_timeout = 60s

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\config\redis.conf
# Redis Configuration for 200k+ concurrent users
# Optimized for high-concurrency voting application

# Network and basic settings
bind 0.0.0.0
protected-mode yes
port 6379
tcp-backlog 511
tcp-keepalive 300
timeout 0

# Memory management
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# Persistence settings
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /data

# AOF persistence for better durability
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes

# Replication settings
replica-read-only yes
replica-serve-stale-data yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-ping-replica-period 10
repl-timeout 60
repl-disable-tcp-nodelay no
repl-backlog-size 64mb
repl-backlog-ttl 3600

# Client handling
maxclients 50000

# Slow log
slowlog-log-slower-than 10000
slowlog-max-len 128

# Latency monitoring
latency-monitor-threshold 100

# Event notification
notify-keyspace-events ""

# Advanced config
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes

# Security
requirepass ${REDIS_PASSWORD}

# Modules
# loadmodule /usr/lib/redis/modules/redisearch.so
# loadmodule /usr/lib/redis/modules/redistimeseries.so

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\config\sentinel1.conf
# Redis Sentinel Configuration - Node 1
port 26379
bind 0.0.0.0
protected-mode no

# Sentinel monitoring
sentinel monitor mymaster redis-primary 6379 2
sentinel auth-pass mymaster ${REDIS_PASSWORD}
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 60000
sentinel parallel-syncs mymaster 1

# Logging
logfile /var/log/redis-sentinel.log
loglevel notice

# Working directory
dir /tmp

# Sentinel configuration rewrite
sentinel deny-scripts-reconfig yes

# Notification scripts (optional)
# sentinel notification-script mymaster /var/redis/notify.sh
# sentinel client-reconfig-script mymaster /var/redis/reconfig.sh

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\hooks\useWebSocket.js
'use client';

import { useEffect, useRef, useState, useCallback } from 'react';
import io from 'socket.io-client';
import { useVoteStore } from '@/store/useVoteStore';
import { toast } from 'sonner';

const WEBSOCKET_URL = process.env.NEXT_PUBLIC_WS_URL || 
  (typeof window !== 'undefined' ? window.location.origin : 'http://localhost:3000');

export function useWebSocket(fingerprint) {
  const socketRef = useRef(null);
  const [connected, setConnected] = useState(false);
  const [latency, setLatency] = useState(null);
  const reconnectAttempts = useRef(0);
  const maxReconnectAttempts = 5;
  const pingInterval = useRef(null);
  
  const updateVotes = useVoteStore((state) => state.updateVotes);
  const updateUserVotes = useVoteStore((state) => state.updateUserVotes);
  const updateRankings = useVoteStore((state) => state.updateRankings);
  const updateStats = useVoteStore((state) => state.updateStats);

  // Initialize WebSocket connection
  const connect = useCallback(() => {
    if (socketRef.current?.connected) return;

    const socket = io(WEBSOCKET_URL, {
      transports: ['websocket', 'polling'],
      auth: {
        fingerprint: fingerprint || 'anonymous'
      },
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionDelayMax: 5000,
      reconnectionAttempts: maxReconnectAttempts
    });

    // Connection event handlers
    socket.on('connect', () => {
      console.log('WebSocket connected');
      setConnected(true);
      reconnectAttempts.current = 0;
      
      // Subscribe to global updates
      socket.emit('subscribe', { type: 'all', id: 'global' });
      
      // Start ping interval for latency measurement
      startPingInterval();
    });

    socket.on('disconnect', (reason) => {
      console.log('WebSocket disconnected:', reason);
      setConnected(false);
      stopPingInterval();
      
      if (reason === 'io server disconnect') {
        // Server initiated disconnect, attempt reconnection
        setTimeout(() => connect(), 1000);
      }
    });

    socket.on('connect_error', (error) => {
      console.error('Connection error:', error.message);
      reconnectAttempts.current++;
      
      if (reconnectAttempts.current >= maxReconnectAttempts) {
        console.error('Max reconnection attempts reached');
        toast.error('Connection to server lost. Please refresh the page.');
      }
    });

    // Data event handlers
    socket.on('initialData', (data) => {
      console.log('Received initial data');
      if (data.votes) updateVotes(data.votes);
      if (data.userVotes) updateUserVotes(data.userVotes);
      if (data.rankings) updateRankings(data.rankings);
      if (data.stats) updateStats(data.stats);
    });

    socket.on('voteUpdate', (data) => {
      console.log('Vote update received:', data);
      // Update specific vote count
      if (data.llmId && data.voteCount !== undefined) {
        updateVotes({ [data.llmId]: data.voteCount });
      }
    });

    socket.on('rankingsUpdate', (data) => {
      console.log('Rankings update received');
      if (data.rankings) {
        updateRankings(data.rankings);
      }
    });

    socket.on('statsUpdate', (data) => {
      console.log('Stats update received');
      if (data.stats) {
        updateStats(data.stats);
      }
    });

    socket.on('voteConfirmed', (data) => {
      console.log('Vote confirmed:', data);
      toast.success('Vote recorded successfully');
    });

    socket.on('voteError', (data) => {
      console.error('Vote error:', data);
      toast.error(data.error || 'Failed to record vote');
    });

    socket.on('error', (error) => {
      console.error('WebSocket error:', error);
      toast.error(error.message || 'Connection error occurred');
    });

    socket.on('serverShutdown', (data) => {
      console.warn('Server shutdown notification:', data);
      toast.warning('Server is restarting. Please wait...');
    });

    socketRef.current = socket;
  }, [fingerprint, updateVotes, updateUserVotes, updateRankings, updateStats]);

  // Disconnect WebSocket
  const disconnect = useCallback(() => {
    if (socketRef.current) {
      socketRef.current.disconnect();
      socketRef.current = null;
      setConnected(false);
      stopPingInterval();
    }
  }, []);

  // Send vote through WebSocket
  const sendVote = useCallback((llmId, voteType) => {
    if (!socketRef.current?.connected) {
      console.warn('WebSocket not connected, vote will be sent via HTTP');
      return false;
    }

    socketRef.current.emit('vote', { llmId, voteType });
    return true;
  }, []);

  // Subscribe to specific updates
  const subscribe = useCallback((type, id) => {
    if (!socketRef.current?.connected) {
      console.warn('WebSocket not connected');
      return;
    }

    socketRef.current.emit('subscribe', { type, id });
  }, []);

  // Unsubscribe from updates
  const unsubscribe = useCallback((type, id) => {
    if (!socketRef.current?.connected) {
      console.warn('WebSocket not connected');
      return;
    }

    socketRef.current.emit('unsubscribe', { type, id });
  }, []);

  // Request data sync
  const requestSync = useCallback(() => {
    if (!socketRef.current?.connected) {
      console.warn('WebSocket not connected');
      return Promise.reject(new Error('Not connected'));
    }

    return new Promise((resolve, reject) => {
      socketRef.current.emit('sync', (response) => {
        if (response.success) {
          if (response.data) {
            if (response.data.votes) updateVotes(response.data.votes);
            if (response.data.userVotes) updateUserVotes(response.data.userVotes);
            if (response.data.rankings) updateRankings(response.data.rankings);
            if (response.data.stats) updateStats(response.data.stats);
          }
          resolve(response);
        } else {
          reject(new Error(response.error || 'Sync failed'));
        }
      });
    });
  }, [updateVotes, updateUserVotes, updateRankings, updateStats]);

  // Get connection stats
  const getStats = useCallback(() => {
    if (!socketRef.current?.connected) {
      return Promise.reject(new Error('Not connected'));
    }

    return new Promise((resolve, reject) => {
      socketRef.current.emit('getStats', (response) => {
        if (response.success) {
          resolve(response);
        } else {
          reject(new Error(response.error || 'Failed to get stats'));
        }
      });
    });
  }, []);

  // Ping for latency measurement
  const startPingInterval = useCallback(() => {
    if (pingInterval.current) return;

    pingInterval.current = setInterval(() => {
      if (socketRef.current?.connected) {
        const start = Date.now();
        socketRef.current.emit('ping', (response) => {
          const roundTrip = Date.now() - start;
          setLatency(roundTrip);
        });
      }
    }, 10000); // Every 10 seconds
  }, []);

  const stopPingInterval = useCallback(() => {
    if (pingInterval.current) {
      clearInterval(pingInterval.current);
      pingInterval.current = null;
    }
  }, []);

  // Effect to manage connection lifecycle
  useEffect(() => {
    if (fingerprint) {
      connect();
    }

    return () => {
      disconnect();
    };
  }, [fingerprint, connect, disconnect]);

  // Reconnection on visibility change
  useEffect(() => {
    const handleVisibilityChange = () => {
      if (!document.hidden && fingerprint && !socketRef.current?.connected) {
        console.log('Page visible, attempting reconnection');
        connect();
      }
    };

    document.addEventListener('visibilitychange', handleVisibilityChange);
    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange);
    };
  }, [fingerprint, connect]);

  // Reconnection on online status change
  useEffect(() => {
    const handleOnline = () => {
      if (fingerprint && !socketRef.current?.connected) {
        console.log('Network online, attempting reconnection');
        connect();
      }
    };

    window.addEventListener('online', handleOnline);
    return () => {
      window.removeEventListener('online', handleOnline);
    };
  }, [fingerprint, connect]);

  return {
    connected,
    latency,
    sendVote,
    subscribe,
    unsubscribe,
    requestSync,
    getStats,
    connect,
    disconnect
  };
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\cache-enhanced.js
import scaledDbManager from './database-scaled.js';
import logger from './logger.js';

class EnhancedCacheManager {
  constructor() {
    this.redis = null;
    this.memoryCache = new Map();
    this.memoryCacheTTL = 60000; // 1 minute for L1 cache
    this.defaultTTL = 3600; // 1 hour for L2 cache
    this.pubsubClient = null;
    this.subscribers = new Map();
  }

  async initialize() {
    try {
      // Get Redis clients for different purposes
      this.redis = scaledDbManager.getRedis('cache');
      this.pubsubClient = scaledDbManager.getRedis('pubsub');
      
      // Setup cleanup interval for memory cache
      setInterval(() => this.cleanupMemoryCache(), 30000);
      
      logger.info('Enhanced Cache Manager initialized');
    } catch (error) {
      logger.error('Failed to initialize Enhanced Cache Manager:', error);
      throw error;
    }
  }

  // L1 Cache (Memory) operations
  getFromMemory(key) {
    const cached = this.memoryCache.get(key);
    if (!cached) return null;
    
    if (Date.now() > cached.expiry) {
      this.memoryCache.delete(key);
      return null;
    }
    
    return cached.value;
  }

  setInMemory(key, value, ttl = this.memoryCacheTTL) {
    this.memoryCache.set(key, {
      value,
      expiry: Date.now() + ttl
    });
  }

  cleanupMemoryCache() {
    const now = Date.now();
    for (const [key, data] of this.memoryCache.entries()) {
      if (now > data.expiry) {
        this.memoryCache.delete(key);
      }
    }
  }

  // Multi-layer cache operations
  async getWithLayers(key) {
    // Check L1 (Memory)
    const memoryValue = this.getFromMemory(key);
    if (memoryValue !== null) {
      return { value: memoryValue, source: 'memory' };
    }
    
    // Check L2 (Redis)
    try {
      const redisValue = await this.redis.get(key);
      if (redisValue) {
        const parsed = JSON.parse(redisValue);
        // Populate L1 cache
        this.setInMemory(key, parsed);
        return { value: parsed, source: 'redis' };
      }
    } catch (error) {
      logger.error('Redis get failed:', error);
    }
    
    return { value: null, source: null };
  }

  async setWithLayers(key, value, options = {}) {
    const { memoryTTL = this.memoryCacheTTL, redisTTL = this.defaultTTL } = options;
    
    // Set in L1 (Memory)
    this.setInMemory(key, value, memoryTTL);
    
    // Set in L2 (Redis)
    try {
      await this.redis.setex(key, redisTTL, JSON.stringify(value));
    } catch (error) {
      logger.error('Redis set failed:', error);
    }
  }

  // Vote counting cache with optimizations
  async getVoteCount(llmId) {
    const { value } = await this.getWithLayers(`vote_count:${llmId}`);
    return value ? parseInt(value, 10) : null;
  }

  async setVoteCount(llmId, count, ttl = 300) {
    await this.setWithLayers(`vote_count:${llmId}`, count, {
      memoryTTL: 60000,
      redisTTL: ttl
    });
  }

  async incrementVoteCount(llmId, increment = 1) {
    try {
      const newCount = await this.redis.incrby(`vote_count:${llmId}`, increment);
      await this.redis.expire(`vote_count:${llmId}`, 300);
      // Update memory cache
      this.setInMemory(`vote_count:${llmId}`, newCount);
      return newCount;
    } catch (error) {
      logger.error('Redis increment vote count failed:', error);
      return null;
    }
  }

  // All votes cache
  async getAllVotes() {
    const { value } = await this.getWithLayers('all_votes');
    return value;
  }

  async setAllVotes(votesData, ttl = 60) {
    await this.setWithLayers('all_votes', votesData, {
      memoryTTL: 30000,
      redisTTL: ttl
    });
  }

  // User session cache
  async getUserSession(fingerprint) {
    const { value } = await this.getWithLayers(`user_session:${fingerprint}`);
    return value;
  }

  async setUserSession(fingerprint, sessionData, ttl = 86400) {
    await this.setWithLayers(`user_session:${fingerprint}`, sessionData, {
      memoryTTL: 300000, // 5 minutes
      redisTTL: ttl
    });
  }

  // User votes cache with batching
  async getUserVotes(fingerprint) {
    const { value } = await this.getWithLayers(`user_votes:${fingerprint}`);
    if (value) return value;
    
    // Try Redis hash for individual votes
    try {
      const cached = await this.redis.hgetall(`user_votes:${fingerprint}`);
      const votes = {};
      for (const [llmId, voteType] of Object.entries(cached)) {
        votes[llmId] = parseInt(voteType, 10);
      }
      
      if (Object.keys(votes).length > 0) {
        // Cache in memory
        this.setInMemory(`user_votes:${fingerprint}`, votes);
        return votes;
      }
    } catch (error) {
      logger.error('Redis get user votes failed:', error);
    }
    
    return null;
  }

  async setUserVote(fingerprint, llmId, voteType) {
    try {
      // Update Redis hash
      if (voteType === 0) {
        await this.redis.hdel(`user_votes:${fingerprint}`, llmId);
      } else {
        await this.redis.hset(`user_votes:${fingerprint}`, llmId, voteType.toString());
      }
      await this.redis.expire(`user_votes:${fingerprint}`, 86400);
      
      // Invalidate memory cache
      this.memoryCache.delete(`user_votes:${fingerprint}`);
    } catch (error) {
      logger.error('Redis set user vote failed:', error);
    }
  }

  async setAllUserVotes(fingerprint, votes, ttl = 86400) {
    try {
      const pipeline = this.redis.pipeline();
      pipeline.del(`user_votes:${fingerprint}`);
      
      for (const [llmId, voteType] of Object.entries(votes)) {
        if (voteType !== 0) {
          pipeline.hset(`user_votes:${fingerprint}`, llmId, voteType.toString());
        }
      }
      
      pipeline.expire(`user_votes:${fingerprint}`, ttl);
      await pipeline.exec();
      
      // Update memory cache
      this.setInMemory(`user_votes:${fingerprint}`, votes);
    } catch (error) {
      logger.error('Redis set all user votes failed:', error);
    }
  }

  // Rankings cache
  async getRankings() {
    const { value } = await this.getWithLayers('rankings');
    return value;
  }

  async setRankings(rankings, ttl = 60) {
    await this.setWithLayers('rankings', rankings, {
      memoryTTL: 30000,
      redisTTL: ttl
    });
  }

  // Statistics cache
  async getStats() {
    const { value } = await this.getWithLayers('stats');
    return value;
  }

  async setStats(stats, ttl = 60) {
    await this.setWithLayers('stats', stats, {
      memoryTTL: 30000,
      redisTTL: ttl
    });
  }

  // Enhanced rate limiting with sliding window
  async checkRateLimit(key, maxRequests = 100, windowMs = 900000) {
    try {
      const now = Date.now();
      const windowStart = now - windowMs;
      
      // Use Redis sorted set for sliding window
      const pipeline = this.redis.pipeline();
      
      // Remove old entries
      pipeline.zremrangebyscore(key, '-inf', windowStart);
      
      // Add current request
      pipeline.zadd(key, now, `${now}-${Math.random()}`);
      
      // Count requests in window
      pipeline.zcard(key);
      
      // Set expiry
      pipeline.expire(key, Math.ceil(windowMs / 1000));
      
      const results = await pipeline.exec();
      const count = results[2][1];
      
      return {
        totalRequests: count,
        remainingRequests: Math.max(0, maxRequests - count),
        resetTime: new Date(now + windowMs),
        exceeded: count > maxRequests
      };
    } catch (error) {
      logger.error('Rate limit check failed:', error);
      return {
        totalRequests: 0,
        remainingRequests: maxRequests,
        resetTime: new Date(Date.now() + windowMs),
        exceeded: false
      };
    }
  }

  // Temporary restrictions for fraud prevention
  async setTemporaryRestriction(fingerprint, durationSeconds) {
    try {
      await this.redis.setex(
        `restriction:${fingerprint}`,
        durationSeconds,
        JSON.stringify({
          restricted: true,
          until: new Date(Date.now() + durationSeconds * 1000)
        })
      );
    } catch (error) {
      logger.error('Failed to set temporary restriction:', error);
    }
  }

  async checkRestriction(fingerprint) {
    try {
      const restricted = await this.redis.get(`restriction:${fingerprint}`);
      return restricted ? JSON.parse(restricted) : null;
    } catch (error) {
      logger.error('Failed to check restriction:', error);
      return null;
    }
  }

  // Activity tracking for fraud detection
  async trackUserActivity(fingerprint, activity) {
    try {
      const key = `activity:${fingerprint}`;
      const pipeline = this.redis.pipeline();
      
      pipeline.lpush(key, JSON.stringify({
        activity,
        timestamp: Date.now()
      }));
      pipeline.ltrim(key, 0, 999); // Keep last 1000 activities
      pipeline.expire(key, 86400); // 24 hours
      
      await pipeline.exec();
    } catch (error) {
      logger.error('Failed to track user activity:', error);
    }
  }

  async getUserActivityHistory(fingerprint, limit = 100) {
    try {
      const activities = await this.redis.lrange(`activity:${fingerprint}`, 0, limit - 1);
      return activities.map(activity => JSON.parse(activity));
    } catch (error) {
      logger.error('Failed to get activity history:', error);
      return [];
    }
  }

  // Real-time updates pub/sub
  async publishVoteUpdate(llmId, voteData) {
    try {
      const message = JSON.stringify({
        llmId,
        ...voteData,
        timestamp: Date.now()
      });
      
      await this.pubsubClient.publish('vote_updates', message);
      await this.pubsubClient.publish(`vote_updates:${llmId}`, message);
    } catch (error) {
      logger.error('Failed to publish vote update:', error);
    }
  }

  async subscribeToVoteUpdates(callback, llmId = null) {
    try {
      const channel = llmId ? `vote_updates:${llmId}` : 'vote_updates';
      
      if (!this.subscribers.has(channel)) {
        const subscriber = scaledDbManager.getRedis('pubsub').duplicate();
        await subscriber.subscribe(channel);
        
        subscriber.on('message', (ch, message) => {
          if (ch === channel) {
            try {
              const data = JSON.parse(message);
              callback(data);
            } catch (error) {
              logger.error('Failed to parse vote update message:', error);
            }
          }
        });
        
        this.subscribers.set(channel, subscriber);
      }
      
      return () => this.unsubscribeFromVoteUpdates(channel);
    } catch (error) {
      logger.error('Failed to subscribe to vote updates:', error);
      return null;
    }
  }

  async unsubscribeFromVoteUpdates(channel) {
    const subscriber = this.subscribers.get(channel);
    if (subscriber) {
      await subscriber.unsubscribe(channel);
      await subscriber.quit();
      this.subscribers.delete(channel);
    }
  }

  // Cache invalidation
  async invalidateVoteCache(llmId) {
    try {
      // Clear from memory cache
      this.memoryCache.delete(`vote_count:${llmId}`);
      this.memoryCache.delete('all_votes');
      this.memoryCache.delete('rankings');
      this.memoryCache.delete('stats');
      
      // Clear from Redis
      const pipeline = this.redis.pipeline();
      pipeline.del(`vote_count:${llmId}`);
      pipeline.del('all_votes');
      pipeline.del('rankings');
      pipeline.del('stats');
      await pipeline.exec();
    } catch (error) {
      logger.error('Failed to invalidate vote cache:', error);
    }
  }

  async invalidateAllCaches() {
    try {
      // Clear memory cache
      this.memoryCache.clear();
      
      // Clear Redis caches
      const pattern = 'vote_count:*';
      const keys = await this.redis.keys(pattern);
      
      if (keys.length > 0) {
        await this.redis.del(...keys);
      }
      
      await this.redis.del('all_votes', 'rankings', 'stats');
      
      logger.info('All caches invalidated');
    } catch (error) {
      logger.error('Failed to invalidate all caches:', error);
    }
  }

  // Batch operations for efficiency
  async batchGet(keys) {
    const results = {};
    const missingKeys = [];
    
    // Check memory cache first
    for (const key of keys) {
      const value = this.getFromMemory(key);
      if (value !== null) {
        results[key] = value;
      } else {
        missingKeys.push(key);
      }
    }
    
    // Fetch missing keys from Redis
    if (missingKeys.length > 0) {
      try {
        const pipeline = this.redis.pipeline();
        for (const key of missingKeys) {
          pipeline.get(key);
        }
        const redisResults = await pipeline.exec();
        
        for (let i = 0; i < missingKeys.length; i++) {
          const [err, value] = redisResults[i];
          if (!err && value) {
            const parsed = JSON.parse(value);
            results[missingKeys[i]] = parsed;
            this.setInMemory(missingKeys[i], parsed);
          }
        }
      } catch (error) {
        logger.error('Batch get failed:', error);
      }
    }
    
    return results;
  }

  // Health check
  async healthCheck() {
    try {
      const start = Date.now();
      await this.redis.ping();
      const latency = Date.now() - start;
      
      const info = await this.redis.info('memory');
      const memoryUsage = info.match(/used_memory_human:(.+)/)?.[1];
      
      return {
        status: 'healthy',
        latency: `${latency}ms`,
        memoryCache: {
          size: this.memoryCache.size,
          maxSize: 10000
        },
        redisMemory: memoryUsage
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }

  // Cleanup
  async cleanup() {
    // Clear memory cache
    this.memoryCache.clear();
    
    // Close pub/sub subscribers
    for (const [channel, subscriber] of this.subscribers) {
      await subscriber.unsubscribe(channel);
      await subscriber.quit();
    }
    this.subscribers.clear();
    
    logger.info('Enhanced Cache Manager cleaned up');
  }
}

// Singleton instance
const enhancedCacheManager = new EnhancedCacheManager();

export default enhancedCacheManager;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\cache.js
import dbManager from './database.js';
import logger from './logger.js';

class CacheManager {
  constructor() {
    this.redis = null;
    this.defaultTTL = 3600; // 1 hour in seconds
  }

  async initialize() {
    this.redis = dbManager.getRedis();
  }

  // Vote counting cache
  async getVoteCount(llmId) {
    try {
      const cached = await this.redis.get(`vote_count:${llmId}`);
      return cached ? parseInt(cached, 10) : null;
    } catch (error) {
      logger.error('Redis get vote count failed:', error);
      return null;
    }
  }

  async setVoteCount(llmId, count, ttl = this.defaultTTL) {
    try {
      await this.redis.setex(`vote_count:${llmId}`, ttl, count.toString());
    } catch (error) {
      logger.error('Redis set vote count failed:', error);
    }
  }

  async incrementVoteCount(llmId, increment = 1) {
    try {
      const newCount = await this.redis.incrby(`vote_count:${llmId}`, increment);
      await this.redis.expire(`vote_count:${llmId}`, this.defaultTTL);
      return newCount;
    } catch (error) {
      logger.error('Redis increment vote count failed:', error);
      return null;
    }
  }

  // All votes cache
  async getAllVotes() {
    try {
      const cached = await this.redis.get('all_votes');
      return cached ? JSON.parse(cached) : null;
    } catch (error) {
      logger.error('Redis get all votes failed:', error);
      return null;
    }
  }

  async setAllVotes(votesData, ttl = 300) { // 5 minutes for all votes
    try {
      await this.redis.setex('all_votes', ttl, JSON.stringify(votesData));
    } catch (error) {
      logger.error('Redis set all votes failed:', error);
    }
  }

  // User session cache
  async getUserSession(fingerprint) {
    try {
      const cached = await this.redis.get(`user_session:${fingerprint}`);
      return cached ? JSON.parse(cached) : null;
    } catch (error) {
      logger.error('Redis get user session failed:', error);
      return null;
    }
  }

  async setUserSession(fingerprint, sessionData, ttl = 86400) { // 24 hours
    try {
      await this.redis.setex(
        `user_session:${fingerprint}`,
        ttl,
        JSON.stringify(sessionData)
      );
    } catch (error) {
      logger.error('Redis set user session failed:', error);
    }
  }

  // User votes cache
  async getUserVotes(fingerprint) {
    try {
      const cached = await this.redis.hgetall(`user_votes:${fingerprint}`);
      const votes = {};
      for (const [llmId, voteType] of Object.entries(cached)) {
        votes[llmId] = parseInt(voteType, 10);
      }
      return Object.keys(votes).length > 0 ? votes : null;
    } catch (error) {
      logger.error('Redis get user votes failed:', error);
      return null;
    }
  }

  async setUserVote(fingerprint, llmId, voteType) {
    try {
      if (voteType === 0) {
        await this.redis.hdel(`user_votes:${fingerprint}`, llmId);
      } else {
        await this.redis.hset(`user_votes:${fingerprint}`, llmId, voteType.toString());
      }
      await this.redis.expire(`user_votes:${fingerprint}`, 86400); // 24 hours
    } catch (error) {
      logger.error('Redis set user vote failed:', error);
    }
  }

  async setAllUserVotes(fingerprint, votes, ttl = 86400) {
    try {
      const pipeline = this.redis.pipeline();
      pipeline.del(`user_votes:${fingerprint}`);
      
      for (const [llmId, voteType] of Object.entries(votes)) {
        if (voteType !== 0) {
          pipeline.hset(`user_votes:${fingerprint}`, llmId, voteType.toString());
        }
      }
      
      pipeline.expire(`user_votes:${fingerprint}`, ttl);
      await pipeline.exec();
    } catch (error) {
      logger.error('Redis set all user votes failed:', error);
    }
  }

  // Rankings cache
  async getRankings() {
    try {
      const cached = await this.redis.get('rankings');
      return cached ? JSON.parse(cached) : null;
    } catch (error) {
      logger.error('Redis get rankings failed:', error);
      return null;
    }
  }

  async setRankings(rankings, ttl = 300) { // 5 minutes
    try {
      await this.redis.setex('rankings', ttl, JSON.stringify(rankings));
    } catch (error) {
      logger.error('Redis set rankings failed:', error);
    }
  }

  // Statistics cache
  async getStats() {
    try {
      const cached = await this.redis.get('stats');
      return cached ? JSON.parse(cached) : null;
    } catch (error) {
      logger.error('Redis get stats failed:', error);
      return null;
    }
  }

  async setStats(stats, ttl = 300) { // 5 minutes
    try {
      await this.redis.setex('stats', ttl, JSON.stringify(stats));
    } catch (error) {
      logger.error('Redis set stats failed:', error);
    }
  }

  // Rate limiting
  async checkRateLimit(key, maxRequests = 100, windowMs = 900000) { // 15 minutes default
    try {
      const current = await this.redis.incr(key);
      
      if (current === 1) {
        await this.redis.expire(key, Math.ceil(windowMs / 1000));
      }
      
      const ttl = await this.redis.ttl(key);
      
      return {
        totalRequests: current,
        remainingRequests: Math.max(0, maxRequests - current),
        resetTime: new Date(Date.now() + ttl * 1000),
        exceeded: current > maxRequests
      };
    } catch (error) {
      logger.error('Redis rate limit check failed:', error);
      return {
        totalRequests: 0,
        remainingRequests: maxRequests,
        resetTime: new Date(Date.now() + windowMs),
        exceeded: false
      };
    }
  }

  // Session tracking for fraud detection
  async trackUserActivity(fingerprint, activity) {
    try {
      const key = `activity:${fingerprint}`;
      const pipeline = this.redis.pipeline();
      
      pipeline.lpush(key, JSON.stringify({
        activity,
        timestamp: Date.now()
      }));
      pipeline.ltrim(key, 0, 99); // Keep last 100 activities
      pipeline.expire(key, 86400); // 24 hours
      
      await pipeline.exec();
    } catch (error) {
      logger.error('Redis track activity failed:', error);
    }
  }

  async getUserActivityHistory(fingerprint, limit = 20) {
    try {
      const activities = await this.redis.lrange(`activity:${fingerprint}`, 0, limit - 1);
      return activities.map(activity => JSON.parse(activity));
    } catch (error) {
      logger.error('Redis get activity history failed:', error);
      return [];
    }
  }

  // Real-time updates pub/sub
  async publishVoteUpdate(llmId, voteData) {
    try {
      await this.redis.publish('vote_updates', JSON.stringify({
        llmId,
        ...voteData,
        timestamp: Date.now()
      }));
    } catch (error) {
      logger.error('Redis publish vote update failed:', error);
    }
  }

  async subscribeToVoteUpdates(callback) {
    try {
      const subscriber = this.redis.duplicate();
      await subscriber.subscribe('vote_updates');
      subscriber.on('message', (channel, message) => {
        if (channel === 'vote_updates') {
          try {
            const data = JSON.parse(message);
            callback(data);
          } catch (error) {
            logger.error('Failed to parse vote update message:', error);
          }
        }
      });
      return subscriber;
    } catch (error) {
      logger.error('Redis subscribe to vote updates failed:', error);
      return null;
    }
  }

  // Cache invalidation
  async invalidateVoteCache(llmId) {
    try {
      const pipeline = this.redis.pipeline();
      pipeline.del(`vote_count:${llmId}`);
      pipeline.del('all_votes');
      pipeline.del('rankings');
      pipeline.del('stats');
      await pipeline.exec();
    } catch (error) {
      logger.error('Redis invalidate vote cache failed:', error);
    }
  }

  async invalidateAllCaches() {
    try {
      const pattern = 'vote_count:*';
      const keys = await this.redis.keys(pattern);
      
      if (keys.length > 0) {
        await this.redis.del(...keys);
      }
      
      await this.redis.del('all_votes', 'rankings', 'stats');
    } catch (error) {
      logger.error('Redis invalidate all caches failed:', error);
    }
  }

  // Cleanup expired keys
  async cleanup() {
    try {
      // This is handled automatically by Redis TTL, but we can add custom cleanup logic
      const expiredSessions = await this.redis.keys('user_session:*');
      let cleanedCount = 0;
      
      for (const key of expiredSessions) {
        const ttl = await this.redis.ttl(key);
        if (ttl === -1) { // Key exists but has no TTL
          await this.redis.expire(key, 86400); // Set 24 hour TTL
          cleanedCount++;
        }
      }
      
      if (cleanedCount > 0) {
        logger.info(`Cleaned up ${cleanedCount} session keys without TTL`);
      }
    } catch (error) {
      logger.error('Redis cleanup failed:', error);
    }
  }

  // Health check
  async healthCheck() {
    try {
      const start = Date.now();
      await this.redis.ping();
      const latency = Date.now() - start;
      
      return {
        status: 'healthy',
        latency: `${latency}ms`
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
}

// Singleton instance
const cacheManager = new CacheManager();

export default cacheManager;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\config.js
// Configuration helper to manage environment variables
// Falls back to file-based storage if database is not configured

const config = {
  // Database configuration - always enabled for production
  database: {
    enabled: true, // Database is required, no fallback
    host: process.env.POSTGRES_HOST || 'postgres',
    port: parseInt(process.env.POSTGRES_PORT || '5432'),
    database: process.env.POSTGRES_DB || 'llm_tracker',
    user: process.env.POSTGRES_USER || 'postgres',
    password: process.env.POSTGRES_PASSWORD || 'password',
    url: process.env.DATABASE_URL,
  },
  
  // Redis configuration - always enabled for production
  redis: {
    enabled: true, // Redis is required for caching and real-time updates
    host: process.env.REDIS_HOST || 'redis',
    port: parseInt(process.env.REDIS_PORT || '6379'),
    password: process.env.REDIS_PASSWORD || undefined,
    url: process.env.REDIS_URL,
  },
  
  // Application settings
  app: {
    env: process.env.NODE_ENV || 'development',
    port: parseInt(process.env.PORT || '3000'),
    logLevel: process.env.LOG_LEVEL || 'info',
  },
  
  // Security settings
  security: {
    jwtSecret: process.env.JWT_SECRET || 'development-secret-change-in-production',
    jwtExpiresIn: process.env.JWT_EXPIRES_IN || '7d',
    sessionSecret: process.env.SESSION_SECRET || 'session-secret-change-in-production',
    bcryptRounds: parseInt(process.env.BCRYPT_ROUNDS || '12'),
  },
  
  // Rate limiting
  rateLimit: {
    maxRequests: parseInt(process.env.RATE_LIMIT_MAX || '100'),
    windowMs: parseInt(process.env.RATE_LIMIT_WINDOW_MS || '900000'),
  },
  
  // Feature flags
  features: {
    analytics: process.env.ENABLE_ANALYTICS !== 'false', // Default to true
    realTime: process.env.ENABLE_REAL_TIME !== 'false', // Default to true for real-time updates
    useDatabase: true, // Always use database, no fallback to file storage
  },
};

// Database and Redis are mandatory - no fallback to file storage
// This ensures the system can handle 500k+ users with proper scaling
config.features.useDatabase = true;

// Log configuration status (without sensitive data)
if (typeof window === 'undefined') { // Only on server side
  console.log('Configuration loaded:', {
    environment: config.app.env,
    databaseEnabled: true, // Always enabled
    redisEnabled: true, // Always enabled
    postgresHost: config.database.host,
    redisHost: config.redis.host,
    features: config.features,
  });
}

export default config;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\database-scaled.js
import { Pool } from 'pg';
import Redis from 'ioredis';
import logger from './logger.js';

class ScaledDatabaseManager {
  constructor() {
    this.primaryPool = null;
    this.readPools = [];
    this.currentReadIndex = 0;
    this.redisSentinel = null;
    this.redisClients = new Map();
    this.initialized = false;
  }

  async initializePostgres() {
    try {
      // Initialize primary pool
      this.primaryPool = new Pool({
        host: process.env.POSTGRES_HOST || 'pgbouncer',
        port: process.env.PGBOUNCER_PORT || 6432,
        database: process.env.POSTGRES_DB || 'llm_tracker',
        user: process.env.POSTGRES_USER || 'llm_user',
        password: process.env.POSTGRES_PASSWORD || 'changeme',
        max: 50,
        min: 10,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 5000,
        statement_timeout: 10000,
        query_timeout: 10000,
        application_name: 'llm-tracker-primary',
        ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false
      });

      // Initialize read replica pools
      const readReplicas = process.env.POSTGRES_READ_HOSTS?.split(',') || [];
      for (const host of readReplicas) {
        const readPool = new Pool({
          host,
          port: process.env.POSTGRES_PORT || 5432,
          database: process.env.POSTGRES_DB || 'llm_tracker',
          user: process.env.POSTGRES_USER || 'llm_user',
          password: process.env.POSTGRES_PASSWORD || 'changeme',
          max: 30,
          min: 5,
          idleTimeoutMillis: 30000,
          connectionTimeoutMillis: 5000,
          statement_timeout: 10000,
          query_timeout: 10000,
          application_name: `llm-tracker-read-${host}`,
          ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false
        });
        
        this.readPools.push(readPool);
      }

      // Test connections
      await this.testConnection(this.primaryPool, 'Primary');
      for (let i = 0; i < this.readPools.length; i++) {
        await this.testConnection(this.readPools[i], `Read Replica ${i + 1}`);
      }
      
      logger.info(`PostgreSQL initialized: 1 primary, ${this.readPools.length} read replicas`);
    } catch (error) {
      logger.error('Failed to initialize PostgreSQL:', error);
      throw error;
    }
  }

  async testConnection(pool, name) {
    try {
      const client = await pool.connect();
      await client.query('SELECT NOW()');
      client.release();
      logger.info(`${name} PostgreSQL connection successful`);
    } catch (error) {
      logger.error(`${name} PostgreSQL connection failed:`, error);
      throw error;
    }
  }

  async initializeRedisSentinel() {
    try {
      const sentinels = process.env.REDIS_SENTINELS?.split(',').map(s => {
        const [host, port] = s.split(':');
        return { host, port: parseInt(port) || 26379 };
      }) || [{ host: 'localhost', port: 26379 }];

      // Create Redis Sentinel client
      this.redisSentinel = new Redis({
        sentinels,
        name: process.env.REDIS_MASTER_NAME || 'mymaster',
        password: process.env.REDIS_PASSWORD || undefined,
        sentinelPassword: process.env.REDIS_SENTINEL_PASSWORD || undefined,
        retryStrategy: (times) => Math.min(times * 50, 2000),
        reconnectOnError: (err) => {
          logger.error('Redis reconnect on error:', err);
          return true;
        },
        maxRetriesPerRequest: 3,
        enableReadyCheck: true,
        lazyConnect: false,
        keepAlive: 30000,
        family: 4,
        connectTimeout: 10000,
        commandTimeout: 5000,
        db: 0
      });

      // Handle Redis events
      this.redisSentinel.on('connect', () => {
        logger.info('Redis Sentinel connected');
      });

      this.redisSentinel.on('ready', () => {
        logger.info('Redis Sentinel ready');
      });

      this.redisSentinel.on('error', (error) => {
        logger.error('Redis Sentinel error:', error);
      });

      this.redisSentinel.on('+switch-master', (master) => {
        logger.info('Redis master switched:', master);
      });

      // Test connection
      await this.redisSentinel.ping();
      
      // Create separate clients for different purposes
      await this.createRedisClients();
      
      logger.info('Redis Sentinel initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Redis Sentinel:', error);
      // Fallback to standard Redis if Sentinel fails
      await this.initializeRedisStandard();
    }
  }

  async createRedisClients() {
    // Create specialized Redis clients for different purposes
    const purposes = ['cache', 'pubsub', 'session', 'ratelimit'];
    
    for (const purpose of purposes) {
      const client = this.redisSentinel.duplicate();
      await client.ping();
      this.redisClients.set(purpose, client);
    }
  }

  async initializeRedisStandard() {
    try {
      const redisConfig = {
        host: process.env.REDIS_HOST || 'localhost',
        port: process.env.REDIS_PORT || 6379,
        password: process.env.REDIS_PASSWORD || undefined,
        retryDelayOnFailover: 100,
        maxRetriesPerRequest: 3,
        lazyConnect: false,
        keepAlive: 30000,
        family: 4,
        connectTimeout: 10000,
        commandTimeout: 5000,
      };

      this.redisSentinel = new Redis(redisConfig);
      await this.redisSentinel.ping();
      await this.createRedisClients();
      
      logger.info('Standard Redis initialized (fallback mode)');
    } catch (error) {
      logger.error('Failed to initialize standard Redis:', error);
      throw error;
    }
  }

  async initialize() {
    if (this.initialized) return;
    
    try {
      await Promise.all([
        this.initializePostgres(),
        this.initializeRedisSentinel()
      ]);
      
      this.initialized = true;
      logger.info('Scaled Database Manager initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Scaled Database Manager:', error);
      throw error;
    }
  }

  // Get appropriate pool based on query type
  getPool(isWrite = false) {
    if (isWrite || this.readPools.length === 0) {
      return this.primaryPool;
    }
    
    // Round-robin load balancing for read queries
    const pool = this.readPools[this.currentReadIndex];
    this.currentReadIndex = (this.currentReadIndex + 1) % this.readPools.length;
    return pool;
  }

  // Get Redis client for specific purpose
  getRedis(purpose = 'cache') {
    const client = this.redisClients.get(purpose) || this.redisSentinel;
    if (!client) {
      throw new Error('Redis not initialized');
    }
    return client;
  }

  // Execute query with automatic routing
  async query(text, params = [], options = {}) {
    const { isWrite = false, preferPrimary = false } = options;
    const isWriteQuery = isWrite || this.isWriteQuery(text);
    const pool = preferPrimary ? this.primaryPool : this.getPool(isWriteQuery);
    
    const start = Date.now();
    let retries = 0;
    const maxRetries = 2;
    
    while (retries <= maxRetries) {
      try {
        const result = await pool.query(text, params);
        const duration = Date.now() - start;
        
        if (duration > 500) {
          logger.warn('Slow query detected', { 
            query: text.substring(0, 100),
            duration: `${duration}ms`,
            pool: isWriteQuery ? 'primary' : 'read-replica'
          });
        }
        
        return result;
      } catch (error) {
        retries++;
        
        if (retries > maxRetries) {
          logger.error('Query failed after retries', { 
            query: text.substring(0, 100),
            error: error.message,
            retries
          });
          throw error;
        }
        
        // If read replica fails, fallback to primary
        if (!isWriteQuery && pool !== this.primaryPool) {
          logger.warn('Read replica failed, falling back to primary');
          return this.primaryPool.query(text, params);
        }
        
        await this.delay(100 * retries);
      }
    }
  }

  // Determine if query is a write operation
  isWriteQuery(text) {
    const writeKeywords = ['INSERT', 'UPDATE', 'DELETE', 'CREATE', 'ALTER', 'DROP', 'TRUNCATE'];
    const upperText = text.trim().toUpperCase();
    return writeKeywords.some(keyword => upperText.startsWith(keyword));
  }

  // Enhanced transaction with automatic retries
  async transaction(callback, options = {}) {
    const { maxRetries = 3, retryDelay = 100 } = options;
    let retries = 0;
    
    while (retries < maxRetries) {
      const client = await this.primaryPool.connect();
      
      try {
        await client.query('BEGIN');
        const result = await callback(client);
        await client.query('COMMIT');
        return result;
      } catch (error) {
        await client.query('ROLLBACK');
        
        // Check if error is retryable
        if (this.isRetryableError(error) && retries < maxRetries - 1) {
          retries++;
          logger.warn(`Transaction retry ${retries}/${maxRetries}`, { error: error.message });
          await this.delay(retryDelay * retries);
          continue;
        }
        
        throw error;
      } finally {
        client.release();
      }
    }
  }

  // Check if error is retryable
  isRetryableError(error) {
    const retryableCodes = [
      '40001', // serialization_failure
      '40P01', // deadlock_detected
      '08006', // connection_failure
      '08001', // sqlclient_unable_to_establish_sqlconnection
      '08004', // sqlserver_rejected_establishment_of_sqlconnection
      '57P03', // cannot_connect_now
    ];
    
    return retryableCodes.includes(error.code);
  }

  // Delay utility
  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  // Batch query execution
  async batchQuery(queries) {
    const client = await this.primaryPool.connect();
    
    try {
      await client.query('BEGIN');
      
      const results = [];
      for (const { text, params } of queries) {
        const result = await client.query(text, params);
        results.push(result);
      }
      
      await client.query('COMMIT');
      return results;
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }

  // Health check for all components
  async healthCheck() {
    const results = {
      postgresqlPrimary: false,
      postgresqlReplicas: [],
      redis: false,
      redisSentinel: false,
      timestamp: new Date().toISOString()
    };

    // Check primary PostgreSQL
    try {
      await this.primaryPool.query('SELECT 1');
      results.postgresqlPrimary = true;
    } catch (error) {
      logger.error('Primary PostgreSQL health check failed:', error);
    }

    // Check read replicas
    for (let i = 0; i < this.readPools.length; i++) {
      try {
        await this.readPools[i].query('SELECT 1');
        results.postgresqlReplicas.push({ index: i, healthy: true });
      } catch (error) {
        logger.error(`Read replica ${i} health check failed:`, error);
        results.postgresqlReplicas.push({ index: i, healthy: false });
      }
    }

    // Check Redis
    try {
      await this.redisSentinel.ping();
      results.redis = true;
      
      // Check Sentinel status
      const sentinelInfo = await this.redisSentinel.call('SENTINEL', 'masters');
      results.redisSentinel = sentinelInfo && sentinelInfo.length > 0;
    } catch (error) {
      logger.error('Redis health check failed:', error);
    }

    return results;
  }

  // Get pool statistics
  async getPoolStats() {
    const stats = {
      primary: {
        totalCount: this.primaryPool.totalCount,
        idleCount: this.primaryPool.idleCount,
        waitingCount: this.primaryPool.waitingCount
      },
      replicas: []
    };

    for (let i = 0; i < this.readPools.length; i++) {
      stats.replicas.push({
        index: i,
        totalCount: this.readPools[i].totalCount,
        idleCount: this.readPools[i].idleCount,
        waitingCount: this.readPools[i].waitingCount
      });
    }

    return stats;
  }

  // Close all connections
  async closeAll() {
    const promises = [];
    
    if (this.primaryPool) {
      promises.push(this.primaryPool.end());
    }
    
    for (const pool of this.readPools) {
      promises.push(pool.end());
    }
    
    for (const [purpose, client] of this.redisClients) {
      promises.push(client.quit());
    }
    
    if (this.redisSentinel) {
      promises.push(this.redisSentinel.quit());
    }
    
    await Promise.all(promises);
    logger.info('All database connections closed');
  }
}

// Singleton instance
const scaledDbManager = new ScaledDatabaseManager();

export default scaledDbManager;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\database.js
import { Pool } from 'pg';
import Redis from 'ioredis';
import logger from './logger.js';

class DatabaseManager {
  constructor() {
    this.pool = null;
    this.redis = null;
  }

  async initializePostgres() {
    try {
      this.pool = new Pool({
        host: process.env.POSTGRES_HOST || 'localhost',
        port: process.env.POSTGRES_PORT || 5432,
        database: process.env.POSTGRES_DB || 'llm_tracker',
        user: process.env.POSTGRES_USER || 'postgres',
        password: process.env.POSTGRES_PASSWORD || 'password',
        max: 20, // Maximum pool size
        idleTimeoutMillis: 30000, // Close idle clients after 30 seconds
        connectionTimeoutMillis: 2000, // Return error after 2 seconds if connection could not be established
        statement_timeout: 10000, // Terminate any statement that takes over 10 seconds
        query_timeout: 10000, // Terminate any query that takes over 10 seconds
        application_name: 'llm-popularity-tracker',
        ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false
      });

      // Test connection
      const client = await this.pool.connect();
      await client.query('SELECT NOW()');
      client.release();
      
      logger.info('PostgreSQL connection pool initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize PostgreSQL:', error);
      throw error;
    }
  }

  async initializeRedis() {
    try {
      const redisConfig = {
        host: process.env.REDIS_HOST || 'localhost',
        port: process.env.REDIS_PORT || 6379,
        password: process.env.REDIS_PASSWORD || undefined,
        retryDelayOnFailover: 100,
        maxRetriesPerRequest: 3,
        lazyConnect: true,
        keepAlive: 30000,
        family: 4, // Force IPv4
        connectTimeout: 10000,
        commandTimeout: 5000,
      };

      this.redis = new Redis(redisConfig);
      
      // Test connection
      await this.redis.connect();
      await this.redis.ping();
      
      this.redis.on('error', (error) => {
        logger.error('Redis error:', error);
      });

      this.redis.on('connect', () => {
        logger.info('Redis connected successfully');
      });

      this.redis.on('ready', () => {
        logger.info('Redis ready to accept commands');
      });

      logger.info('Redis connection initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Redis:', error);
      throw error;
    }
  }

  async initialize() {
    await Promise.all([
      this.initializePostgres(),
      this.initializeRedis()
    ]);
  }

  getPool() {
    if (!this.pool) {
      throw new Error('Database pool not initialized. Call initialize() first.');
    }
    return this.pool;
  }

  getRedis() {
    if (!this.redis) {
      throw new Error('Redis not initialized. Call initialize() first.');
    }
    return this.redis;
  }

  async query(text, params = []) {
    const start = Date.now();
    try {
      const result = await this.pool.query(text, params);
      const duration = Date.now() - start;
      logger.debug('Query executed', { 
        query: text, 
        duration: `${duration}ms`, 
        rows: result.rowCount 
      });
      return result;
    } catch (error) {
      const duration = Date.now() - start;
      logger.error('Query failed', { 
        query: text, 
        duration: `${duration}ms`, 
        error: error.message 
      });
      throw error;
    }
  }

  async transaction(callback) {
    const client = await this.pool.connect();
    try {
      await client.query('BEGIN');
      const result = await callback(client);
      await client.query('COMMIT');
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }

  async closeAll() {
    const promises = [];
    
    if (this.pool) {
      promises.push(this.pool.end());
    }
    
    if (this.redis) {
      promises.push(this.redis.quit());
    }
    
    await Promise.all(promises);
    logger.info('All database connections closed');
  }

  // Health check methods
  async healthCheck() {
    const results = {
      postgres: false,
      redis: false,
      timestamp: new Date().toISOString()
    };

    try {
      await this.pool.query('SELECT 1');
      results.postgres = true;
    } catch (error) {
      logger.error('PostgreSQL health check failed:', error);
    }

    try {
      await this.redis.ping();
      results.redis = true;
    } catch (error) {
      logger.error('Redis health check failed:', error);
    }

    return results;
  }
}

// Singleton instance
const dbManager = new DatabaseManager();

export default dbManager;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\fingerprint.js
import FingerprintJS from '@fingerprintjs/fingerprintjs';

class FingerprintService {
  constructor() {
    this.fpPromise = null;
    this.cachedFingerprint = null;
    this.storageKey = 'llm-tracker-fingerprint';
  }

  async initialize() {
    if (!this.fpPromise) {
      this.fpPromise = FingerprintJS.load();
    }
    return this.fpPromise;
  }

  async getFingerprint() {
    // Return cached fingerprint if available
    if (this.cachedFingerprint) {
      return this.cachedFingerprint;
    }

    // Try to get from localStorage first
    const storedFingerprint = this.getStoredFingerprint();
    if (storedFingerprint) {
      this.cachedFingerprint = storedFingerprint;
      return storedFingerprint;
    }

    // Generate new fingerprint
    try {
      const fp = await this.initialize();
      const result = await fp.get();
      const fingerprint = result.visitorId;
      
      // Store for future use
      this.storeFingerprint(fingerprint);
      this.cachedFingerprint = fingerprint;
      
      return fingerprint;
    } catch (error) {
      console.error('Failed to generate fingerprint:', error);
      // Fallback to a random ID stored in localStorage
      return this.generateFallbackFingerprint();
    }
  }

  getStoredFingerprint() {
    if (typeof window === 'undefined') return null;
    
    try {
      return localStorage.getItem(this.storageKey);
    } catch (error) {
      console.error('Failed to read stored fingerprint:', error);
      return null;
    }
  }

  storeFingerprint(fingerprint) {
    if (typeof window === 'undefined') return;
    
    try {
      localStorage.setItem(this.storageKey, fingerprint);
      // Also store in sessionStorage as backup
      sessionStorage.setItem(this.storageKey, fingerprint);
    } catch (error) {
      console.error('Failed to store fingerprint:', error);
    }
  }

  generateFallbackFingerprint() {
    // Generate a random fallback ID
    const fallbackId = 'fallback_' + Math.random().toString(36).substring(2) + Date.now().toString(36);
    this.storeFingerprint(fallbackId);
    this.cachedFingerprint = fallbackId;
    return fallbackId;
  }

  // Get fingerprint with multiple fallback options
  async getFingerprintWithFallbacks() {
    // Primary: Generated fingerprint
    try {
      return await this.getFingerprint();
    } catch (error) {
      console.error('Primary fingerprint failed:', error);
    }

    // Secondary: SessionStorage
    try {
      const sessionFingerprint = sessionStorage.getItem(this.storageKey);
      if (sessionFingerprint) {
        this.cachedFingerprint = sessionFingerprint;
        return sessionFingerprint;
      }
    } catch (error) {
      console.error('SessionStorage fallback failed:', error);
    }

    // Tertiary: Generate new fallback
    return this.generateFallbackFingerprint();
  }

  // Clear stored fingerprint (for testing or user request)
  clearFingerprint() {
    this.cachedFingerprint = null;
    if (typeof window !== 'undefined') {
      try {
        localStorage.removeItem(this.storageKey);
        sessionStorage.removeItem(this.storageKey);
      } catch (error) {
        console.error('Failed to clear fingerprint:', error);
      }
    }
  }
}

// Singleton instance
const fingerprintService = new FingerprintService();

export default fingerprintService;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\llm-data.js
export const llmData = [
  {
    id: "gpt-4o",
    name: "GPT-4o",
    company: "OpenAI",
    description: "Most advanced multimodal AI with vision, analysis, and coding capabilities",
    useCases: ["General purpose", "Code generation", "Creative writing", "Vision tasks"],
    releaseYear: 2024,
    color: "from-green-500 to-emerald-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/200px-OpenAI_Logo.svg.png",
  },
  {
    id: "claude-3-5-sonnet",
    name: "Claude 3.5 Sonnet",
    company: "Anthropic",
    description: "Balanced model excelling at analysis, coding, and nuanced conversation",
    useCases: ["Code analysis", "Research", "Writing", "Complex reasoning"],
    releaseYear: 2024,
    color: "from-orange-500 to-amber-600",
    logo: "",
    image: "https://www.anthropic.com/_next/static/media/claude-logo.2f5f0b53.svg",
  },
  {
    id: "gemini-ultra",
    name: "Gemini Ultra",
    company: "Google",
    description: "Google's flagship model with strong multimodal and reasoning abilities",
    useCases: ["Multimodal tasks", "Scientific research", "Code", "Mathematics"],
    releaseYear: 2024,
    color: "from-blue-500 to-cyan-600",
    logo: "",
    image: "https://www.gstatic.com/lamda/images/gemini_sparkle_v002_d4735304ff6292a690345.svg",
  },
  {
    id: "llama-3-70b",
    name: "Llama 3 70B",
    company: "Meta",
    description: "Open-source powerhouse for customizable AI applications",
    useCases: ["Open source projects", "Fine-tuning", "Research", "Commercial use"],
    releaseYear: 2024,
    color: "from-purple-500 to-violet-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Meta_Platforms_Inc._logo.svg/200px-Meta_Platforms_Inc._logo.svg.png",
  },
  {
    id: "mistral-large",
    name: "Mistral Large",
    company: "Mistral AI",
    description: "European AI champion with strong multilingual capabilities",
    useCases: ["Multilingual tasks", "European languages", "Code", "Efficiency"],
    releaseYear: 2024,
    color: "from-red-500 to-pink-600",
    logo: "",
    image: "https://docs.mistral.ai/img/logo.svg",
  },
  {
    id: "command-r-plus",
    name: "Command R+",
    company: "Cohere",
    description: "Enterprise-focused with excellent retrieval and grounding",
    useCases: ["Enterprise search", "RAG systems", "Document analysis", "Business"],
    releaseYear: 2024,
    color: "from-indigo-500 to-blue-600",
    logo: "",
    image: "https://cohere.com/favicon.svg",
  },
  {
    id: "grok",
    name: "Grok",
    company: "xAI",
    description: "Real-time knowledge with humor and unconventional responses",
    useCases: ["Real-time info", "Social media", "Humor", "Current events"],
    releaseYear: 2023,
    color: "from-gray-600 to-slate-700",
    logo: "",
    image: "https://grok.x.ai/assets/grok-logo-light.svg",
  },
  {
    id: "perplexity",
    name: "Perplexity",
    company: "Perplexity AI",
    description: "Search-enhanced AI with real-time web access and citations",
    useCases: ["Web search", "Research", "Fact-checking", "Citations"],
    releaseYear: 2024,
    color: "from-teal-500 to-cyan-600",
    logo: "",
    image: "https://www.perplexity.ai/favicon.svg",
  },
  {
    id: "qwen-2-5",
    name: "Qwen 2.5",
    company: "Alibaba",
    description: "Strong Asian language support with competitive performance",
    useCases: ["Chinese language", "Asian markets", "E-commerce", "Translation"],
    releaseYear: 2024,
    color: "from-yellow-500 to-orange-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Alibaba-Logo.svg/200px-Alibaba-Logo.svg.png",
  },
  {
    id: "deepseek-coder",
    name: "DeepSeek Coder",
    company: "DeepSeek",
    description: "Specialized coding model with excellent debugging capabilities",
    useCases: ["Code generation", "Debugging", "Code review", "Documentation"],
    releaseYear: 2024,
    color: "from-green-600 to-teal-700",
    logo: "",
    image: "https://github.com/deepseek-ai.png",
  },
  {
    id: "phi-3",
    name: "Phi-3",
    company: "Microsoft",
    description: "Small but mighty model optimized for edge deployment",
    useCases: ["Edge computing", "Mobile apps", "Low resource", "Fast inference"],
    releaseYear: 2024,
    color: "from-blue-600 to-indigo-700",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/4/44/Microsoft_logo.svg/200px-Microsoft_logo.svg.png",
  },
  {
    id: "falcon-180b",
    name: "Falcon 180B",
    company: "TII UAE",
    description: "Open-source giant with strong performance across tasks",
    useCases: ["Open source", "Research", "Arabic language", "General purpose"],
    releaseYear: 2023,
    color: "from-amber-600 to-yellow-700",
    logo: "",
    image: "https://www.tii.ae/sites/default/files/2022-12/TII-logo-WHITE.png",
  },
  {
    id: "vicuna-33b",
    name: "Vicuna-33B",
    company: "LMSYS",
    description: "Fine-tuned Llama model with improved conversational abilities",
    useCases: ["Chatbots", "Open source", "Fine-tuning base", "Research"],
    releaseYear: 2023,
    color: "from-pink-500 to-rose-600",
    logo: "",
    image: "https://github.com/lm-sys.png",
  },
  {
    id: "solar-10-7b",
    name: "SOLAR-10.7B",
    company: "Upstage AI",
    description: "Efficient Korean model with strong multilingual capabilities",
    useCases: ["Korean language", "Efficient inference", "Asian languages", "Small models"],
    releaseYear: 2024,
    color: "from-orange-600 to-red-700",
    logo: "",
    image: "https://github.com/UpstageAI.png",
  },
  {
    id: "yi-34b",
    name: "Yi-34B",
    company: "01.AI",
    description: "Bilingual model excelling in Chinese and English tasks",
    useCases: ["Chinese-English", "Translation", "Bilingual tasks", "Open source"],
    releaseYear: 2024,
    color: "from-purple-600 to-pink-700",
    logo: "",
    image: "https://github.com/01-ai.png",
  },
  {
    id: "mixtral-8x7b",
    name: "Mixtral 8x7B",
    company: "Mistral AI",
    description: "Mixture of experts model with excellent efficiency",
    useCases: ["Efficient inference", "MoE architecture", "Code", "Multiple languages"],
    releaseYear: 2024,
    color: "from-violet-600 to-purple-700",
    logo: "",
    image: "https://docs.mistral.ai/img/logo.svg",
  },
  {
    id: "bard",
    name: "Bard (Gemini Pro)",
    company: "Google",
    description: "Google's conversational AI with web access and multimodal features",
    useCases: ["Web browsing", "Conversation", "Creative tasks", "Integration"],
    releaseYear: 2024,
    color: "from-blue-500 to-green-600",
    logo: "",
    image: "https://www.gstatic.com/lamda/images/gemini_sparkle_v002_d4735304ff6292a690345.svg",
  },
  {
    id: "ernie-4",
    name: "ERNIE 4.0",
    company: "Baidu",
    description: "Chinese AI leader with strong understanding of Chinese culture",
    useCases: ["Chinese market", "Cultural context", "Search", "Business"],
    releaseYear: 2024,
    color: "from-red-600 to-orange-700",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/en/thumb/3/37/Baidu_Logo.svg/200px-Baidu_Logo.svg.png",
  },
  {
    id: "stablelm-2",
    name: "StableLM 2",
    company: "Stability AI",
    description: "Open model from the makers of Stable Diffusion",
    useCases: ["Open source", "Customization", "Research", "Creative apps"],
    releaseYear: 2024,
    color: "from-purple-500 to-indigo-600",
    logo: "",
    image: "https://github.com/Stability-AI.png",
  },
  {
    id: "inflection-2-5",
    name: "Inflection-2.5",
    company: "Inflection AI",
    description: "Personal AI with empathetic and supportive conversation style",
    useCases: ["Personal assistant", "Emotional support", "Coaching", "Companionship"],
    releaseYear: 2024,
    color: "from-teal-600 to-blue-700",
    logo: "",
    image: "https://github.com/InflectionAI.png",
  },
];

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\logger.js
import pino from 'pino';
import pretty from 'pino-pretty';

const isDevelopment = process.env.NODE_ENV === 'development';
const logLevel = process.env.LOG_LEVEL || 'info';

// Create logger configuration
const loggerConfig = {
  level: logLevel,
  timestamp: pino.stdTimeFunctions.isoTime,
  formatters: {
    level: (label) => {
      return { level: label.toUpperCase() };
    },
  },
  serializers: {
    err: pino.stdSerializers.err,
    req: (req) => ({
      method: req.method,
      url: req.url,
      headers: {
        'user-agent': req.headers['user-agent'],
        'x-forwarded-for': req.headers['x-forwarded-for'],
        'x-real-ip': req.headers['x-real-ip'],
      },
      remoteAddress: req.connection?.remoteAddress,
      remotePort: req.connection?.remotePort,
    }),
    res: (res) => ({
      statusCode: res.statusCode,
      headers: {
        'content-type': res.headers['content-type'],
        'content-length': res.headers['content-length'],
      },
    }),
  },
};

// Create logger instance
let logger;

if (isDevelopment) {
  // Pretty print for development
  logger = pino(
    loggerConfig,
    pretty({
      colorize: true,
      translateTime: 'SYS:standard',
      ignore: 'pid,hostname',
    })
  );
} else {
  // JSON format for production
  logger = pino(loggerConfig);
}

// Create child loggers for different modules
const createChildLogger = (module) => {
  return logger.child({ module });
};

// Export logger with additional methods
const enhancedLogger = Object.assign(logger, {
  child: createChildLogger,
  
  // Request logging middleware
  logRequest: (req, res, next) => {
    const start = Date.now();
    const requestId = req.headers['x-request-id'] || Math.random().toString(36).substr(2, 9);
    
    req.log = logger.child({ requestId });
    
    req.log.info({
      req,
      event: 'request-start'
    }, 'Request started');
    
    const originalEnd = res.end;
    res.end = function(...args) {
      const duration = Date.now() - start;
      req.log.info({
        res,
        event: 'request-end',
        duration: `${duration}ms`
      }, 'Request completed');
      
      originalEnd.apply(this, args);
    };
    
    if (next) next();
  },
  
  // Security event logging
  security: {
    loginAttempt: (fingerprint, success, reason = null) => {
      logger.warn({
        event: 'login-attempt',
        fingerprint: fingerprint?.substring(0, 8) + '...',
        success,
        reason,
        timestamp: new Date().toISOString()
      }, `Login attempt ${success ? 'succeeded' : 'failed'}`);
    },
    
    rateLimitExceeded: (ip, endpoint, limit) => {
      logger.warn({
        event: 'rate-limit-exceeded',
        ip: ip?.substring(0, 8) + '...',
        endpoint,
        limit,
        timestamp: new Date().toISOString()
      }, 'Rate limit exceeded');
    },
    
    suspiciousActivity: (fingerprint, activity, details) => {
      logger.error({
        event: 'suspicious-activity',
        fingerprint: fingerprint?.substring(0, 8) + '...',
        activity,
        details,
        timestamp: new Date().toISOString()
      }, 'Suspicious activity detected');
    }
  },
  
  // Performance logging
  performance: {
    dbQuery: (query, duration, rows) => {
      const level = duration > 1000 ? 'warn' : duration > 500 ? 'info' : 'debug';
      logger[level]({
        event: 'db-query',
        query: query?.substring(0, 100) + (query?.length > 100 ? '...' : ''),
        duration: `${duration}ms`,
        rows,
        timestamp: new Date().toISOString()
      }, 'Database query executed');
    },
    
    apiResponse: (endpoint, method, duration, statusCode) => {
      const level = duration > 2000 ? 'warn' : duration > 1000 ? 'info' : 'debug';
      logger[level]({
        event: 'api-response',
        endpoint,
        method,
        duration: `${duration}ms`,
        statusCode,
        timestamp: new Date().toISOString()
      }, 'API response');
    }
  },
  
  // Business logic logging
  business: {
    voteSubmitted: (fingerprint, llmId, voteType, previousVote) => {
      logger.info({
        event: 'vote-submitted',
        fingerprint: fingerprint?.substring(0, 8) + '...',
        llmId,
        voteType,
        previousVote,
        timestamp: new Date().toISOString()
      }, 'Vote submitted');
    },
    
    fraudulentVoteDetected: (fingerprint, reason, details) => {
      logger.error({
        event: 'fraudulent-vote-detected',
        fingerprint: fingerprint?.substring(0, 8) + '...',
        reason,
        details,
        timestamp: new Date().toISOString()
      }, 'Fraudulent vote detected');
    }
  }
});

export default enhancedLogger;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\middleware.js
import Joi from 'joi';
import { NextResponse } from 'next/server';
import cacheManager from './cache.js';
import logger from './logger.js';

// Input validation schemas
const schemas = {
  vote: Joi.object({
    fingerprint: Joi.string().required().min(10).max(255),
    llmId: Joi.string().required().min(1).max(255),
    voteType: Joi.number().integer().valid(-1, 0, 1).required()
  }),
  
  sync: Joi.object({
    fingerprint: Joi.string().required().min(10).max(255)
  }),
  
  fingerprint: Joi.string().alphanum().min(10).max(255)
};

// Get client IP helper
function getClientIP(request) {
  const forwarded = request.headers.get('x-forwarded-for');
  const realIP = request.headers.get('x-real-ip');
  const remoteAddress = request.headers.get('remote-address');
  
  if (forwarded) {
    return forwarded.split(',')[0].trim();
  }
  
  return realIP || remoteAddress || 'unknown';
}

// Get user agent helper
function getUserAgent(request) {
  return request.headers.get('user-agent') || 'unknown';
}

// Rate limiting middleware
export async function rateLimit(request, options = {}) {
  const {
    maxRequests = 100,
    windowMs = 900000, // 15 minutes
    keyGenerator = (req) => getClientIP(req),
    skipSuccessfulRequests = false,
    skipFailedRequests = false
  } = options;

  try {
    const key = `rate_limit:${keyGenerator(request)}`;
    const rateLimitInfo = await cacheManager.checkRateLimit(key, maxRequests, windowMs);
    
    if (rateLimitInfo.exceeded) {
      logger.security.rateLimitExceeded(
        getClientIP(request),
        new URL(request.url).pathname,
        maxRequests
      );
      
      return NextResponse.json(
        {
          error: 'Rate limit exceeded',
          message: `Too many requests. Limit: ${maxRequests} per ${Math.floor(windowMs / 1000 / 60)} minutes`,
          retryAfter: rateLimitInfo.resetTime.toISOString()
        },
        { 
          status: 429,
          headers: {
            'X-RateLimit-Limit': maxRequests.toString(),
            'X-RateLimit-Remaining': rateLimitInfo.remainingRequests.toString(),
            'X-RateLimit-Reset': Math.floor(rateLimitInfo.resetTime.getTime() / 1000).toString(),
            'Retry-After': Math.ceil((rateLimitInfo.resetTime.getTime() - Date.now()) / 1000).toString()
          }
        }
      );
    }
    
    // Add rate limit headers to response
    return {
      rateLimitHeaders: {
        'X-RateLimit-Limit': maxRequests.toString(),
        'X-RateLimit-Remaining': rateLimitInfo.remainingRequests.toString(),
        'X-RateLimit-Reset': Math.floor(rateLimitInfo.resetTime.getTime() / 1000).toString()
      }
    };
  } catch (error) {
    logger.error('Rate limiting failed:', error);
    // Allow request to continue if rate limiting fails
    return null;
  }
}

// Input validation middleware
export function validateInput(schema, data) {
  try {
    const { error, value } = schema.validate(data, {
      stripUnknown: true,
      abortEarly: false
    });
    
    if (error) {
      const errorMessages = error.details.map(detail => ({
        field: detail.path.join('.'),
        message: detail.message,
        value: detail.context?.value
      }));
      
      return {
        isValid: false,
        errors: errorMessages,
        data: null
      };
    }
    
    return {
      isValid: true,
      errors: null,
      data: value
    };
  } catch (error) {
    logger.error('Input validation failed:', error);
    return {
      isValid: false,
      errors: [{ field: 'unknown', message: 'Validation error occurred' }],
      data: null
    };
  }
}

// Security headers middleware
export function securityHeaders() {
  return {
    'X-Frame-Options': 'DENY',
    'X-Content-Type-Options': 'nosniff',
    'X-XSS-Protection': '1; mode=block',
    'Referrer-Policy': 'strict-origin-when-cross-origin',
    'Content-Security-Policy': "default-src 'self'; script-src 'self' 'unsafe-eval' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' ws: wss:",
    'Strict-Transport-Security': 'max-age=31536000; includeSubDomains',
    'Permissions-Policy': 'geolocation=(), microphone=(), camera=()'
  };
}

// Request logging middleware
export function logRequest(request, startTime = Date.now()) {
  const ip = getClientIP(request);
  const userAgent = getUserAgent(request);
  const method = request.method;
  const url = new URL(request.url);
  const path = url.pathname;
  
  logger.info({
    event: 'request-start',
    method,
    path,
    ip: ip.substring(0, 8) + '...',
    userAgent: userAgent.substring(0, 100),
    timestamp: new Date().toISOString()
  }, `${method} ${path}`);
  
  return {
    ip,
    userAgent,
    method,
    path,
    startTime
  };
}

// Response logging middleware
export function logResponse(requestInfo, response, error = null) {
  const duration = Date.now() - requestInfo.startTime;
  const status = response?.status || (error ? 500 : 200);
  
  const logLevel = error ? 'error' : status >= 400 ? 'warn' : 'info';
  
  logger[logLevel]({
    event: 'request-end',
    method: requestInfo.method,
    path: requestInfo.path,
    status,
    duration: `${duration}ms`,
    ip: requestInfo.ip.substring(0, 8) + '...',
    error: error?.message,
    timestamp: new Date().toISOString()
  }, `${requestInfo.method} ${requestInfo.path} ${status} ${duration}ms`);
}

// Fraud detection middleware
export async function detectFraud(fingerprint, activity, context = {}) {
  try {
    const suspiciousIndicators = [];
    
    // Track activity
    await cacheManager.trackUserActivity(fingerprint, {
      type: activity,
      context,
      timestamp: Date.now()
    });
    
    // Get recent activity history
    const recentActivity = await cacheManager.getUserActivityHistory(fingerprint, 50);
    
    // Check for suspicious patterns
    
    // 1. Too many votes in short time
    if (activity === 'vote') {
      const recentVotes = recentActivity.filter(a => 
        a.activity.type === 'vote' && 
        Date.now() - a.activity.timestamp < 60000 // Last minute
      );
      
      if (recentVotes.length > 10) {
        suspiciousIndicators.push('rapid_voting');
      }
    }
    
    // 2. Repeated identical requests
    const identicalRequests = recentActivity.filter(a => 
      JSON.stringify(a.activity.context) === JSON.stringify(context) &&
      Date.now() - a.activity.timestamp < 300000 // Last 5 minutes
    );
    
    if (identicalRequests.length > 5) {
      suspiciousIndicators.push('repeated_requests');
    }
    
    // 3. Unusual voting patterns (alternating votes rapidly)
    if (activity === 'vote') {
      const recentVoteChanges = recentActivity
        .filter(a => a.activity.type === 'vote')
        .slice(0, 10);
      
      let alternatingPattern = 0;
      for (let i = 1; i < recentVoteChanges.length; i++) {
        const current = recentVoteChanges[i - 1].activity.context.voteType;
        const previous = recentVoteChanges[i].activity.context.voteType;
        if (current !== previous) {
          alternatingPattern++;
        }
      }
      
      if (alternatingPattern >= 5) {
        suspiciousIndicators.push('alternating_votes');
      }
    }
    
    // Log suspicious activity
    if (suspiciousIndicators.length > 0) {
      logger.security.suspiciousActivity(
        fingerprint,
        activity,
        {
          indicators: suspiciousIndicators,
          context,
          recentActivityCount: recentActivity.length
        }
      );
      
      return {
        isSuspicious: true,
        indicators: suspiciousIndicators,
        riskScore: suspiciousIndicators.length * 25 // 0-100 scale
      };
    }
    
    return {
      isSuspicious: false,
      indicators: [],
      riskScore: 0
    };
  } catch (error) {
    logger.error('Fraud detection failed:', error);
    return {
      isSuspicious: false,
      indicators: [],
      riskScore: 0
    };
  }
}

// Combined middleware for API routes
export async function apiMiddleware(request, options = {}) {
  const startTime = Date.now();
  const requestInfo = logRequest(request, startTime);
  
  try {
    // Apply rate limiting
    const rateLimitResult = await rateLimit(request, options.rateLimit);
    if (rateLimitResult?.status === 429) {
      logResponse(requestInfo, rateLimitResult);
      return rateLimitResult;
    }
    
    // Parse and validate request body if it exists
    let validatedData = null;
    if (request.method !== 'GET' && options.schema) {
      try {
        const body = await request.json();
        const validation = validateInput(options.schema, body);
        
        if (!validation.isValid) {
          const errorResponse = NextResponse.json(
            {
              error: 'Validation failed',
              details: validation.errors
            },
            { status: 400 }
          );
          
          logResponse(requestInfo, errorResponse);
          return errorResponse;
        }
        
        validatedData = validation.data;
      } catch (error) {
        const errorResponse = NextResponse.json(
          { error: 'Invalid JSON in request body' },
          { status: 400 }
        );
        
        logResponse(requestInfo, errorResponse);
        return errorResponse;
      }
    }
    
    return {
      success: true,
      requestInfo,
      validatedData,
      rateLimitHeaders: rateLimitResult?.rateLimitHeaders || {}
    };
  } catch (error) {
    logger.error('API middleware failed:', error);
    const errorResponse = NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    );
    
    logResponse(requestInfo, errorResponse, error);
    return errorResponse;
  }
}

// Export schemas for use in route handlers
export { schemas };

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\vote-manager-db.js
import dbManager from './database.js';
import cacheManager from './cache.js';
import logger from './logger.js';

class DatabaseVoteManager {
  constructor() {
    this.initialized = false;
  }

  async initialize() {
    if (this.initialized) return;
    
    try {
      await dbManager.initialize();
      await cacheManager.initialize();
      this.initialized = true;
      logger.info('DatabaseVoteManager initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize DatabaseVoteManager:', error);
      throw error;
    }
  }

  async vote(fingerprint, llmId, voteType, metadata = {}) {
    await this.initialize();
    
    try {
      return await dbManager.transaction(async (client) => {
        // Get or create user session
        let userSession = await this.getOrCreateUserSession(client, fingerprint, metadata);
        
        // Check for existing vote
        const { rows: existingVotes } = await client.query(
          'SELECT vote_type FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
          [fingerprint, llmId]
        );
        
        const currentVote = existingVotes[0]?.vote_type || 0;
        const voteChange = voteType - currentVote;
        
        if (voteChange === 0) {
          return { success: false, error: 'Vote unchanged' };
        }
        
        // Update or insert user vote
        if (currentVote === 0 && voteType !== 0) {
          // New vote
          await client.query(
            `INSERT INTO user_votes (fingerprint, llm_id, vote_type, previous_vote, ip_address, user_agent)
             VALUES ($1, $2, $3, $4, $5, $6)`,
            [fingerprint, llmId, voteType, currentVote, metadata.ip || null, metadata.userAgent || null]
          );
        } else if (voteType === 0) {
          // Remove vote
          await client.query(
            'DELETE FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
            [fingerprint, llmId]
          );
        } else {
          // Update vote
          await client.query(
            `UPDATE user_votes 
             SET vote_type = $3, previous_vote = $4, updated_at = NOW()
             WHERE fingerprint = $1 AND llm_id = $2`,
            [fingerprint, llmId, voteType, currentVote]
          );
        }
        
        // Update aggregate votes
        const { rows: voteRows } = await client.query(
          'SELECT * FROM votes WHERE llm_id = $1',
          [llmId]
        );
        
        if (voteRows.length === 0) {
          // Create vote record
          await client.query(
            `INSERT INTO votes (llm_id, vote_count, positive_votes, negative_votes)
             VALUES ($1, $2, $3, $4)`,
            [
              llmId,
              voteType,
              voteType === 1 ? 1 : 0,
              voteType === -1 ? 1 : 0
            ]
          );
        } else {
          // Update vote counts
          let { vote_count, positive_votes, negative_votes } = voteRows[0];
          vote_count += voteChange;
          
          // Adjust positive/negative counts
          if (currentVote === 1) positive_votes--;
          if (currentVote === -1) negative_votes--;
          if (voteType === 1) positive_votes++;
          if (voteType === -1) negative_votes++;
          
          await client.query(
            `UPDATE votes 
             SET vote_count = $2, positive_votes = $3, negative_votes = $4, updated_at = NOW()
             WHERE llm_id = $1`,
            [llmId, vote_count, positive_votes, negative_votes]
          );
        }
        
        // Update user session activity
        await client.query(
          `UPDATE user_sessions 
           SET last_activity = NOW(), vote_count = vote_count + 1
           WHERE fingerprint = $1`,
          [fingerprint]
        );
        
        // Log analytics event
        await client.query(
          `INSERT INTO analytics (event_type, event_data, fingerprint, ip_address, user_agent)
           VALUES ($1, $2, $3, $4, $5)`,
          [
            'vote',
            JSON.stringify({ llmId, voteType, previousVote: currentVote }),
            fingerprint,
            metadata.ip || null,
            metadata.userAgent || null
          ]
        );
        
        // CRITICAL FIX: Get fresh votes directly from database within transaction
        // This ensures we return the correct, up-to-date vote counts
        const { rows: freshVoteRows } = await client.query(`
          SELECT l.id, COALESCE(v.vote_count, 0) as count
          FROM llms l
          LEFT JOIN votes v ON l.id = v.llm_id
          ORDER BY l.id
        `);
        
        const votes = {};
        freshVoteRows.forEach(row => {
          votes[row.id] = parseInt(row.count);
        });
        
        // Now invalidate cache and update with fresh data
        await cacheManager.invalidateVoteCache(llmId);
        await cacheManager.setUserVote(fingerprint, llmId, voteType);
        await cacheManager.setAllVotes(votes); // Update cache with fresh data
        
        // Publish real-time update with correct vote count
        await cacheManager.publishVoteUpdate(llmId, {
          voteCount: votes[llmId],
          voteType,
          previousVote: currentVote
        });
        
        logger.business.voteSubmitted(fingerprint, llmId, voteType, currentVote);
        
        return {
          success: true,
          votes, // This now contains fresh, correct data from database
          userVote: voteType,
          previousVote: currentVote
        };
      });
    } catch (error) {
      logger.error('Vote transaction failed:', error);
      throw error;
    }
  }

  async getOrCreateUserSession(client, fingerprint, metadata) {
    const { rows } = await client.query(
      'SELECT * FROM user_sessions WHERE fingerprint = $1',
      [fingerprint]
    );
    
    if (rows.length === 0) {
      const { rows: newSession } = await client.query(
        `INSERT INTO user_sessions (fingerprint, ip_address, user_agent)
         VALUES ($1, $2, $3)
         RETURNING *`,
        [fingerprint, metadata.ip || null, metadata.userAgent || null]
      );
      return newSession[0];
    }
    
    return rows[0];
  }

  async getUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getUserVotes(fingerprint);
      if (cached) return cached;
      
      // Fetch from database
      const { rows } = await dbManager.query(
        'SELECT llm_id, vote_type FROM user_votes WHERE fingerprint = $1',
        [fingerprint]
      );
      
      const votes = {};
      rows.forEach(row => {
        votes[row.llm_id] = row.vote_type;
      });
      
      // Cache the result
      await cacheManager.setAllUserVotes(fingerprint, votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get user votes:', error);
      return {};
    }
  }

  async getVotes() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getAllVotes();
      if (cached) return cached;
      
      // Fetch from database
      const { rows } = await dbManager.query(`
        SELECT l.id, COALESCE(v.vote_count, 0) as count
        FROM llms l
        LEFT JOIN votes v ON l.id = v.llm_id
        ORDER BY l.id
      `);
      
      const votes = {};
      rows.forEach(row => {
        votes[row.id] = row.count;
      });
      
      // Cache the result
      await cacheManager.setAllVotes(votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get votes:', error);
      
      // Fallback to empty votes
      const votes = {};
      const { llmData } = await import('./llm-data.js');
      llmData.forEach(llm => {
        votes[llm.id] = 0;
      });
      return votes;
    }
  }

  async getRankings() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getRankings();
      if (cached) return cached;
      
      // Fetch from database
      const { rows } = await dbManager.query(`
        SELECT 
          l.id,
          COALESCE(v.vote_count, 0) as count,
          RANK() OVER (ORDER BY COALESCE(v.vote_count, 0) DESC) as rank
        FROM llms l
        LEFT JOIN votes v ON l.id = v.llm_id
        ORDER BY count DESC, l.id
      `);
      
      const rankings = rows.map(row => ({
        id: row.id,
        count: parseInt(row.count),
        rank: parseInt(row.rank)
      }));
      
      // Cache the result
      await cacheManager.setRankings(rankings);
      
      return rankings;
    } catch (error) {
      logger.error('Failed to get rankings:', error);
      return [];
    }
  }

  async getStats() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getStats();
      if (cached) return cached;
      
      const now = new Date();
      const oneHourAgo = new Date(now - 60 * 60 * 1000);
      const todayStart = new Date(now.setHours(0, 0, 0, 0));
      
      // Get total votes
      const { rows: totalRows } = await dbManager.query(
        'SELECT COALESCE(SUM(ABS(vote_count)), 0) as total FROM votes'
      );
      
      // Get votes today
      const { rows: todayRows } = await dbManager.query(
        'SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1',
        [todayStart]
      );
      
      // Get votes last hour
      const { rows: hourRows } = await dbManager.query(
        'SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1',
        [oneHourAgo]
      );
      
      // Get trending models (most voted in last hour)
      const { rows: trendingRows } = await dbManager.query(`
        SELECT llm_id, COUNT(*) as recent_votes
        FROM user_votes
        WHERE created_at >= $1 AND vote_type != 0
        GROUP BY llm_id
        ORDER BY recent_votes DESC
        LIMIT 3
      `, [oneHourAgo]);
      
      // Get top model
      const { rows: topRows } = await dbManager.query(`
        SELECT llm_id
        FROM votes
        WHERE vote_count = (SELECT MAX(vote_count) FROM votes)
        LIMIT 1
      `);
      
      const stats = {
        totalVotes: parseInt(totalRows[0].total),
        votesToday: parseInt(todayRows[0].count),
        votesLastHour: parseInt(hourRows[0].count),
        trending: trendingRows.map(row => row.llm_id),
        topModel: topRows[0]?.llm_id || null
      };
      
      // Cache the result
      await cacheManager.setStats(stats);
      
      return stats;
    } catch (error) {
      logger.error('Failed to get stats:', error);
      return {
        totalVotes: 0,
        votesToday: 0,
        votesLastHour: 0,
        trending: [],
        topModel: null
      };
    }
  }

  async syncUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      const userVotes = await this.getUserVotes(fingerprint);
      const allVotes = await this.getVotes();
      const rankings = await this.getRankings();
      const stats = await this.getStats();
      
      return {
        votes: allVotes,
        userVotes,
        rankings,
        stats
      };
    } catch (error) {
      logger.error('Failed to sync user votes:', error);
      throw error;
    }
  }

  async checkHealth() {
    try {
      const dbHealth = await dbManager.healthCheck();
      const cacheHealth = await cacheManager.healthCheck();
      
      return {
        database: dbHealth,
        cache: cacheHealth,
        status: dbHealth.postgres && cacheHealth.status === 'healthy' ? 'healthy' : 'degraded'
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
}

// Singleton instance
let voteManager;

export function getVoteManager() {
  if (!voteManager) {
    voteManager = new DatabaseVoteManager();
  }
  return voteManager;
}

export default getVoteManager();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\vote-manager-enhanced.js
import dbManager from './database.js';
import cacheManager from './cache.js';
import logger from './logger.js';

class EnhancedVoteManager {
  constructor() {
    this.initialized = false;
    this.materializedViewRefreshInterval = null;
  }

  async initialize() {
    if (this.initialized) return;
    
    try {
      await dbManager.initialize();
      await cacheManager.initialize();
      
      // Setup periodic materialized view refresh (if they exist)
      this.setupMaterializedViewRefresh();
      
      this.initialized = true;
      logger.info('Enhanced Vote Manager initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Enhanced Vote Manager:', error);
      throw error;
    }
  }

  setupMaterializedViewRefresh() {
    // Check if materialized views exist before setting up refresh
    this.materializedViewRefreshInterval = setInterval(async () => {
      try {
        await this.refreshMaterializedViews();
      } catch (error) {
        // Materialized views might not exist yet, that's ok
        logger.debug('Materialized views not available yet');
      }
    }, 5 * 60 * 1000); // Every 5 minutes
  }

  async refreshMaterializedViews() {
    try {
      // Try to refresh if they exist (PostgreSQL doesn't support IF EXISTS with REFRESH)
      await dbManager.query('REFRESH MATERIALIZED VIEW CONCURRENTLY mv_vote_summary');
    } catch (error) {
      // Ignore errors if views don't exist
      if (!error.message.includes('does not exist')) {
        logger.debug('Materialized view refresh error:', error.message);
      }
    }
    
    try {
      await dbManager.query('REFRESH MATERIALIZED VIEW CONCURRENTLY mv_hourly_stats');
    } catch (error) {
      // Ignore if view doesn't exist
      if (!error.message.includes('does not exist')) {
        logger.debug('Hourly stats view refresh error:', error.message);
      }
    }
  }

  async vote(fingerprint, llmId, voteType, metadata = {}) {
    await this.initialize();
    
    const start = Date.now();
    
    try {
      // Enhanced rate limiting
      const rateLimitKey = `rate_limit:vote:${fingerprint}`;
      const rateLimit = await cacheManager.checkRateLimit(rateLimitKey, 60, 60000);
      
      if (rateLimit.exceeded) {
        return {
          success: false,
          error: 'Rate limit exceeded',
          resetTime: rateLimit.resetTime
        };
      }
      
      return await dbManager.transaction(async (client) => {
        // Get or create user session with caching
        let userSession = await cacheManager.getUserSession(fingerprint);
        
        if (!userSession) {
          const { rows } = await client.query(
            'SELECT * FROM user_sessions WHERE fingerprint = $1',
            [fingerprint]
          );
          
          if (rows.length === 0) {
            const { rows: newSession } = await client.query(
              `INSERT INTO user_sessions (fingerprint, ip_address, user_agent)
               VALUES ($1, $2, $3)
               RETURNING *`,
              [fingerprint, metadata.ip || null, metadata.userAgent || null]
            );
            userSession = newSession[0];
          } else {
            userSession = rows[0];
          }
          
          // Cache the session
          await cacheManager.setUserSession(fingerprint, userSession);
        }
        
        // Check for existing vote
        const { rows: existingVotes } = await client.query(
          'SELECT vote_type FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
          [fingerprint, llmId]
        );
        
        const currentVote = existingVotes[0]?.vote_type || 0;
        const voteChange = voteType - currentVote;
        
        if (voteChange === 0) {
          return { success: false, error: 'Vote unchanged' };
        }
        
        // Update or insert user vote
        if (currentVote === 0 && voteType !== 0) {
          await client.query(
            `INSERT INTO user_votes (fingerprint, llm_id, vote_type, previous_vote, ip_address, user_agent)
             VALUES ($1, $2, $3, $4, $5, $6)`,
            [fingerprint, llmId, voteType, currentVote, metadata.ip || null, metadata.userAgent || null]
          );
        } else if (voteType === 0) {
          await client.query(
            'DELETE FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
            [fingerprint, llmId]
          );
        } else {
          await client.query(
            `UPDATE user_votes 
             SET vote_type = $3, previous_vote = $4, updated_at = NOW()
             WHERE fingerprint = $1 AND llm_id = $2`,
            [fingerprint, llmId, voteType, currentVote]
          );
        }
        
        // Update aggregate votes
        const { rows: voteRows } = await client.query(
          'SELECT * FROM votes WHERE llm_id = $1',
          [llmId]
        );
        
        if (voteRows.length === 0) {
          await client.query(
            `INSERT INTO votes (llm_id, vote_count, positive_votes, negative_votes)
             VALUES ($1, $2, $3, $4)`,
            [
              llmId,
              voteType,
              voteType === 1 ? 1 : 0,
              voteType === -1 ? 1 : 0
            ]
          );
        } else {
          let { vote_count, positive_votes, negative_votes } = voteRows[0];
          vote_count += voteChange;
          
          if (currentVote === 1) positive_votes--;
          if (currentVote === -1) negative_votes--;
          if (voteType === 1) positive_votes++;
          if (voteType === -1) negative_votes++;
          
          await client.query(
            `UPDATE votes 
             SET vote_count = $2, positive_votes = $3, negative_votes = $4, updated_at = NOW()
             WHERE llm_id = $1`,
            [llmId, vote_count, positive_votes, negative_votes]
          );
        }
        
        // Update user session activity
        await client.query(
          `UPDATE user_sessions 
           SET last_activity = NOW(), vote_count = vote_count + 1
           WHERE fingerprint = $1`,
          [fingerprint]
        );
        
        // Log analytics event
        await client.query(
          `INSERT INTO analytics (event_type, event_data, fingerprint, ip_address, user_agent)
           VALUES ($1, $2, $3, $4, $5)`,
          [
            'vote',
            JSON.stringify({ llmId, voteType, previousVote: currentVote }),
            fingerprint,
            metadata.ip || null,
            metadata.userAgent || null
          ]
        );
        
        // ALWAYS get fresh votes directly from votes table after an update
        // Don't use materialized view here as it may be stale
        const { rows: freshVoteRows } = await client.query(`
          SELECT l.id, COALESCE(v.vote_count, 0) as count
          FROM llms l
          LEFT JOIN votes v ON l.id = v.llm_id
          ORDER BY l.id
        `);
        
        const votes = {};
        freshVoteRows.forEach(row => {
          votes[row.id] = parseInt(row.count);
        });
        
        // Update caches
        await cacheManager.invalidateVoteCache(llmId);
        await cacheManager.setUserVote(fingerprint, llmId, voteType);
        await cacheManager.setAllVotes(votes);
        
        // Track activity for fraud detection
        await cacheManager.trackUserActivity(fingerprint, {
          action: 'vote',
          llmId,
          voteType,
          previousVote: currentVote
        });
        
        // Publish real-time update
        await cacheManager.publishVoteUpdate(llmId, {
          voteCount: votes[llmId],
          voteType,
          previousVote: currentVote
        });
        
        const duration = Date.now() - start;
        logger.info(`Vote processed in ${duration}ms`, {
          fingerprint: fingerprint.substring(0, 8) + '...',
          llmId,
          voteType,
          duration
        });
        
        return {
          success: true,
          votes,
          userVote: voteType,
          previousVote: currentVote
        };
      });
    } catch (error) {
      logger.error('Vote transaction failed:', error);
      throw error;
    }
  }

  async getUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getUserVotes(fingerprint);
      if (cached) return cached;
      
      // Fetch from database
      const { rows } = await dbManager.query(
        'SELECT llm_id, vote_type FROM user_votes WHERE fingerprint = $1',
        [fingerprint]
      );
      
      const votes = {};
      rows.forEach(row => {
        votes[row.llm_id] = row.vote_type;
      });
      
      // Cache the result
      await cacheManager.setAllUserVotes(fingerprint, votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get user votes:', error);
      return {};
    }
  }

  async getVotes() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getAllVotes();
      if (cached) return cached;
      
      // Try materialized view first
      let votes = {};
      try {
        const { rows } = await dbManager.query(
          'SELECT llm_id, vote_count FROM mv_vote_summary'
        );
        rows.forEach(row => {
          votes[row.llm_id] = parseInt(row.vote_count);
        });
      } catch (error) {
        // Fallback to regular query
        const { rows } = await dbManager.query(`
          SELECT l.id, COALESCE(v.vote_count, 0) as count
          FROM llms l
          LEFT JOIN votes v ON l.id = v.llm_id
          ORDER BY l.id
        `);
        
        rows.forEach(row => {
          votes[row.id] = parseInt(row.count);
        });
      }
      
      // Cache the result
      await cacheManager.setAllVotes(votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get votes:', error);
      
      // Fallback to empty votes
      const votes = {};
      const { llmData } = await import('./llm-data.js');
      llmData.forEach(llm => {
        votes[llm.id] = 0;
      });
      return votes;
    }
  }

  async getRankings() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getRankings();
      if (cached) return cached;
      
      // Try materialized view first
      let rankings = [];
      try {
        const { rows } = await dbManager.query(
          'SELECT llm_id as id, vote_count as count, rank FROM mv_vote_summary ORDER BY rank'
        );
        rankings = rows.map(row => ({
          id: row.id,
          count: parseInt(row.count),
          rank: parseInt(row.rank)
        }));
      } catch (error) {
        // Fallback to regular query
        const { rows } = await dbManager.query(`
          SELECT 
            l.id,
            COALESCE(v.vote_count, 0) as count,
            RANK() OVER (ORDER BY COALESCE(v.vote_count, 0) DESC) as rank
          FROM llms l
          LEFT JOIN votes v ON l.id = v.llm_id
          ORDER BY count DESC, l.id
        `);
        
        rankings = rows.map(row => ({
          id: row.id,
          count: parseInt(row.count),
          rank: parseInt(row.rank)
        }));
      }
      
      // Cache the result
      await cacheManager.setRankings(rankings);
      
      return rankings;
    } catch (error) {
      logger.error('Failed to get rankings:', error);
      return [];
    }
  }

  async getStats() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await cacheManager.getStats();
      if (cached) return cached;
      
      const now = new Date();
      const oneHourAgo = new Date(now - 60 * 60 * 1000);
      const todayStart = new Date(now.setHours(0, 0, 0, 0));
      
      // Use parallel queries for better performance
      const queries = await Promise.all([
        dbManager.query('SELECT COALESCE(SUM(ABS(vote_count)), 0) as total FROM votes'),
        dbManager.query('SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1', [todayStart]),
        dbManager.query('SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1', [oneHourAgo]),
        dbManager.query(`
          SELECT llm_id, COUNT(*) as recent_votes
          FROM user_votes
          WHERE created_at >= $1 AND vote_type != 0
          GROUP BY llm_id
          ORDER BY recent_votes DESC
          LIMIT 3
        `, [oneHourAgo]),
        dbManager.query(`
          SELECT llm_id
          FROM votes
          WHERE vote_count = (SELECT MAX(vote_count) FROM votes)
          LIMIT 1
        `)
      ]);
      
      const stats = {
        totalVotes: parseInt(queries[0].rows[0].total),
        votesToday: parseInt(queries[1].rows[0].count),
        votesLastHour: parseInt(queries[2].rows[0].count),
        trending: queries[3].rows.map(row => row.llm_id),
        topModel: queries[4].rows[0]?.llm_id || null
      };
      
      // Cache the result
      await cacheManager.setStats(stats);
      
      return stats;
    } catch (error) {
      logger.error('Failed to get stats:', error);
      return {
        totalVotes: 0,
        votesToday: 0,
        votesLastHour: 0,
        trending: [],
        topModel: null
      };
    }
  }

  async syncUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      // Use parallel fetching for better performance
      const [userVotes, allVotes, rankings, stats] = await Promise.all([
        this.getUserVotes(fingerprint),
        this.getVotes(),
        this.getRankings(),
        this.getStats()
      ]);
      
      return {
        votes: allVotes,
        userVotes,
        rankings,
        stats
      };
    } catch (error) {
      logger.error('Failed to sync user votes:', error);
      throw error;
    }
  }

  async detectFraud(fingerprint) {
    try {
      // Get user activity history
      const activities = await cacheManager.getUserActivityHistory(fingerprint, 100);
      
      // Check for suspicious patterns
      const suspiciousPatterns = {
        rapidVoting: false,
        sameTargetRepetition: false,
        unusualVolume: false
      };
      
      // Check for rapid voting
      const oneMinuteAgo = Date.now() - 60000;
      const recentVotes = activities.filter(a => 
        a.activity?.action === 'vote' && a.timestamp > oneMinuteAgo
      );
      
      if (recentVotes.length > 10) {
        suspiciousPatterns.rapidVoting = true;
      }
      
      // Check for same target repetition
      const voteCounts = {};
      activities.forEach(a => {
        if (a.activity?.action === 'vote' && a.activity?.llmId) {
          voteCounts[a.activity.llmId] = (voteCounts[a.activity.llmId] || 0) + 1;
        }
      });
      
      const maxVotes = Math.max(...Object.values(voteCounts), 0);
      if (maxVotes > 20) {
        suspiciousPatterns.sameTargetRepetition = true;
      }
      
      // Check for unusual volume
      if (activities.length > 100) {
        suspiciousPatterns.unusualVolume = true;
      }
      
      const isSuspicious = Object.values(suspiciousPatterns).some(v => v);
      
      if (isSuspicious) {
        logger.warn('Suspicious activity detected', { 
          fingerprint: fingerprint.substring(0, 8) + '...', 
          patterns: suspiciousPatterns 
        });
        
        // Implement temporary restrictions
        await cacheManager.setTemporaryRestriction(fingerprint, 3600);
      }
      
      return {
        suspicious: isSuspicious,
        patterns: suspiciousPatterns
      };
    } catch (error) {
      logger.error('Failed to detect fraud:', error);
      return { suspicious: false, patterns: {} };
    }
  }

  async checkHealth() {
    try {
      const dbHealth = await dbManager.healthCheck();
      const cacheHealth = await cacheManager.healthCheck();
      
      return {
        database: dbHealth,
        cache: cacheHealth,
        status: dbHealth.postgres && cacheHealth.status === 'healthy' ? 'healthy' : 'degraded'
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }

  async cleanup() {
    if (this.materializedViewRefreshInterval) {
      clearInterval(this.materializedViewRefreshInterval);
    }
  }
}

// Singleton instance
let voteManager;

export function getEnhancedVoteManager() {
  if (!voteManager) {
    voteManager = new EnhancedVoteManager();
  }
  return voteManager;
}

export default getEnhancedVoteManager();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\vote-manager-optimized.js
import scaledDbManager from './database-scaled.js';
import enhancedCacheManager from './cache-enhanced.js';
import logger from './logger.js';

class OptimizedVoteManager {
  constructor() {
    this.initialized = false;
    this.materializedViewRefreshInterval = null;
  }

  async initialize() {
    if (this.initialized) return;
    
    try {
      await scaledDbManager.initialize();
      await enhancedCacheManager.initialize();
      
      // Setup periodic materialized view refresh
      this.setupMaterializedViewRefresh();
      
      this.initialized = true;
      logger.info('Optimized Vote Manager initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Optimized Vote Manager:', error);
      throw error;
    }
  }

  setupMaterializedViewRefresh() {
    // Refresh materialized views every 5 minutes
    this.materializedViewRefreshInterval = setInterval(async () => {
      try {
        await this.refreshMaterializedViews();
      } catch (error) {
        logger.error('Failed to refresh materialized views:', error);
      }
    }, 5 * 60 * 1000);
  }

  async refreshMaterializedViews() {
    try {
      await scaledDbManager.query('REFRESH MATERIALIZED VIEW CONCURRENTLY mv_vote_summary');
      await scaledDbManager.query('REFRESH MATERIALIZED VIEW CONCURRENTLY mv_hourly_stats');
      logger.info('Materialized views refreshed successfully');
    } catch (error) {
      logger.error('Error refreshing materialized views:', error);
    }
  }

  async vote(fingerprint, llmId, voteType, metadata = {}) {
    await this.initialize();
    
    const start = Date.now();
    
    try {
      // Check rate limiting first
      const rateLimitKey = `rate_limit:vote:${fingerprint}`;
      const rateLimit = await enhancedCacheManager.checkRateLimit(rateLimitKey, 60, 60000);
      
      if (rateLimit.exceeded) {
        return {
          success: false,
          error: 'Rate limit exceeded',
          resetTime: rateLimit.resetTime
        };
      }
      
      // Use optimized transaction with retries
      return await scaledDbManager.transaction(async (client) => {
        // Get or create user session from cache first
        let userSession = await enhancedCacheManager.getUserSession(fingerprint);
        
        if (!userSession) {
          const { rows } = await client.query(
            'SELECT * FROM user_sessions WHERE fingerprint = $1',
            [fingerprint]
          );
          
          if (rows.length === 0) {
            const { rows: newSession } = await client.query(
              `INSERT INTO user_sessions (fingerprint, ip_address, user_agent)
               VALUES ($1, $2, $3)
               RETURNING *`,
              [fingerprint, metadata.ip || null, metadata.userAgent || null]
            );
            userSession = newSession[0];
          } else {
            userSession = rows[0];
          }
          
          // Cache the session
          await enhancedCacheManager.setUserSession(fingerprint, userSession);
        }
        
        // Check for existing vote
        const { rows: existingVotes } = await client.query(
          'SELECT vote_type FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
          [fingerprint, llmId]
        );
        
        const currentVote = existingVotes[0]?.vote_type || 0;
        const voteChange = voteType - currentVote;
        
        if (voteChange === 0) {
          return { success: false, error: 'Vote unchanged' };
        }
        
        // Update or insert user vote
        if (currentVote === 0 && voteType !== 0) {
          await client.query(
            `INSERT INTO user_votes (fingerprint, llm_id, vote_type, previous_vote, ip_address, user_agent)
             VALUES ($1, $2, $3, $4, $5, $6)`,
            [fingerprint, llmId, voteType, currentVote, metadata.ip || null, metadata.userAgent || null]
          );
        } else if (voteType === 0) {
          await client.query(
            'DELETE FROM user_votes WHERE fingerprint = $1 AND llm_id = $2',
            [fingerprint, llmId]
          );
        } else {
          await client.query(
            `UPDATE user_votes 
             SET vote_type = $3, previous_vote = $4, updated_at = NOW()
             WHERE fingerprint = $1 AND llm_id = $2`,
            [fingerprint, llmId, voteType, currentVote]
          );
        }
        
        // The trigger will automatically update vote counts
        // Get fresh votes from materialized view for better performance
        const { rows: voteRows } = await client.query(
          'SELECT llm_id, vote_count FROM mv_vote_summary'
        );
        
        const votes = {};
        voteRows.forEach(row => {
          votes[row.llm_id] = parseInt(row.vote_count);
        });
        
        // Update caches
        await enhancedCacheManager.invalidateVoteCache(llmId);
        await enhancedCacheManager.setUserVote(fingerprint, llmId, voteType);
        await enhancedCacheManager.setAllVotes(votes);
        
        // Track user activity for fraud detection
        await enhancedCacheManager.trackUserActivity(fingerprint, {
          action: 'vote',
          llmId,
          voteType,
          previousVote: currentVote
        });
        
        // Publish real-time update
        await enhancedCacheManager.publishVoteUpdate(llmId, {
          voteCount: votes[llmId],
          voteType,
          previousVote: currentVote,
          fingerprint
        });
        
        const duration = Date.now() - start;
        logger.info(`Vote processed in ${duration}ms`, {
          fingerprint,
          llmId,
          voteType,
          duration
        });
        
        return {
          success: true,
          votes,
          userVote: voteType,
          previousVote: currentVote
        };
      }, { maxRetries: 3, retryDelay: 100 });
    } catch (error) {
      logger.error('Vote transaction failed:', error);
      throw error;
    }
  }

  async getUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      // Try multi-layer cache first
      const cached = await enhancedCacheManager.getUserVotes(fingerprint);
      if (cached) return cached;
      
      // Fetch from read replica
      const { rows } = await scaledDbManager.query(
        'SELECT llm_id, vote_type FROM user_votes WHERE fingerprint = $1',
        [fingerprint],
        { isWrite: false }
      );
      
      const votes = {};
      rows.forEach(row => {
        votes[row.llm_id] = row.vote_type;
      });
      
      // Update cache
      await enhancedCacheManager.setAllUserVotes(fingerprint, votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get user votes:', error);
      return {};
    }
  }

  async getVotes() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await enhancedCacheManager.getAllVotes();
      if (cached) return cached;
      
      // Fetch from materialized view for better performance
      const { rows } = await scaledDbManager.query(
        'SELECT llm_id, vote_count FROM mv_vote_summary',
        [],
        { isWrite: false }
      );
      
      const votes = {};
      rows.forEach(row => {
        votes[row.llm_id] = parseInt(row.vote_count);
      });
      
      // Update cache
      await enhancedCacheManager.setAllVotes(votes);
      
      return votes;
    } catch (error) {
      logger.error('Failed to get votes:', error);
      
      // Fallback to direct query
      const { rows } = await scaledDbManager.query(`
        SELECT l.id, COALESCE(v.vote_count, 0) as count
        FROM llms l
        LEFT JOIN votes v ON l.id = v.llm_id
        ORDER BY l.id
      `, [], { isWrite: false });
      
      const votes = {};
      rows.forEach(row => {
        votes[row.id] = parseInt(row.count);
      });
      
      return votes;
    }
  }

  async getRankings() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await enhancedCacheManager.getRankings();
      if (cached) return cached;
      
      // Fetch from materialized view
      const { rows } = await scaledDbManager.query(
        'SELECT llm_id as id, vote_count as count, rank FROM mv_vote_summary ORDER BY rank',
        [],
        { isWrite: false }
      );
      
      const rankings = rows.map(row => ({
        id: row.id,
        count: parseInt(row.count),
        rank: parseInt(row.rank)
      }));
      
      // Update cache
      await enhancedCacheManager.setRankings(rankings);
      
      return rankings;
    } catch (error) {
      logger.error('Failed to get rankings:', error);
      return [];
    }
  }

  async getStats() {
    await this.initialize();
    
    try {
      // Try cache first
      const cached = await enhancedCacheManager.getStats();
      if (cached) return cached;
      
      const now = new Date();
      const oneHourAgo = new Date(now - 60 * 60 * 1000);
      const todayStart = new Date(now.setHours(0, 0, 0, 0));
      
      // Use parallel queries for better performance
      const [totalResult, todayResult, hourResult, trendingResult, topResult] = await Promise.all([
        scaledDbManager.query(
          'SELECT COALESCE(SUM(ABS(vote_count)), 0) as total FROM mv_vote_summary',
          [],
          { isWrite: false }
        ),
        scaledDbManager.query(
          'SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1',
          [todayStart],
          { isWrite: false }
        ),
        scaledDbManager.query(
          'SELECT COUNT(*) as count FROM user_votes WHERE created_at >= $1',
          [oneHourAgo],
          { isWrite: false }
        ),
        scaledDbManager.query(`
          SELECT llm_id, SUM(vote_count) as recent_votes
          FROM mv_hourly_stats
          WHERE hour >= $1
          GROUP BY llm_id
          ORDER BY recent_votes DESC
          LIMIT 3
        `, [oneHourAgo], { isWrite: false }),
        scaledDbManager.query(
          'SELECT llm_id FROM mv_vote_summary WHERE rank = 1 LIMIT 1',
          [],
          { isWrite: false }
        )
      ]);
      
      const stats = {
        totalVotes: parseInt(totalResult.rows[0].total),
        votesToday: parseInt(todayResult.rows[0].count),
        votesLastHour: parseInt(hourResult.rows[0].count),
        trending: trendingResult.rows.map(row => row.llm_id),
        topModel: topResult.rows[0]?.llm_id || null
      };
      
      // Update cache
      await enhancedCacheManager.setStats(stats);
      
      return stats;
    } catch (error) {
      logger.error('Failed to get stats:', error);
      return {
        totalVotes: 0,
        votesToday: 0,
        votesLastHour: 0,
        trending: [],
        topModel: null
      };
    }
  }

  async syncUserVotes(fingerprint) {
    await this.initialize();
    
    try {
      // Use parallel fetching for better performance
      const [userVotes, allVotes, rankings, stats] = await Promise.all([
        this.getUserVotes(fingerprint),
        this.getVotes(),
        this.getRankings(),
        this.getStats()
      ]);
      
      return {
        votes: allVotes,
        userVotes,
        rankings,
        stats
      };
    } catch (error) {
      logger.error('Failed to sync user votes:', error);
      throw error;
    }
  }

  async detectFraud(fingerprint) {
    try {
      // Get user activity history
      const activities = await enhancedCacheManager.getUserActivityHistory(fingerprint, 100);
      
      // Check for suspicious patterns
      const suspiciousPatterns = {
        rapidVoting: false,
        sameTargetRepetition: false,
        unusualVolume: false
      };
      
      // Check for rapid voting (more than 10 votes per minute)
      const oneMinuteAgo = Date.now() - 60000;
      const recentVotes = activities.filter(a => 
        a.activity.action === 'vote' && a.timestamp > oneMinuteAgo
      );
      
      if (recentVotes.length > 10) {
        suspiciousPatterns.rapidVoting = true;
      }
      
      // Check for same target repetition
      const voteCounts = {};
      activities.forEach(a => {
        if (a.activity.action === 'vote') {
          voteCounts[a.activity.llmId] = (voteCounts[a.activity.llmId] || 0) + 1;
        }
      });
      
      const maxVotes = Math.max(...Object.values(voteCounts));
      if (maxVotes > 20) {
        suspiciousPatterns.sameTargetRepetition = true;
      }
      
      // Check for unusual volume
      if (activities.length > 100) {
        suspiciousPatterns.unusualVolume = true;
      }
      
      const isSuspicious = Object.values(suspiciousPatterns).some(v => v);
      
      if (isSuspicious) {
        logger.warn('Suspicious activity detected', { fingerprint, patterns: suspiciousPatterns });
        
        // Implement temporary restrictions
        await enhancedCacheManager.setTemporaryRestriction(fingerprint, 3600); // 1 hour
      }
      
      return {
        suspicious: isSuspicious,
        patterns: suspiciousPatterns
      };
    } catch (error) {
      logger.error('Failed to detect fraud:', error);
      return { suspicious: false, patterns: {} };
    }
  }

  async checkHealth() {
    try {
      const dbHealth = await scaledDbManager.healthCheck();
      const cacheHealth = await enhancedCacheManager.healthCheck();
      const poolStats = await scaledDbManager.getPoolStats();
      
      const isHealthy = dbHealth.postgresqlPrimary && 
                       dbHealth.redis && 
                       cacheHealth.status === 'healthy';
      
      return {
        database: dbHealth,
        cache: cacheHealth,
        poolStats,
        status: isHealthy ? 'healthy' : 'degraded'
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }

  async cleanup() {
    if (this.materializedViewRefreshInterval) {
      clearInterval(this.materializedViewRefreshInterval);
    }
    
    await scaledDbManager.closeAll();
  }
}

// Singleton instance
let voteManager;

export function getOptimizedVoteManager() {
  if (!voteManager) {
    voteManager = new OptimizedVoteManager();
  }
  return voteManager;
}

export default getOptimizedVoteManager();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\vote-manager-wrapper.js
import config from './config.js';
import logger from './logger.js';

// This wrapper now exclusively uses the database vote manager
// No fallback to file storage - designed for 500k+ users

let voteManager;

export async function getVoteManager() {
  if (voteManager) {
    return voteManager;
  }
  
  // Always use database-backed manager - no fallback
  try {
    const { getEnhancedVoteManager: getDbVoteManager } = await import('./vote-manager-enhanced.js');
    voteManager = getDbVoteManager();
    
    // Test the connection
    await voteManager.initialize();
    
    logger.info('Database-backed vote manager initialized successfully');
    return voteManager;
  } catch (error) {
    logger.error('Failed to initialize database vote manager:', error);
    throw new Error(`Database connection required. Please ensure PostgreSQL and Redis are running. Error: ${error.message}`);
  }
}

// Direct export of database manager - no adapter needed
export async function getAdaptedVoteManager() {
  return await getVoteManager();
}

// Always using database storage
export function isUsingDatabaseStorage() {
  return true;
}

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\lib\websocket-server.js
import { Server } from 'socket.io';
import { createAdapter } from '@socket.io/redis-adapter';
import enhancedCacheManager from './cache-enhanced.js';
import scaledDbManager from './database-scaled.js';
import logger from './logger.js';

class WebSocketServer {
  constructor() {
    this.io = null;
    this.rooms = new Map();
    this.userConnections = new Map();
    this.metrics = {
      totalConnections: 0,
      activeConnections: 0,
      messagesPerSecond: 0,
      roomSubscriptions: new Map()
    };
  }

  async initialize(server, options = {}) {
    try {
      // Create Socket.IO server with optimizations for scale
      this.io = new Server(server, {
        cors: {
          origin: process.env.CORS_ORIGIN || '*',
          methods: ['GET', 'POST']
        },
        transports: ['websocket', 'polling'],
        pingTimeout: 60000,
        pingInterval: 25000,
        upgradeTimeout: 30000,
        maxHttpBufferSize: 1e6,
        allowEIO3: true,
        ...options
      });

      // Setup Redis adapter for horizontal scaling
      await this.setupRedisAdapter();

      // Setup middleware
      this.setupMiddleware();

      // Setup event handlers
      this.setupEventHandlers();

      // Setup metrics collection
      this.setupMetrics();

      logger.info('WebSocket server initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize WebSocket server:', error);
      throw error;
    }
  }

  async setupRedisAdapter() {
    try {
      const pubClient = scaledDbManager.getRedis('pubsub');
      const subClient = pubClient.duplicate();
      
      this.io.adapter(createAdapter(pubClient, subClient));
      
      logger.info('Redis adapter configured for Socket.IO');
    } catch (error) {
      logger.error('Failed to setup Redis adapter:', error);
      // Fallback to in-memory adapter
      logger.warn('Using in-memory adapter (not suitable for multi-instance)');
    }
  }

  setupMiddleware() {
    // Authentication middleware
    this.io.use(async (socket, next) => {
      try {
        const fingerprint = socket.handshake.auth.fingerprint;
        
        if (!fingerprint) {
          return next(new Error('Authentication failed: No fingerprint'));
        }

        // Check for restrictions
        const restriction = await enhancedCacheManager.checkRestriction(fingerprint);
        if (restriction && restriction.restricted) {
          return next(new Error('Access restricted'));
        }

        // Rate limiting per connection
        const rateLimitKey = `ws_connect:${fingerprint}`;
        const rateLimit = await enhancedCacheManager.checkRateLimit(rateLimitKey, 10, 60000);
        
        if (rateLimit.exceeded) {
          return next(new Error('Connection rate limit exceeded'));
        }

        socket.fingerprint = fingerprint;
        socket.userId = socket.handshake.auth.userId || fingerprint;
        
        next();
      } catch (error) {
        logger.error('WebSocket authentication error:', error);
        next(error);
      }
    });

    // Logging middleware
    this.io.use((socket, next) => {
      logger.debug('WebSocket connection attempt', {
        fingerprint: socket.fingerprint,
        ip: socket.handshake.address
      });
      next();
    });
  }

  setupEventHandlers() {
    this.io.on('connection', async (socket) => {
      this.metrics.totalConnections++;
      this.metrics.activeConnections++;

      // Track user connection
      this.trackUserConnection(socket);

      logger.info('WebSocket client connected', {
        id: socket.id,
        fingerprint: socket.fingerprint
      });

      // Auto-join user to their personal room
      socket.join(`user:${socket.fingerprint}`);

      // Handle room subscriptions
      socket.on('subscribe', async (data) => {
        await this.handleSubscribe(socket, data);
      });

      socket.on('unsubscribe', async (data) => {
        await this.handleUnsubscribe(socket, data);
      });

      // Handle vote events
      socket.on('vote', async (data) => {
        await this.handleVote(socket, data);
      });

      // Handle sync request
      socket.on('sync', async (callback) => {
        await this.handleSync(socket, callback);
      });

      // Handle stats request
      socket.on('getStats', async (callback) => {
        await this.handleGetStats(socket, callback);
      });

      // Handle ping for latency measurement
      socket.on('ping', (callback) => {
        if (typeof callback === 'function') {
          callback({ timestamp: Date.now() });
        }
      });

      // Handle disconnection
      socket.on('disconnect', (reason) => {
        this.metrics.activeConnections--;
        this.untrackUserConnection(socket);
        
        logger.info('WebSocket client disconnected', {
          id: socket.id,
          fingerprint: socket.fingerprint,
          reason
        });
      });

      // Send initial data
      await this.sendInitialData(socket);
    });
  }

  trackUserConnection(socket) {
    if (!this.userConnections.has(socket.fingerprint)) {
      this.userConnections.set(socket.fingerprint, new Set());
    }
    this.userConnections.get(socket.fingerprint).add(socket.id);
  }

  untrackUserConnection(socket) {
    const connections = this.userConnections.get(socket.fingerprint);
    if (connections) {
      connections.delete(socket.id);
      if (connections.size === 0) {
        this.userConnections.delete(socket.fingerprint);
      }
    }
  }

  async handleSubscribe(socket, data) {
    try {
      const { type, id } = data;
      
      // Validate subscription type
      const validTypes = ['llm', 'rankings', 'stats', 'all'];
      if (!validTypes.includes(type)) {
        socket.emit('error', { message: 'Invalid subscription type' });
        return;
      }

      const room = `${type}:${id || 'global'}`;
      socket.join(room);

      // Track room subscription
      if (!this.metrics.roomSubscriptions.has(room)) {
        this.metrics.roomSubscriptions.set(room, 0);
      }
      this.metrics.roomSubscriptions.set(
        room,
        this.metrics.roomSubscriptions.get(room) + 1
      );

      socket.emit('subscribed', { room, type, id });
      
      logger.debug('Client subscribed to room', {
        socketId: socket.id,
        room
      });
    } catch (error) {
      logger.error('Subscribe error:', error);
      socket.emit('error', { message: 'Subscription failed' });
    }
  }

  async handleUnsubscribe(socket, data) {
    try {
      const { type, id } = data;
      const room = `${type}:${id || 'global'}`;
      
      socket.leave(room);
      
      // Update room subscription count
      if (this.metrics.roomSubscriptions.has(room)) {
        const count = this.metrics.roomSubscriptions.get(room) - 1;
        if (count <= 0) {
          this.metrics.roomSubscriptions.delete(room);
        } else {
          this.metrics.roomSubscriptions.set(room, count);
        }
      }

      socket.emit('unsubscribed', { room });
      
      logger.debug('Client unsubscribed from room', {
        socketId: socket.id,
        room
      });
    } catch (error) {
      logger.error('Unsubscribe error:', error);
      socket.emit('error', { message: 'Unsubscription failed' });
    }
  }

  async handleVote(socket, data) {
    try {
      const { llmId, voteType } = data;
      
      // Rate limiting for votes
      const rateLimitKey = `ws_vote:${socket.fingerprint}`;
      const rateLimit = await enhancedCacheManager.checkRateLimit(rateLimitKey, 60, 60000);
      
      if (rateLimit.exceeded) {
        socket.emit('voteError', {
          error: 'Rate limit exceeded',
          resetTime: rateLimit.resetTime
        });
        return;
      }

      // Process vote (delegate to vote manager)
      // This is a placeholder - integrate with your vote manager
      socket.emit('voteConfirmed', {
        llmId,
        voteType,
        timestamp: Date.now()
      });

      // Broadcast update to relevant rooms
      this.broadcastVoteUpdate(llmId, {
        fingerprint: socket.fingerprint,
        voteType
      });
    } catch (error) {
      logger.error('Vote handling error:', error);
      socket.emit('voteError', { error: 'Vote failed' });
    }
  }

  async handleSync(socket, callback) {
    try {
      // Get latest data from cache
      const [votes, rankings, stats] = await Promise.all([
        enhancedCacheManager.getAllVotes(),
        enhancedCacheManager.getRankings(),
        enhancedCacheManager.getStats()
      ]);

      const userVotes = await enhancedCacheManager.getUserVotes(socket.fingerprint);

      if (typeof callback === 'function') {
        callback({
          success: true,
          data: {
            votes: votes || {},
            userVotes: userVotes || {},
            rankings: rankings || [],
            stats: stats || {}
          },
          timestamp: Date.now()
        });
      }
    } catch (error) {
      logger.error('Sync error:', error);
      if (typeof callback === 'function') {
        callback({
          success: false,
          error: 'Sync failed'
        });
      }
    }
  }

  async handleGetStats(socket, callback) {
    try {
      const stats = await enhancedCacheManager.getStats();
      
      if (typeof callback === 'function') {
        callback({
          success: true,
          stats: stats || {},
          connectionStats: {
            totalConnections: this.metrics.totalConnections,
            activeConnections: this.metrics.activeConnections,
            roomSubscriptions: this.metrics.roomSubscriptions.size
          },
          timestamp: Date.now()
        });
      }
    } catch (error) {
      logger.error('Get stats error:', error);
      if (typeof callback === 'function') {
        callback({
          success: false,
          error: 'Failed to get stats'
        });
      }
    }
  }

  async sendInitialData(socket) {
    try {
      const [votes, rankings, stats] = await Promise.all([
        enhancedCacheManager.getAllVotes(),
        enhancedCacheManager.getRankings(),
        enhancedCacheManager.getStats()
      ]);

      const userVotes = await enhancedCacheManager.getUserVotes(socket.fingerprint);

      socket.emit('initialData', {
        votes: votes || {},
        userVotes: userVotes || {},
        rankings: rankings || [],
        stats: stats || {},
        timestamp: Date.now()
      });
    } catch (error) {
      logger.error('Failed to send initial data:', error);
      socket.emit('error', { message: 'Failed to load initial data' });
    }
  }

  // Broadcast methods
  broadcastVoteUpdate(llmId, voteData) {
    const updateData = {
      type: 'voteUpdate',
      llmId,
      ...voteData,
      timestamp: Date.now()
    };

    // Broadcast to specific LLM room
    this.io.to(`llm:${llmId}`).emit('voteUpdate', updateData);
    
    // Broadcast to global room
    this.io.to('all:global').emit('voteUpdate', updateData);
  }

  broadcastRankingsUpdate(rankings) {
    const updateData = {
      type: 'rankingsUpdate',
      rankings,
      timestamp: Date.now()
    };

    this.io.to('rankings:global').emit('rankingsUpdate', updateData);
    this.io.to('all:global').emit('rankingsUpdate', updateData);
  }

  broadcastStatsUpdate(stats) {
    const updateData = {
      type: 'statsUpdate',
      stats,
      timestamp: Date.now()
    };

    this.io.to('stats:global').emit('statsUpdate', updateData);
    this.io.to('all:global').emit('statsUpdate', updateData);
  }

  // Setup metrics collection
  setupMetrics() {
    setInterval(() => {
      // Calculate messages per second
      // This would need actual message counting implementation
      
      logger.debug('WebSocket metrics', {
        totalConnections: this.metrics.totalConnections,
        activeConnections: this.metrics.activeConnections,
        uniqueUsers: this.userConnections.size,
        activeRooms: this.metrics.roomSubscriptions.size
      });
    }, 60000); // Every minute
  }

  // Get server statistics
  getStats() {
    return {
      totalConnections: this.metrics.totalConnections,
      activeConnections: this.metrics.activeConnections,
      uniqueUsers: this.userConnections.size,
      rooms: Array.from(this.metrics.roomSubscriptions.entries()).map(([room, count]) => ({
        room,
        subscribers: count
      }))
    };
  }

  // Graceful shutdown
  async shutdown() {
    logger.info('Shutting down WebSocket server...');
    
    // Notify all clients
    this.io.emit('serverShutdown', {
      message: 'Server is shutting down',
      timestamp: Date.now()
    });

    // Close all connections
    this.io.close();
    
    logger.info('WebSocket server shut down successfully');
  }
}

// Singleton instance
const wsServer = new WebSocketServer();

export default wsServer;

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\monitoring\alerts\alerts.yml
groups:
  - name: llm_tracker_alerts
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current value: {{ $value }}%)"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current value: {{ $value }}%)"

      # PostgreSQL down
      - alert: PostgreSQLDown
        expr: up{job="postgresql"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL exporter is not responding"

      # Redis down
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis exporter is not responding"

      # Application down
      - alert: ApplicationDown
        expr: up{job="app"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Application instance {{ $labels.instance }} is down"
          description: "Application metrics endpoint is not responding"

      # High database connections
      - alert: HighDatabaseConnections
        expr: pg_stat_database_numbackends{datname="llm_tracker"} > 400
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High number of database connections"
          description: "Database has {{ $value }} active connections"

      # Slow queries
      - alert: SlowQueries
        expr: rate(pg_stat_database_blks_read{datname="llm_tracker"}[5m]) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk read rate indicating slow queries"
          description: "Database is reading {{ $value }} blocks/sec from disk"

      # Redis memory usage
      - alert: RedisHighMemory
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis is using {{ $value }}% of max memory"

      # Disk space
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Less than 10% disk space remaining ({{ $value }}%)"

      # Network errors
      - alert: HighNetworkErrors
        expr: rate(node_network_receive_errs_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network receive errors on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} has {{ $value }} errors/sec"

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\monitoring\prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  scrape_timeout: 10s

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: []

# Load rules once and periodically evaluate them
rule_files:
  - "alerts/*.yml"

# Scrape configurations
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'llm-tracker-node'

  # PostgreSQL Exporter
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'llm-tracker-postgres'

  # Redis Exporter
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'llm-tracker-redis'

  # Application metrics (if exposed)
  - job_name: 'app'
    metrics_path: '/api/metrics'
    static_configs:
      - targets:
          - 'app:3000'
          - 'app_2:3000'
          - 'app_3:3000'
          - 'app_4:3000'
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        regex: 'app(_\d+)?:3000'
        replacement: 'app${1}'

  # Nginx metrics (if nginx-prometheus-exporter is added)
  # - job_name: 'nginx'
  #   static_configs:
  #     - targets: ['nginx-exporter:9113']

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\migrate.js
const { Pool } = require('pg');
const path = require('path');

// Database configuration
const dbConfig = {
  connectionString: process.env.DATABASE_URL || 'postgresql://llm_user:changeme@localhost:5432/llm_tracker',
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
};

const pool = new Pool(dbConfig);

const migrations = [
  {
    name: '001_create_llms_table',
    up: `
      CREATE TABLE IF NOT EXISTS llms (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        company VARCHAR(255) NOT NULL,
        description TEXT,
        use_cases JSONB DEFAULT '[]',
        release_year INTEGER,
        color VARCHAR(100),
        logo VARCHAR(10),
        image_url VARCHAR(500),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- Create index for faster queries
      CREATE INDEX IF NOT EXISTS idx_llms_company ON llms(company);
      CREATE INDEX IF NOT EXISTS idx_llms_release_year ON llms(release_year);
    `,
    down: `
      DROP INDEX IF EXISTS idx_llms_release_year;
      DROP INDEX IF EXISTS idx_llms_company;
      DROP TABLE IF EXISTS llms;
    `
  },
  
  {
    name: '002_create_votes_table',
    up: `
      CREATE TABLE IF NOT EXISTS votes (
        id SERIAL PRIMARY KEY,
        llm_id VARCHAR(255) NOT NULL REFERENCES llms(id) ON DELETE CASCADE,
        vote_count INTEGER DEFAULT 0,
        positive_votes INTEGER DEFAULT 0,
        negative_votes INTEGER DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        UNIQUE(llm_id)
      );
      
      -- Create indexes for performance
      CREATE INDEX IF NOT EXISTS idx_votes_llm_id ON votes(llm_id);
      CREATE INDEX IF NOT EXISTS idx_votes_vote_count ON votes(vote_count DESC);
    `,
    down: `
      DROP INDEX IF EXISTS idx_votes_vote_count;
      DROP INDEX IF EXISTS idx_votes_llm_id;
      DROP TABLE IF EXISTS votes;
    `
  },
  
  {
    name: '003_create_user_sessions_table',
    up: `
      CREATE TABLE IF NOT EXISTS user_sessions (
        id SERIAL PRIMARY KEY,
        fingerprint VARCHAR(255) NOT NULL,
        ip_address INET,
        user_agent TEXT,
        country VARCHAR(2),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_activity TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        is_suspicious BOOLEAN DEFAULT FALSE,
        vote_count INTEGER DEFAULT 0,
        UNIQUE(fingerprint)
      );
      
      -- Create indexes
      CREATE INDEX IF NOT EXISTS idx_user_sessions_fingerprint ON user_sessions(fingerprint);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_last_activity ON user_sessions(last_activity);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_suspicious ON user_sessions(is_suspicious);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_ip ON user_sessions(ip_address);
    `,
    down: `
      DROP INDEX IF EXISTS idx_user_sessions_ip;
      DROP INDEX IF EXISTS idx_user_sessions_suspicious;
      DROP INDEX IF EXISTS idx_user_sessions_last_activity;
      DROP INDEX IF EXISTS idx_user_sessions_fingerprint;
      DROP TABLE IF EXISTS user_sessions;
    `
  },
  
  {
    name: '004_create_user_votes_table',
    up: `
      CREATE TABLE IF NOT EXISTS user_votes (
        id SERIAL PRIMARY KEY,
        fingerprint VARCHAR(255) NOT NULL,
        llm_id VARCHAR(255) NOT NULL REFERENCES llms(id) ON DELETE CASCADE,
        vote_type INTEGER NOT NULL CHECK (vote_type IN (-1, 0, 1)),
        previous_vote INTEGER DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        ip_address INET,
        user_agent TEXT,
        UNIQUE(fingerprint, llm_id)
      );
      
      -- Create indexes for performance
      CREATE INDEX IF NOT EXISTS idx_user_votes_fingerprint ON user_votes(fingerprint);
      CREATE INDEX IF NOT EXISTS idx_user_votes_llm_id ON user_votes(llm_id);
      CREATE INDEX IF NOT EXISTS idx_user_votes_created_at ON user_votes(created_at DESC);
      CREATE INDEX IF NOT EXISTS idx_user_votes_vote_type ON user_votes(vote_type);
    `,
    down: `
      DROP INDEX IF EXISTS idx_user_votes_vote_type;
      DROP INDEX IF EXISTS idx_user_votes_created_at;
      DROP INDEX IF EXISTS idx_user_votes_llm_id;
      DROP INDEX IF EXISTS idx_user_votes_fingerprint;
      DROP TABLE IF EXISTS user_votes;
    `
  },
  
  {
    name: '005_create_analytics_table',
    up: `
      CREATE TABLE IF NOT EXISTS analytics (
        id SERIAL PRIMARY KEY,
        event_type VARCHAR(100) NOT NULL,
        event_data JSONB NOT NULL DEFAULT '{}',
        fingerprint VARCHAR(255),
        ip_address INET,
        user_agent TEXT,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- Create indexes for analytics queries
      CREATE INDEX IF NOT EXISTS idx_analytics_event_type ON analytics(event_type);
      CREATE INDEX IF NOT EXISTS idx_analytics_created_at ON analytics(created_at DESC);
      CREATE INDEX IF NOT EXISTS idx_analytics_fingerprint ON analytics(fingerprint);
    `,
    down: `
      DROP INDEX IF EXISTS idx_analytics_fingerprint;
      DROP INDEX IF EXISTS idx_analytics_created_at;
      DROP INDEX IF EXISTS idx_analytics_event_type;
      DROP TABLE IF EXISTS analytics;
    `
  },
  
  {
    name: '006_create_rate_limits_table',
    up: `
      CREATE TABLE IF NOT EXISTS rate_limits (
        id SERIAL PRIMARY KEY,
        key VARCHAR(255) NOT NULL,
        points INTEGER DEFAULT 0,
        expire_at TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        UNIQUE(key)
      );
      
      -- Create indexes
      CREATE INDEX IF NOT EXISTS idx_rate_limits_key ON rate_limits(key);
      CREATE INDEX IF NOT EXISTS idx_rate_limits_expire_at ON rate_limits(expire_at);
    `,
    down: `
      DROP INDEX IF EXISTS idx_rate_limits_expire_at;
      DROP INDEX IF EXISTS idx_rate_limits_key;
      DROP TABLE IF EXISTS rate_limits;
    `
  },
  
  {
    name: '007_create_triggers',
    up: `
      -- Create function to update timestamp
      CREATE OR REPLACE FUNCTION update_updated_at_column()
      RETURNS TRIGGER AS $$
      BEGIN
          NEW.updated_at = NOW();
          RETURN NEW;
      END;
      $$ language 'plpgsql';
      
      -- Create triggers for updated_at
      CREATE TRIGGER update_llms_updated_at BEFORE UPDATE ON llms 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        
      CREATE TRIGGER update_votes_updated_at BEFORE UPDATE ON votes 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        
      CREATE TRIGGER update_user_votes_updated_at BEFORE UPDATE ON user_votes 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
    `,
    down: `
      DROP TRIGGER IF EXISTS update_user_votes_updated_at ON user_votes;
      DROP TRIGGER IF EXISTS update_votes_updated_at ON votes;
      DROP TRIGGER IF EXISTS update_llms_updated_at ON llms;
      DROP FUNCTION IF EXISTS update_updated_at_column();
    `
  }
];

async function runMigrations() {
  const client = await pool.connect();
  
  try {
    console.log('Starting database migrations...');
    
    // Create migrations table
    await client.query(`
      CREATE TABLE IF NOT EXISTS migrations (
        id SERIAL PRIMARY KEY,
        name VARCHAR(255) NOT NULL UNIQUE,
        executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Get already executed migrations
    const { rows: executedMigrations } = await client.query(
      'SELECT name FROM migrations ORDER BY id'
    );
    
    const executedNames = executedMigrations.map(row => row.name);
    
    // Run pending migrations
    for (const migration of migrations) {
      if (!executedNames.includes(migration.name)) {
        console.log(`Running migration: ${migration.name}`);
        
        // Start transaction
        await client.query('BEGIN');
        
        try {
          await client.query(migration.up);
          await client.query(
            'INSERT INTO migrations (name) VALUES ($1)',
            [migration.name]
          );
          await client.query('COMMIT');
          
          console.log(` Migration completed: ${migration.name}`);
        } catch (error) {
          await client.query('ROLLBACK');
          throw error;
        }
      } else {
        console.log(` Migration already executed: ${migration.name}`);
      }
    }
    
    console.log('\nAll migrations completed successfully!');
  } catch (error) {
    console.error('Migration failed:', error);
    process.exit(1);
  } finally {
    client.release();
    await pool.end();
  }
}

// Run migrations
runMigrations();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\migrate.mjs
import dbManager from '../lib/database.js';
import logger from '../lib/logger.js';

const migrations = [
  {
    name: '001_create_llms_table',
    up: `
      CREATE TABLE IF NOT EXISTS llms (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        company VARCHAR(255) NOT NULL,
        description TEXT,
        use_cases JSONB DEFAULT '[]',
        release_year INTEGER,
        color VARCHAR(100),
        logo VARCHAR(10),
        image_url VARCHAR(500),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- Create index for faster queries
      CREATE INDEX IF NOT EXISTS idx_llms_company ON llms(company);
      CREATE INDEX IF NOT EXISTS idx_llms_release_year ON llms(release_year);
    `,
    down: `
      DROP INDEX IF EXISTS idx_llms_release_year;
      DROP INDEX IF EXISTS idx_llms_company;
      DROP TABLE IF EXISTS llms;
    `
  },
  
  {
    name: '002_create_votes_table',
    up: `
      CREATE TABLE IF NOT EXISTS votes (
        id SERIAL PRIMARY KEY,
        llm_id VARCHAR(255) NOT NULL REFERENCES llms(id) ON DELETE CASCADE,
        vote_count INTEGER DEFAULT 0,
        positive_votes INTEGER DEFAULT 0,
        negative_votes INTEGER DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        UNIQUE(llm_id)
      );
      
      -- Create indexes for performance
      CREATE INDEX IF NOT EXISTS idx_votes_llm_id ON votes(llm_id);
      CREATE INDEX IF NOT EXISTS idx_votes_vote_count ON votes(vote_count DESC);
    `,
    down: `
      DROP INDEX IF EXISTS idx_votes_vote_count;
      DROP INDEX IF EXISTS idx_votes_llm_id;
      DROP TABLE IF EXISTS votes;
    `
  },
  
  {
    name: '003_create_user_sessions_table',
    up: `
      CREATE TABLE IF NOT EXISTS user_sessions (
        id SERIAL PRIMARY KEY,
        fingerprint VARCHAR(255) NOT NULL,
        ip_address INET,
        user_agent TEXT,
        country VARCHAR(2),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_activity TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        is_suspicious BOOLEAN DEFAULT FALSE,
        vote_count INTEGER DEFAULT 0,
        UNIQUE(fingerprint)
      );
      
      -- Create indexes
      CREATE INDEX IF NOT EXISTS idx_user_sessions_fingerprint ON user_sessions(fingerprint);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_last_activity ON user_sessions(last_activity);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_suspicious ON user_sessions(is_suspicious);
      CREATE INDEX IF NOT EXISTS idx_user_sessions_ip ON user_sessions(ip_address);
    `,
    down: `
      DROP INDEX IF EXISTS idx_user_sessions_ip;
      DROP INDEX IF EXISTS idx_user_sessions_suspicious;
      DROP INDEX IF EXISTS idx_user_sessions_last_activity;
      DROP INDEX IF EXISTS idx_user_sessions_fingerprint;
      DROP TABLE IF EXISTS user_sessions;
    `
  },
  
  {
    name: '004_create_user_votes_table',
    up: `
      CREATE TABLE IF NOT EXISTS user_votes (
        id SERIAL PRIMARY KEY,
        fingerprint VARCHAR(255) NOT NULL,
        llm_id VARCHAR(255) NOT NULL REFERENCES llms(id) ON DELETE CASCADE,
        vote_type INTEGER NOT NULL CHECK (vote_type IN (-1, 0, 1)),
        previous_vote INTEGER DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        ip_address INET,
        user_agent TEXT,
        UNIQUE(fingerprint, llm_id)
      );
      
      -- Create indexes for performance
      CREATE INDEX IF NOT EXISTS idx_user_votes_fingerprint ON user_votes(fingerprint);
      CREATE INDEX IF NOT EXISTS idx_user_votes_llm_id ON user_votes(llm_id);
      CREATE INDEX IF NOT EXISTS idx_user_votes_created_at ON user_votes(created_at DESC);
      CREATE INDEX IF NOT EXISTS idx_user_votes_vote_type ON user_votes(vote_type);
    `,
    down: `
      DROP INDEX IF EXISTS idx_user_votes_vote_type;
      DROP INDEX IF EXISTS idx_user_votes_created_at;
      DROP INDEX IF EXISTS idx_user_votes_llm_id;
      DROP INDEX IF EXISTS idx_user_votes_fingerprint;
      DROP TABLE IF EXISTS user_votes;
    `
  },
  
  {
    name: '005_create_analytics_table',
    up: `
      CREATE TABLE IF NOT EXISTS analytics (
        id SERIAL PRIMARY KEY,
        event_type VARCHAR(100) NOT NULL,
        event_data JSONB NOT NULL DEFAULT '{}',
        fingerprint VARCHAR(255),
        ip_address INET,
        user_agent TEXT,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- Create indexes for analytics queries
      CREATE INDEX IF NOT EXISTS idx_analytics_event_type ON analytics(event_type);
      CREATE INDEX IF NOT EXISTS idx_analytics_created_at ON analytics(created_at DESC);
      CREATE INDEX IF NOT EXISTS idx_analytics_fingerprint ON analytics(fingerprint);
    `,
    down: `
      DROP INDEX IF EXISTS idx_analytics_fingerprint;
      DROP INDEX IF EXISTS idx_analytics_created_at;
      DROP INDEX IF EXISTS idx_analytics_event_type;
      DROP TABLE IF EXISTS analytics;
    `
  },
  
  {
    name: '006_create_rate_limits_table',
    up: `
      CREATE TABLE IF NOT EXISTS rate_limits (
        id SERIAL PRIMARY KEY,
        key VARCHAR(255) NOT NULL,
        points INTEGER DEFAULT 0,
        expire_at TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        UNIQUE(key)
      );
      
      -- Create indexes
      CREATE INDEX IF NOT EXISTS idx_rate_limits_key ON rate_limits(key);
      CREATE INDEX IF NOT EXISTS idx_rate_limits_expire_at ON rate_limits(expire_at);
    `,
    down: `
      DROP INDEX IF EXISTS idx_rate_limits_expire_at;
      DROP INDEX IF EXISTS idx_rate_limits_key;
      DROP TABLE IF EXISTS rate_limits;
    `
  },
  
  {
    name: '007_create_triggers',
    up: `
      -- Create function to update timestamp
      CREATE OR REPLACE FUNCTION update_updated_at_column()
      RETURNS TRIGGER AS $$
      BEGIN
          NEW.updated_at = NOW();
          RETURN NEW;
      END;
      $$ language 'plpgsql';
      
      -- Create triggers for updated_at
      CREATE TRIGGER update_llms_updated_at BEFORE UPDATE ON llms 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        
      CREATE TRIGGER update_votes_updated_at BEFORE UPDATE ON votes 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        
      CREATE TRIGGER update_user_votes_updated_at BEFORE UPDATE ON user_votes 
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
    `,
    down: `
      DROP TRIGGER IF EXISTS update_user_votes_updated_at ON user_votes;
      DROP TRIGGER IF EXISTS update_votes_updated_at ON votes;
      DROP TRIGGER IF EXISTS update_llms_updated_at ON llms;
      DROP FUNCTION IF EXISTS update_updated_at_column();
    `
  }
];

async function runMigrations() {
  try {
    await dbManager.initialize();
    
    // Create migrations table
    await dbManager.query(`
      CREATE TABLE IF NOT EXISTS migrations (
        id SERIAL PRIMARY KEY,
        name VARCHAR(255) NOT NULL UNIQUE,
        executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Get already executed migrations
    const { rows: executedMigrations } = await dbManager.query(
      'SELECT name FROM migrations ORDER BY id'
    );
    
    const executedNames = executedMigrations.map(row => row.name);
    
    // Run pending migrations
    for (const migration of migrations) {
      if (!executedNames.includes(migration.name)) {
        logger.info(`Running migration: ${migration.name}`);
        
        await dbManager.transaction(async (client) => {
          await client.query(migration.up);
          await client.query(
            'INSERT INTO migrations (name) VALUES ($1)',
            [migration.name]
          );
        });
        
        logger.info(`Migration completed: ${migration.name}`);
      } else {
        logger.info(`Migration already executed: ${migration.name}`);
      }
    }
    
    logger.info('All migrations completed successfully');
  } catch (error) {
    logger.error('Migration failed:', error);
    process.exit(1);
  } finally {
    await dbManager.closeAll();
  }
}

// Run migrations if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  runMigrations();
}

export { runMigrations, migrations };

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\reset-all.js
const { Pool } = require('pg');

// Database configuration
const dbConfig = {
  connectionString: process.env.DATABASE_URL || 'postgresql://llm_user:changeme@localhost:5432/llm_tracker',
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
};

const pool = new Pool(dbConfig);

async function resetAll() {
  const client = await pool.connect();
  
  try {
    console.log(' Resetting ALL data to zero...\n');
    
    // Start transaction
    await client.query('BEGIN');
    
    // 1. Clear all user votes
    console.log('1. Clearing all user votes...');
    const userVotesResult = await client.query('DELETE FROM user_votes');
    console.log(`    Deleted ${userVotesResult.rowCount} user votes`);
    
    // 2. Clear all user sessions 
    console.log('2. Clearing all user sessions...');
    const sessionsResult = await client.query('DELETE FROM user_sessions');
    console.log(`    Deleted ${sessionsResult.rowCount} user sessions`);
    
    // 3. Clear analytics
    console.log('3. Clearing analytics data...');
    const analyticsResult = await client.query('DELETE FROM analytics');
    console.log(`    Deleted ${analyticsResult.rowCount} analytics records`);
    
    // 4. Reset all vote counts to 0
    console.log('4. Resetting all vote counts to 0...');
    const votesResult = await client.query(
      'UPDATE votes SET vote_count = 0, positive_votes = 0, negative_votes = 0'
    );
    console.log(`    Reset ${votesResult.rowCount} vote records to 0`);
    
    // 5. Clear rate limits
    console.log('5. Clearing rate limits...');
    const rateLimitsResult = await client.query('DELETE FROM rate_limits');
    console.log(`    Deleted ${rateLimitsResult.rowCount} rate limit records`);
    
    // Commit transaction
    await client.query('COMMIT');
    
    console.log('\n ALL DATA RESET SUCCESSFULLY!');
    console.log('    All user votes: CLEARED');
    console.log('    All user sessions: CLEARED');  
    console.log('    All analytics: CLEARED');
    console.log('    All vote counts: RESET TO 0');
    console.log('    All rate limits: CLEARED');
    console.log('\n Next steps:');
    console.log('   1. Clear browser localStorage: Application  Storage  Local Storage  Clear All');
    console.log('   2. Clear Redis cache: docker-compose exec redis redis-cli FLUSHDB');
    console.log('   3. Refresh the browser page');
    
  } catch (error) {
    await client.query('ROLLBACK');
    console.error(' Reset failed:', error);
    process.exit(1);
  } finally {
    client.release();
    await pool.end();
  }
}

// Run reset
resetAll();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\reset.mjs
import dbManager from '../lib/database.js';
import cacheManager from '../lib/cache.js';
import logger from '../lib/logger.js';
import { migrations } from './migrate.js';
import { seedDatabase } from './seed.js';

async function resetDatabase() {
  try {
    await dbManager.initialize();
    logger.warn('  Database reset initiated - ALL DATA WILL BE LOST!');
    
    // Drop all tables in reverse order to respect foreign keys
    logger.info('Dropping all tables...');
    
    await dbManager.transaction(async (client) => {
      // First, get all custom tables (excluding system tables)
      const { rows: tables } = await client.query(`
        SELECT tablename FROM pg_tables 
        WHERE schemaname = 'public' 
        ORDER BY tablename DESC
      `);
      
      // Drop all tables
      for (const { tablename } of tables) {
        logger.info(`Dropping table: ${tablename}`);
        await client.query(`DROP TABLE IF EXISTS ${tablename} CASCADE`);
      }
      
      // Drop all functions
      await client.query(`DROP FUNCTION IF EXISTS update_updated_at_column() CASCADE`);
    });
    
    logger.info('All tables dropped successfully');
    
    // Clear Redis cache if available
    try {
      await cacheManager.initialize();
      logger.info('Clearing Redis cache...');
      await cacheManager.invalidateAllCaches();
      logger.info('Redis cache cleared');
    } catch (error) {
      logger.warn('Could not clear Redis cache:', error.message);
    }
    
    // Re-run all migrations
    logger.info('Re-creating database schema...');
    
    // Create migrations table
    await dbManager.query(`
      CREATE TABLE IF NOT EXISTS migrations (
        id SERIAL PRIMARY KEY,
        name VARCHAR(255) NOT NULL UNIQUE,
        executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Run all migrations
    for (const migration of migrations) {
      logger.info(`Running migration: ${migration.name}`);
      await dbManager.transaction(async (client) => {
        await client.query(migration.up);
        await client.query(
          'INSERT INTO migrations (name) VALUES ($1)',
          [migration.name]
        );
      });
    }
    
    logger.info('Database schema recreated successfully');
    
    // Optionally seed the database
    const shouldSeed = process.argv.includes('--seed');
    if (shouldSeed) {
      logger.info('Seeding database with initial data...');
      await seedDatabase();
    } else {
      logger.info('Skipping database seeding (use --seed flag to seed)');
    }
    
    logger.info(' Database reset completed successfully');
    
  } catch (error) {
    logger.error('Database reset failed:', error);
    process.exit(1);
  } finally {
    await dbManager.closeAll();
  }
}

// Run reset if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  // Safety check for production
  if (process.env.NODE_ENV === 'production' && !process.argv.includes('--force')) {
    logger.error(' Cannot reset production database without --force flag');
    logger.error('This action will DELETE ALL DATA. Use: npm run db:reset -- --force');
    process.exit(1);
  }
  
  resetDatabase();
}

export { resetDatabase };

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\seed.js
const { Pool } = require('pg');

// Database configuration
const dbConfig = {
  connectionString: process.env.DATABASE_URL || 'postgresql://llm_user:changeme@localhost:5432/llm_tracker',
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
};

const pool = new Pool(dbConfig);

const llmData = [
  {
    id: "gpt-4o",
    name: "GPT-4o",
    company: "OpenAI",
    description: "Most advanced multimodal AI with vision, analysis, and coding capabilities",
    useCases: ["General purpose", "Code generation", "Creative writing", "Vision tasks"],
    releaseYear: 2024,
    color: "from-green-500 to-emerald-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/200px-OpenAI_Logo.svg.png",
  },
  {
    id: "claude-3-5-sonnet",
    name: "Claude 3.5 Sonnet",
    company: "Anthropic",
    description: "Balanced model excelling at analysis, coding, and nuanced conversation",
    useCases: ["Code analysis", "Research", "Writing", "Complex reasoning"],
    releaseYear: 2024,
    color: "from-orange-500 to-amber-600",
    logo: "",
    image: "https://www.anthropic.com/_next/static/media/claude-logo.2f5f0b53.svg",
  },
  {
    id: "gemini-ultra",
    name: "Gemini Ultra",
    company: "Google",
    description: "Google's flagship model with strong multimodal and reasoning abilities",
    useCases: ["Multimodal tasks", "Scientific research", "Code", "Mathematics"],
    releaseYear: 2024,
    color: "from-blue-500 to-cyan-600",
    logo: "",
    image: "https://www.gstatic.com/lamda/images/gemini_sparkle_v002_d4735304ff6292a690345.svg",
  },
  {
    id: "llama-3-70b",
    name: "Llama 3 70B",
    company: "Meta",
    description: "Open-source powerhouse for customizable AI applications",
    useCases: ["Open source projects", "Fine-tuning", "Research", "Commercial use"],
    releaseYear: 2024,
    color: "from-purple-500 to-violet-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Meta_Platforms_Inc._logo.svg/200px-Meta_Platforms_Inc._logo.svg.png",
  },
  {
    id: "mistral-large",
    name: "Mistral Large",
    company: "Mistral AI",
    description: "European AI champion with strong multilingual capabilities",
    useCases: ["Multilingual tasks", "European languages", "Code", "Efficiency"],
    releaseYear: 2024,
    color: "from-red-500 to-pink-600",
    logo: "",
    image: "https://docs.mistral.ai/img/logo.svg",
  },
  {
    id: "command-r-plus",
    name: "Command R+",
    company: "Cohere",
    description: "Enterprise-focused with excellent retrieval and grounding",
    useCases: ["Enterprise search", "RAG systems", "Document analysis", "Business"],
    releaseYear: 2024,
    color: "from-indigo-500 to-blue-600",
    logo: "",
    image: "https://cohere.com/favicon.svg",
  },
  {
    id: "grok",
    name: "Grok",
    company: "xAI",
    description: "Real-time knowledge with humor and unconventional responses",
    useCases: ["Real-time info", "Social media", "Humor", "Current events"],
    releaseYear: 2023,
    color: "from-gray-600 to-slate-700",
    logo: "",
    image: "https://grok.x.ai/assets/grok-logo-light.svg",
  },
  {
    id: "perplexity",
    name: "Perplexity",
    company: "Perplexity AI",
    description: "Search-enhanced AI with real-time web access and citations",
    useCases: ["Web search", "Research", "Fact-checking", "Citations"],
    releaseYear: 2024,
    color: "from-teal-500 to-cyan-600",
    logo: "",
    image: "https://www.perplexity.ai/favicon.svg",
  },
  {
    id: "qwen-2-5",
    name: "Qwen 2.5",
    company: "Alibaba",
    description: "Strong Asian language support with competitive performance",
    useCases: ["Chinese language", "Asian markets", "E-commerce", "Translation"],
    releaseYear: 2024,
    color: "from-yellow-500 to-orange-600",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Alibaba-Logo.svg/200px-Alibaba-Logo.svg.png",
  },
  {
    id: "falcon-180b",
    name: "Falcon 180B",
    company: "TII UAE",
    description: "Open-source giant with strong performance across tasks",
    useCases: ["Open source", "Research", "Arabic language", "General purpose"],
    releaseYear: 2023,
    color: "from-amber-600 to-yellow-700",
    logo: "",
    image: "https://www.tii.ae/sites/default/files/2022-12/TII-logo-WHITE.png",
  },
  {
    id: "bard",
    name: "Bard (Gemini Pro)",
    company: "Google",
    description: "Google's conversational AI with web access and multimodal features",
    useCases: ["Web browsing", "Conversation", "Creative tasks", "Integration"],
    releaseYear: 2024,
    color: "from-blue-500 to-green-600",
    logo: "",
    image: "https://www.gstatic.com/lamda/images/gemini_sparkle_v002_d4735304ff6292a690345.svg",
  },
  {
    id: "ernie-4",
    name: "ERNIE 4.0",
    company: "Baidu",
    description: "Chinese AI leader with strong understanding of Chinese culture",
    useCases: ["Chinese market", "Cultural context", "Search", "Business"],
    releaseYear: 2024,
    color: "from-red-600 to-orange-700",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/en/thumb/3/37/Baidu_Logo.svg/200px-Baidu_Logo.svg.png",
  },
  {
    id: "deepseek-coder",
    name: "DeepSeek Coder",
    company: "DeepSeek",
    description: "Specialized coding model with excellent debugging capabilities",
    useCases: ["Code generation", "Debugging", "Code review", "Documentation"],
    releaseYear: 2024,
    color: "from-green-600 to-teal-700",
    logo: "",
    image: "https://github.com/deepseek-ai.png",
  },
  {
    id: "mixtral-8x7b",
    name: "Mixtral 8x7B",
    company: "Mistral AI",
    description: "Mixture of experts model with excellent efficiency",
    useCases: ["Efficient inference", "MoE architecture", "Code", "Multiple languages"],
    releaseYear: 2024,
    color: "from-violet-600 to-purple-700",
    logo: "",
    image: "https://docs.mistral.ai/img/logo.svg",
  },
  {
    id: "yi-34b",
    name: "Yi-34B",
    company: "01.AI",
    description: "Bilingual model excelling in Chinese and English tasks",
    useCases: ["Chinese-English", "Translation", "Bilingual tasks", "Open source"],
    releaseYear: 2024,
    color: "from-purple-600 to-pink-700",
    logo: "",
    image: "https://github.com/01-ai.png",
  },
  {
    id: "solar-10-7b",
    name: "SOLAR-10.7B",
    company: "Upstage AI",
    description: "Efficient Korean model with strong multilingual capabilities",
    useCases: ["Korean language", "Efficient inference", "Asian languages", "Small models"],
    releaseYear: 2024,
    color: "from-orange-600 to-red-700",
    logo: "",
    image: "https://github.com/UpstageAI.png",
  },
  {
    id: "inflection-2-5",
    name: "Inflection-2.5",
    company: "Inflection AI",
    description: "Personal AI with empathetic and supportive conversation style",
    useCases: ["Personal assistant", "Emotional support", "Coaching", "Companionship"],
    releaseYear: 2024,
    color: "from-teal-600 to-blue-700",
    logo: "",
    image: "https://github.com/InflectionAI.png",
  },
  {
    id: "vicuna-33b",
    name: "Vicuna-33B",
    company: "LMSYS",
    description: "Fine-tuned Llama model with improved conversational abilities",
    useCases: ["Chatbots", "Open source", "Fine-tuning base", "Research"],
    releaseYear: 2023,
    color: "from-pink-500 to-rose-600",
    logo: "",
    image: "https://github.com/lm-sys.png",
  },
  {
    id: "phi-3",
    name: "Phi-3",
    company: "Microsoft",
    description: "Small but mighty model optimized for edge deployment",
    useCases: ["Edge computing", "Mobile apps", "Low resource", "Fast inference"],
    releaseYear: 2024,
    color: "from-blue-600 to-indigo-700",
    logo: "",
    image: "https://upload.wikimedia.org/wikipedia/commons/thumb/4/44/Microsoft_logo.svg/200px-Microsoft_logo.svg.png",
  },
  {
    id: "stablelm-2",
    name: "StableLM 2",
    company: "Stability AI",
    description: "Open model from the makers of Stable Diffusion",
    useCases: ["Open source", "Customization", "Research", "Creative apps"],
    releaseYear: 2024,
    color: "from-purple-500 to-indigo-600",
    logo: "",
    image: "https://github.com/Stability-AI.png",
  }
];

async function seedDatabase() {
  const client = await pool.connect();
  
  try {
    console.log('Starting database seeding...\n');
    
    // Start transaction
    await client.query('BEGIN');
    
    // Insert LLMs
    console.log('Inserting LLM data...');
    for (const llm of llmData) {
      await client.query(
        `INSERT INTO llms (id, name, company, description, use_cases, release_year, color, logo, image_url)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
         ON CONFLICT (id) DO UPDATE SET
           name = EXCLUDED.name,
           company = EXCLUDED.company,
           description = EXCLUDED.description,
           use_cases = EXCLUDED.use_cases,
           release_year = EXCLUDED.release_year,
           color = EXCLUDED.color,
           logo = EXCLUDED.logo,
           image_url = EXCLUDED.image_url,
           updated_at = NOW()`,
        [
          llm.id,
          llm.name,
          llm.company,
          llm.description,
          JSON.stringify(llm.useCases),
          llm.releaseYear,
          llm.color,
          llm.logo,
          llm.image
        ]
      );
      console.log(`   ${llm.name}`);
    }
    
    // Initialize votes table with zero votes for each LLM
    console.log('\nInitializing votes table with 0 votes...');
    for (const llm of llmData) {
      await client.query(
        `INSERT INTO votes (llm_id, vote_count, positive_votes, negative_votes)
        VALUES ($1, 0, 0, 0)
        ON CONFLICT (llm_id) DO NOTHING`,  // Changed from DO UPDATE to DO NOTHING
        [llm.id]
      );
      console.log(`   ${llm.name}: initialized with 0 votes`);
    }
    console.log('   All votes initialized to 0')
    
    // Commit transaction
    await client.query('COMMIT');
    
    console.log('\n Database seeded successfully!');
    
  } catch (error) {
    await client.query('ROLLBACK');
    console.error(' Seeding failed:', error);
    process.exit(1);
  } finally {
    client.release();
    await pool.end();
  }
}

// Run seeding
seedDatabase();

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\scripts\seed.mjs
import dbManager from '../lib/database.js';
import { llmData } from '../lib/llm-data.js';
import logger from '../lib/logger.js';

async function seedDatabase() {
  try {
    await dbManager.initialize();
    logger.info('Starting database seeding...');
    
    // Begin transaction
    await dbManager.transaction(async (client) => {
      // Insert LLMs
      logger.info('Inserting LLMs...');
      for (const llm of llmData) {
        await client.query(
          `INSERT INTO llms (
            id, name, company, description, use_cases, 
            release_year, color, logo, image_url
          ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
          ON CONFLICT (id) DO UPDATE SET
            name = EXCLUDED.name,
            company = EXCLUDED.company,
            description = EXCLUDED.description,
            use_cases = EXCLUDED.use_cases,
            release_year = EXCLUDED.release_year,
            color = EXCLUDED.color,
            logo = EXCLUDED.logo,
            image_url = EXCLUDED.image_url,
            updated_at = NOW()`,
          [
            llm.id,
            llm.name,
            llm.company,
            llm.description,
            JSON.stringify(llm.useCases || []),
            llm.releaseYear,
            llm.color,
            llm.logo,
            llm.image || null
          ]
        );
      }
      logger.info(`Inserted ${llmData.length} LLMs`);
      
      // Initialize vote counts for each LLM
      logger.info('Initializing vote counts...');
      for (const llm of llmData) {
        await client.query(
          `INSERT INTO votes (llm_id, vote_count, positive_votes, negative_votes)
           VALUES ($1, 0, 0, 0)
           ON CONFLICT (llm_id) DO NOTHING`,
          [llm.id]
        );
      }
      logger.info('Vote counts initialized');
      
      // Insert sample analytics events (optional)
      logger.info('Inserting sample analytics events...');
      const sampleEvents = [
        { event_type: 'page_view', event_data: { page: 'home' } },
        { event_type: 'app_start', event_data: { version: '0.1.0' } },
      ];
      
      for (const event of sampleEvents) {
        await client.query(
          `INSERT INTO analytics (event_type, event_data)
           VALUES ($1, $2)`,
          [event.event_type, JSON.stringify(event.event_data)]
        );
      }
      logger.info('Sample analytics events inserted');
    });
    
    // Verify seed data
    const { rows: llmCount } = await dbManager.query('SELECT COUNT(*) FROM llms');
    const { rows: voteCount } = await dbManager.query('SELECT COUNT(*) FROM votes');
    
    logger.info('Database seeding completed successfully');
    logger.info(`Total LLMs: ${llmCount[0].count}`);
    logger.info(`Total vote records: ${voteCount[0].count}`);
    
  } catch (error) {
    logger.error('Database seeding failed:', error);
    process.exit(1);
  } finally {
    await dbManager.closeAll();
  }
}

// Run seed if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  seedDatabase();
}

export { seedDatabase };

// File: c:\Users\Taimoor\Documents\GitHub\llm-popularity-tracker\store\useVoteStore.js
import { create } from 'zustand';
import axios from 'axios';
import { llmData } from '@/lib/llm-data';
import fingerprintService from '@/lib/fingerprint';

const useVoteStore = create((set, get) => ({
  llms: llmData,
  votes: {},
  userVotes: {},
  rankings: [],
  stats: {
    totalVotes: 0,
    votesToday: 0,
    votesLastHour: 0,
    trending: [],
    topModel: null,
  },
  loading: false,
  error: null,
  lastUpdate: null,
  fingerprint: null,
  
  // Initialize votes and fingerprint
  initializeVotes: async () => {
    console.log(' [INIT] Starting vote store initialization...');
    set({ loading: true, error: null });
    try {
      // Get fingerprint first
      console.log(' [INIT] Getting fingerprint...');
      const fingerprint = await fingerprintService.getFingerprintWithFallbacks();
      console.log(' [INIT] Fingerprint obtained:', fingerprint.substring(0, 8) + '...');
      set({ fingerprint });
      
      // Load votes from localStorage first for immediate UI update
      console.log(' [INIT] Loading votes from localStorage...');
      const localVotes = get().loadVotesFromLocalStorage();
      if (localVotes.userVotes && Object.keys(localVotes.userVotes).length > 0) {
        console.log(' [INIT] Found local user votes:', Object.keys(localVotes.userVotes).length, 'votes');
        set({ userVotes: localVotes.userVotes });
      } else {
        console.log(' [INIT] No local user votes found');
      }
      
      // Then sync with server
      console.log(' [INIT] Syncing with server...');
      const response = await axios.post('/api/vote/sync', { fingerprint });
      console.log(' [INIT] Server sync response:', {
        votesCount: response.data.votes ? Object.keys(response.data.votes).length : 0,
        userVotesCount: response.data.userVotes ? Object.keys(response.data.userVotes).length : 0,
        hasRankings: !!response.data.rankings,
        hasStats: !!response.data.stats
      });
      
      const serverData = {
        votes: response.data.votes || {},
        userVotes: response.data.userVotes || {},
        rankings: response.data.rankings || [],
        stats: response.data.stats || get().stats,
        loading: false,
        lastUpdate: new Date(),
      };
      
      console.log(' [INIT] Setting server data:', {
        totalVotes: Object.values(serverData.votes).reduce((sum, count) => sum + Math.abs(count), 0),
        userVotesCount: Object.keys(serverData.userVotes).length
      });
      
      set(serverData);
      
      // Update rankings and stats after loading
      get().updateRankings();
      
      // Save to localStorage
      get().saveVotesToLocalStorage(serverData);
      console.log(' [INIT] Vote store initialization completed successfully');
      
    } catch (error) {
      console.error(' [INIT] Failed to load votes:', error);
      
      // Try to load from localStorage as fallback
      const localData = get().loadVotesFromLocalStorage();
      if (localData.votes) {
        console.log(' [INIT] Using localStorage fallback data');
        set({ 
          ...localData,
          loading: false, 
          error: 'Using offline data',
        });
      } else {
        console.log(' [INIT] Initializing with zero votes');
        set({ 
          loading: false, 
          error: 'Failed to load votes',
        });
        // Initialize with zero votes
        const initialVotes = {};
        llmData.forEach(llm => {
          initialVotes[llm.id] = 0;
        });
        set({ votes: initialVotes });
      }
    }
  },
  
  // Vote for an LLM
  vote: async (llmId, voteType) => {
    console.log(' [VOTE] Starting vote process:', { llmId, voteType });
    
    const currentUserVote = get().userVotes[llmId] || 0;
    const fingerprint = get().fingerprint;
    
    console.log(' [VOTE] Current state:', { 
      currentUserVote, 
      fingerprint: fingerprint ? fingerprint.substring(0, 8) + '...' : 'none',
      currentVotes: get().votes[llmId] || 0
    });
    
    if (!fingerprint) {
      console.error(' [VOTE] No fingerprint available');
      return;
    }
    
    // Optimistically update UI
    const optimisticVotes = { ...get().votes };
    const optimisticUserVotes = { ...get().userVotes };
    
    // Calculate vote change
    const voteChange = voteType - currentUserVote;
    optimisticVotes[llmId] = (optimisticVotes[llmId] || 0) + voteChange;
    
    console.log(' [VOTE] Vote change calculation:', { 
      voteChange, 
      oldVoteCount: get().votes[llmId] || 0,
      newVoteCount: optimisticVotes[llmId]
    });
    
    if (voteType === 0) {
      delete optimisticUserVotes[llmId];
    } else {
      optimisticUserVotes[llmId] = voteType;
    }
    
    const newState = { 
      votes: optimisticVotes,
      userVotes: optimisticUserVotes,
    };
    
    console.log(' [VOTE] Optimistic UI update:', { 
      newVoteCount: newState.votes[llmId],
      newUserVote: newState.userVotes[llmId] || 0
    });
    
    set(newState);
    
    // Save to localStorage immediately
    get().saveVotesToLocalStorage(newState);
    console.log(' [VOTE] Saved optimistic state to localStorage');
    
    try {
      console.log(' [VOTE] Sending request to server:', { fingerprint: fingerprint.substring(0, 8) + '...', llmId, voteType });
      
      const response = await axios.post('/api/vote', { 
        fingerprint, 
        llmId, 
        voteType 
      });
      
      console.log(' [VOTE] Server response:', {
        success: response.data.success,
        userVote: response.data.userVote,
        previousVote: response.data.previousVote,
        serverVoteCount: response.data.votes ? response.data.votes[llmId] : 'not provided'
      });
      
      if (response.data.success) {
        // Properly update the user vote for this specific LLM
        const updatedUserVotes = { ...get().userVotes };
        if (response.data.userVote === 0) {
          delete updatedUserVotes[llmId];
        } else {
          updatedUserVotes[llmId] = response.data.userVote;
        }
        
        // Use server's authoritative vote counts for all LLMs
        const serverState = {
          votes: response.data.votes || get().votes,
          userVotes: updatedUserVotes,
          lastUpdate: new Date(),
        };
        
        console.log(' [VOTE] Updating state with server data:', {
          finalVoteCount: serverState.votes[llmId],
          finalUserVote: serverState.userVotes[llmId] || 0
        });
        
        set(serverState);
        
        // Update rankings and stats
        get().updateRankings();
        
        // Save updated state to localStorage
        get().saveVotesToLocalStorage(serverState);
        console.log(' [VOTE] Saved final state to localStorage');
      }
    } catch (error) {
      console.error(' [VOTE] Server request failed:', error);
      console.log(' [VOTE] Keeping optimistic update in place');
      
      // Don't revert - keep the optimistic update as it's saved locally
      // Update rankings even if server sync fails
      get().updateRankings();
      
      set({ 
        error: 'Failed to sync vote with server',
      });
      
      // Clear error after 3 seconds
      setTimeout(() => set({ error: null }), 3000);
    }
  },
  
  // Update rankings based on votes
  updateRankings: () => {
    const votes = get().votes;
    const rankings = Object.entries(votes)
      .map(([id, count]) => ({ id, count }))
      .sort((a, b) => b.count - a.count)
      .map((item, index) => ({ ...item, rank: index + 1 }));
    
    // Calculate stats in real-time
    const totalVotes = Object.values(votes).reduce((sum, count) => sum + Math.abs(count), 0);
    const topModel = rankings[0]?.id || null;
    const topModelName = topModel ? get().getLLMById(topModel)?.name || topModel : null;
    
    const updatedStats = {
      ...get().stats,
      totalVotes,
      votesToday: totalVotes, // For now, treat all votes as today's votes
      votesLastHour: totalVotes, // For now, treat all votes as last hour's votes  
      topModel: topModelName,
    };
    
    set({ rankings, stats: updatedStats });
  },
  
  // Fetch latest stats
  fetchStats: async () => {
    try {
      const response = await axios.get('/api/stats');
      set({
        stats: response.data.stats,
        rankings: response.data.rankings,
        lastUpdate: new Date(),
      });
    } catch (error) {
      console.error('Failed to fetch stats:', error);
    }
  },
  
  // Get LLM by ID
  getLLMById: (id) => {
    return get().llms.find(llm => llm.id === id);
  },
  
  // Get user's vote for an LLM
  getUserVote: (llmId) => {
    return get().userVotes[llmId] || 0;
  },
  
  // Get total votes for an LLM
  getVoteCount: (llmId) => {
    return get().votes[llmId] || 0;
  },
  
  // Check if LLM is trending
  isTrending: (llmId) => {
    return get().stats.trending.includes(llmId);
  },
  
  // Get rank position for an LLM
  getRank: (llmId) => {
    const ranking = get().rankings.find(r => r.id === llmId);
    return ranking ? ranking.rank : null;
  },
  
  // Save votes to localStorage
  saveVotesToLocalStorage: (state) => {
    if (typeof window === 'undefined') return;
    
    try {
      const dataToSave = {
        votes: state.votes || get().votes,
        userVotes: state.userVotes || get().userVotes,
        lastUpdate: state.lastUpdate || get().lastUpdate,
        timestamp: new Date().toISOString(),
      };
      
      localStorage.setItem('llm-tracker-votes', JSON.stringify(dataToSave));
      sessionStorage.setItem('llm-tracker-votes-backup', JSON.stringify(dataToSave));
      
      // Also save with fingerprint as key for additional persistence
      const fingerprint = get().fingerprint;
      if (fingerprint) {
        localStorage.setItem(`llm-tracker-votes-${fingerprint}`, JSON.stringify(dataToSave));
      }
    } catch (error) {
      console.error('Failed to save votes to localStorage:', error);
    }
  },
  
  // Load votes from localStorage
  loadVotesFromLocalStorage: () => {
    if (typeof window === 'undefined') return {};
    
    try {
      // Try primary storage first
      let stored = localStorage.getItem('llm-tracker-votes');
      
      if (!stored) {
        // Try sessionStorage backup
        stored = sessionStorage.getItem('llm-tracker-votes-backup');
      }
      
      if (!stored) {
        // Try fingerprint-based storage
        const fingerprint = get().fingerprint;
        if (fingerprint) {
          stored = localStorage.getItem(`llm-tracker-votes-${fingerprint}`);
        }
      }
      
      if (stored) {
        const data = JSON.parse(stored);
        return {
          votes: data.votes || {},
          userVotes: data.userVotes || {},
          lastUpdate: data.lastUpdate ? new Date(data.lastUpdate) : null,
        };
      }
    } catch (error) {
      console.error('Failed to load votes from localStorage:', error);
    }
    
    return {};
  },
  
  // Sync with server for real-time updates
  syncWithServer: async () => {
    try {
      const fingerprint = get().fingerprint;
      if (!fingerprint) {
        console.debug(' [SYNC] Skipping sync - no fingerprint available');
        return; // Skip sync if no fingerprint available
      }
      
      console.debug(' [SYNC] Starting background sync...');
      const response = await axios.post('/api/vote/sync', { fingerprint });
      
      if (response.data) {
        const currentUserVotes = get().userVotes;
        
        // Only update if there are actual changes to prevent unnecessary re-renders
        const newVotes = response.data.votes || {};
        const newStats = response.data.stats || get().stats;
        const newRankings = response.data.rankings || [];
        
        // Check if votes have actually changed
        const votesChanged = JSON.stringify(newVotes) !== JSON.stringify(get().votes);
        const statsChanged = JSON.stringify(newStats) !== JSON.stringify(get().stats);
        
        console.debug(' [SYNC] Sync data comparison:', {
          votesChanged,
          statsChanged,
          currentTotalVotes: Object.values(get().votes).reduce((sum, count) => sum + Math.abs(count), 0),
          newTotalVotes: Object.values(newVotes).reduce((sum, count) => sum + Math.abs(count), 0)
        });
        
        if (votesChanged || statsChanged) {
          console.debug(' [SYNC] Updating state with new data from server');
          set({
            votes: newVotes,
            rankings: newRankings,
            stats: newStats,
            lastUpdate: new Date(),
            // Keep user votes from current state, don't override with server
            userVotes: { ...currentUserVotes, ...(response.data.userVotes || {}) },
          });
          
          // Update local storage with new data
          get().saveVotesToLocalStorage({
            votes: newVotes,
            userVotes: currentUserVotes,
            lastUpdate: new Date(),
          });
          console.debug(' [SYNC] Background sync completed with updates');
        } else {
          console.debug(' [SYNC] No changes detected, skipping update');
        }
      }
    } catch (error) {
      // Silently fail - this is background sync
      console.debug(' [SYNC] Background sync failed:', error.message);
    }
  },
  
  // Clear all stored data (for testing or user request)
  clearAllStoredData: () => {
    if (typeof window === 'undefined') return;
    
    try {
      localStorage.removeItem('llm-tracker-votes');
      sessionStorage.removeItem('llm-tracker-votes-backup');
      
      // Clear fingerprint-based storage
      const fingerprint = get().fingerprint;
      if (fingerprint) {
        localStorage.removeItem(`llm-tracker-votes-${fingerprint}`);
      }
      
      // Clear fingerprint
      fingerprintService.clearFingerprint();
      
      // Reset state
      const initialVotes = {};
      llmData.forEach(llm => {
        initialVotes[llm.id] = 0;
      });
      
      set({
        votes: initialVotes,
        userVotes: {},
        fingerprint: null,
      });
    } catch (error) {
      console.error('Failed to clear stored data:', error);
    }
  },
}));

export default useVoteStore;

